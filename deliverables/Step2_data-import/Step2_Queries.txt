


##############################################################################################
##############################################################################################
##############################################################################################
##############################################################################################
##############################################################################################
##############################################################################################
##############################################################################################
#################################### QUERYING COURSE INDEX ###################################
##############################################################################################
##############################################################################################
##############################################################################################
##############################################################################################
##############################################################################################
##############################################################################################
##############################################################################################

###Query String
search=knowledge+mining&$count=true&$top=3

###Request URL
https://azurecognetivesearch-service.search.windows.net/indexes/courses-index/docs?api-version=2023-07-01-Preview&search=knowledge%2Bmining&%24count=true&%24top=3

###Results
{
  "@odata.context": "https://azurecognetivesearch-service.search.windows.net/indexes('courses-index')/$metadata#docs(*)",
  "@odata.count": 5,
  "value": [
    {
      "@search.score": 17.433321,
      "PartitionKey": "ms-learn",
      "RowKey": "5244e529-28c6-4b6c-bba0-a3edcb956327",
      "Key": "bXMtbGVhcm41MjQ0ZTUyOS0yOGM2LTRiNmMtYmJhMC1hM2VkY2I5NTYzMjc1",
      "description": "Create a knowledge store with Azure Cognitive Search",
      "duration": 46,
      "instructor": null,
      "level": "intermediate",
      "product": "azure-cognitive-search",
      "rating_average": 4.79,
      "rating_count": 42,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create a knowledge store with Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-knowledge-store-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Cognitive Search",
        "knowledge store"
      ]
    },
    {
      "@search.score": 17.433321,
      "PartitionKey": "ms-learn",
      "RowKey": "c960fef1-1977-4b34-8f09-f00cbe326e34",
      "Key": "bXMtbGVhcm5jOTYwZmVmMS0xOTc3LTRiMzQtOGYwOS1mMDBjYmUzMjZlMzQ1",
      "description": "Create a knowledge store with Azure Cognitive Search",
      "duration": 46,
      "instructor": null,
      "level": "intermediate",
      "product": "azure-cognitive-search",
      "rating_average": 4.79,
      "rating_count": 42,
      "role": "developer",
      "source": "MS Learn",
      "title": "Create a knowledge store with Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-knowledge-store-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Cognitive Search",
        "knowledge store"
      ]
    },
    {
      "@search.score": 17.433321,
      "PartitionKey": "ms-learn",
      "RowKey": "fac634ec-2700-4257-85c1-066454b639d4",
      "Key": "bXMtbGVhcm5mYWM2MzRlYy0yNzAwLTQyNTctODVjMS0wNjY0NTRiNjM5ZDQ1",
      "description": "Create a knowledge store with Azure Cognitive Search",
      "duration": 46,
      "instructor": null,
      "level": "intermediate",
      "product": "azure-cognitive-search",
      "rating_average": 4.79,
      "rating_count": 42,
      "role": "solution-architect",
      "source": "MS Learn",
      "title": "Create a knowledge store with Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-knowledge-store-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Cognitive Search",
        "knowledge store"
      ]
    }
  ]
}






##############################################################################################
##############################################################################################
##############################################################################################
##############################################################################################
##############################################################################################
##############################################################################################
##############################################################################################
#################################### QUERYING LIBRARY INDEX ##################################
##############################################################################################
##############################################################################################
##############################################################################################
##############################################################################################
##############################################################################################
##############################################################################################
##############################################################################################

###Query String
search=multiobjective optimization&$count=true&$top=2

###Request URL
https://azurecognetivesearch-service.search.windows.net/indexes/library-index/docs?api-version=2023-07-01-Preview&search=multiobjective%20optimization&%24count=true&%24top=2

###Results
{
  "@odata.context": "https://azurecognetivesearch-service.search.windows.net/indexes('library-index')/$metadata#docs(*)",
  "@odata.count": 11,
  "value": [
    {
      "@search.score": 5.018669,
      "content": "\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 \nDOI 10.1186/s40467-015-0033-9\n\nRESEARCH Open Access\n\nExtraction methods for uncertain inference\nrules by ant colony optimization\nLing Chen, Yun Sun* and Yuanguo Zhu\n\n*Correspondence:\nchinalsy_881220@163.com\nSchool of Science, Nanjing\nUniversity of Science and\nTechnology, Nanjing 210094, China\n\nAbstract\n\nIn recent years, the research on data mining methods has received increasing\nattention. In this paper, we design an uncertain system with the extracted uncertain\ninference rules to solve the classification problems in data mining. And then, two\nextraction methods integrated with ant colony optimization are proposed for the\ngeneration of the uncertain inference rules. Finally, two applications are given to verify\nthe effectiveness and superiority of the proposed methods.\n\nKeywords: Uncertain inference rule; Uncertain system; Ant colony optimization\nalgorithm; Rules extraction; Data classification\n\nIntroduction\nNowadays, databases and computer networks, coupled with the use of advanced auto-\nmated data generation and collection tools, are widely used in many different fields such\nas finance, E-commerce, logistics, etc. As a result, the amount of data that people have\nto deal with is dramatically increasing. People hope to carry out scientific research, busi-\nness decision, or business management on the basis of the analysis of the existing data.\nHowever, the current data analysis tools have difficulty in processing the data in depth.\nTo compensate for this deficiency, there come the data mining techniques. Data mining is\nthe computational process of discovering some interesting, potentially useful patterns in\nlarge data sets. Those patterns can be concepts, rules, laws, and modes. The overall goal\nof data mining is to extract information from a data set and transform it into an under-\nstandable structure for further use. Data mining helps us to discover valuable information\nand knowledge. Data mining is applied tomany fields in reality. There are many successful\nexamples [1] of data mining in business and science research. For instance, data mining is\nwidely used in financial data analysis, telecommunication, retail, and biomedical research.\nTherefore, the study of data mining technology has an important practical significance.\nThe main jobs of data mining are data description, data classification, data dependency,\n\ndata compartment analysis, data regression, data aggregate, and data prediction. What\ndata classification does is to find a couple of models or functions that can accurately\ndescribe the characteristics of the data sets. Then, we can identify the categories of the\npreviously unknown data. After obtaining themodels or functions from the set of training\ndata with data mining algorithms, we use many methods to describe the output such as\nclassification rules (if-then), decision trees, mathematical formula, and neutral network.\n\n© 2015 Chen et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons\nAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work is properly credited.\n\nmailto: chinalsy_881220@163.com\nhttp://creativecommons.org/licenses/by/4.0\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 2 of 19\n\nThere are a variety of approaches in data mining. For mining objects in different fields,\nmany different specifiedmethods are invented. The approaches we usually used are statis-\ntical methods, machine learning methods, and modern intelligent optimization methods.\nThe statistical methods are very effective methods from the start. In addition, many other\ndata mining methods are invented based on the statistical methods. When dealing with\nclassification problems, Bayesian classification and Bayesian belief network are important\nclassification methods that based on the statistical principle. Machine learning methods\nare mainly used to solve the conceptual learning, pattern classification, and pattern clus-\ntering problems. The core content of machine learning is inductive learning. And there\nalready exist a number of mature technology methods, such as decision tree method for\nclassification problems. Decision trees method is one of the most popular classification\nmethods. The early decision trees algorithm is ID3 method. Later, based on ID3, many\nalgorithms such as C4.5 method [2] are proposed. Besides, there are some variants of the\ndecision trees algorithm including incremental tree structure ID4, ID5, and expandable\ntree structure SLIQ for massive data set.\nIn recent years, intelligent optimization algorithms are widely applied into data min-\n\ning. Neutral network is a simulation model for complex system with nonlinear relations.\nIt is very suitable to deal with complex nonlinear relations in spatial data. Researchers\nhave already proposed different network models to realize the clustering, classification,\nregression, and pattern recognition of the data. Furthermore, many evolution algorithms\nsuch as simulated annealing algorithm are introduced into neutral network algorithm\nas the optimization strategies. Genetic algorithm is a global search algorithm that sim-\nulates the biological evolution and genetic mechanism. It plays an important role in\noptimization and classification machine learning. Mixed algorithms of genetic algorithm\nand other algorithms, such as decision trees, neutral network, have been applied to the\ndata mining technology. Ant colony optimization algorithm is a bionic optimization algo-\nrithm that simulates the behavior of the ants. Based on that, a data mining technique\nant-miner [3] was invented. And Herrera [4] applied it to fuzzy rules learning. How-\never, ant colony optimization algorithm has some weakness such as slow convergence,\nrandom initial solutions. For this reason, some improved ant colony optimization algo-\nrithms are proposed. Zhu proposed an improved ant colony optimization algorithm\n(ACOA) [5] and a mutation ant colony optimization algorithm (MACO) [6] to speed up\nthe algorithms and avoid the solutions getting stuck in local optimums. Hybrid genetic\nant colony optimization [7] and hybrid particle swarm ant colony optimization algo-\nrithm [8] significantly improve the performance of the original ant colony optimization\nalgorithm.\nThe real world is so complex that human being may face different types of indetermi-\n\nnacy everyday. To get a better understanding of the real world, many mathematical tools\nare created. One of them is probability theory which is used to model indeterminacy from\nsamples. However, in many cases, no samples are available to estimate a probability distri-\nbution. In this situation, we have no choice but to invite some domain experts to evaluate\nthe belief degree that each event may occur. We cannot use probability theory to deal\nwith belief degree since human beings usually overweight unlikely events which makes\nthe belief degrees deviate far from the frequency. In view of this, Liu [9] founded uncer-\ntainty theory based on normality axiom, duality axiom, subadditivity axiom, and product\nmeasure axiom. It has become a powerful mathematical tool dealing with indeterminacy.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 3 of 19\n\nMany researchers have done a lot of theoretical work related to uncertainty theory. In\n2008, Liu [10] presented the uncertain differential equation. Later, the existence and\nuniqueness theorem was given [11]. And the stability of uncertain differential equation\nwas discussed [12,13]. Also, some analysis and numerical methods for solving uncertain\ndifferential equation were proposed. With uncertain differential equation describing the\nevolution of the system, we may solve some practical problems. Peng and Yao [14] stud-\nied an option pricing models for stocks. Zhu [15] proposed an uncertain optimal control\nmodel in 2010.\nIn [16,17], Liu proposed and studied the uncertain systems based on the concepts of\n\nuncertain sets, membership functions, and uncertain inference rules. An uncertain sys-\ntem is a function from its inputs to outputs based on the uncertain inference rule. Usually,\nan uncertain system consists of five parts: inputs, rule-base, uncertain inference rules,\nexpected value operator, and outputs. Following that, Gao et al. [18] generalized uncertain\ninference rules and described uncertain systems with them. Peng and Chen [19] proved\nthat uncertain systems are universal approximator and then demonstrated that the uncer-\ntain controller is a reasonable tool. Gao [20] designed an uncertain inference controller\nthat successfully balanced an inverted pendulum with 5 × 5 if-then rules. What is more\nimportant is that this uncertain inference controller has a good ability of robustness.\nOn the basis of uncertainty theory, we consider two extraction methods for uncertain\n\ninference rules by ant colony optimization algorithm. In the next section, we review the\nant colony optimization algorithm and give some basic concepts about uncertain sets.\nThen, we formulate a model to extract inference rules based on data set. And then, we\npropose an extraction method for uncertain inference rules by ant colony optimization\nalgorithm with a mutation operation. Finally, we combine the ant colony optimiza-\ntion algorithm with simulated annealing algorithm to speed up the extraction method.\nIn the last section, we discuss two typical classification problems in data mining with\nour results.\n\nPreliminary\nIn this section, we review the ant colony optimization algorithm. And then, we give some\nbasic concepts on uncertainty sets.\n\nAnt colony optimization algorithm\n\nAnt colony optimization algorithm, initiated by Dorigo, is a heuristic optimization\napproach. It simulates the behavior of real ants when they forage for food which relies on\nthe pheromone communication. In ant colony optimization algorithm, each path of artifi-\ncial ants walking from the food sources to the nest is a candidate solution to the problem.\nWhen walking on the path, the ants will release pheromone which evaporates over time.\nAnd the artificial ants will lay down more pheromone on the path corresponding to the\nbetter solution. While one ant has many paths to go, it will make a choice according to\nthe amount of the pheromone on the paths. The more pheromone there is on the path,\nthe better the solution is. As a result, bad paths will disappear since the pheromone evap-\norates over time. And good paths will be reserved since ants walking on it increases the\npheromone levels. Finally, one path which is used by most of the ants is left. Then, the\noptimal solution to the problem is obtained.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 4 of 19\n\nConsider the following optimization problem:\n\n⎧⎪⎪⎪⎨\n⎪⎪⎪⎩\nmin f (x)\ns.t.\n\ng(x) ≥ 0\nx ∈ D\n\n(1)\n\nwhere x is the decision variable in the domain D. And f (x) is the objective function while\ng(x) is the constraint function.\nWe can use ant colony optimization algorithm to obtain the optimal solution to the\n\nproblem (1). The parameters in the algorithm are initial pheromone τ0, ant transfer prob-\nability p, number of ants M, pheromone evaporation rate ρ, and number of iterations T .\nThe procedures are as follows.\n\nStep 1 Randomly generate a feasible solution x0 and set optimal solution s = x0. Initialize\nall pheromone trails with the same pheromone level τ0. Set k ← 0.\nStep 2 The artificial ant generates a walking path x in some probability p according to\n\nthe pheromone trails. If x ∈ D, then go to Step 3; otherwise, repeat Step 2 until x ∈ D.\nStep 3 Repeat Step 2 until for each ant and generate M feasible solutions. Let sk be the\n\nbest solution in this iteration.\nStep 4 If f (sk) < f (s), then s ← sk and update the pheromone trails according to the\n\noptimal solution in the current iteration.\nStep 5 If k < T , then k ← k + 1 and go to Step 2; otherwise, terminate.\nStep 6 Report the optimal solution.\n\nUncertain set\n\nLet � be a nonempty set and L be σ -algebra over �. Each � ∈ L is called an event. For\nany �, M{�} ∈ [0, 1]. The set function M defined on L is called an uncertain measure\nif it satisfies the following three axiom: M{�} = 1; M{�} + M{�c} = 1 for any � ∈ L;\nM\n\n{⋃∞\ni=1 �i\n\n} ≤ ∑∞\ni=1M{�i} for all �1,�2, · · · ∈ L. Then, the triplet (�,L,M) is called\n\nan uncertainty space [9]. The product uncertain measureM is an uncertain measure sat-\nisfying M\n\n{∏∞\ni=1 �k\n\n} = ∞∧\ni=1\n\nMk{�k}, where �k are arbitrarily chosen events from Lk for\nk = 1, 2, · · · , respectively.\n\nDefinition 1. [16] An uncertain set is a function ξ from an uncertainty space (�,L,M)\n\nto a collection of sets of real numbers such that both {B ⊂ ξ} and {ξ ⊂ B} are events for\nany Borel set B.\n\nExample 1. Take (�,L,M) to be {γ1, γ2, γ3} with power set L. Then, the set-valued\nfunction\n\nξ(γ ) =\n\n⎧⎪⎪⎨\n⎪⎪⎩\n[ 1, 3] , if γ = γ1\n\n[ 2, 4] , if γ = γ2\n\n[ 3, 5] , if γ = γ3\n\nis an uncertain set on (�,L,M).\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 5 of 19\n\nDefinition 2. [16] The uncertain sets ξ1, ξ2, ξ3, · · · , ξn are said to be independent if for\nany Borel sets B1,B2,B3, · · · ,Bn, we have\n\nM\n\n{ n⋂\ni=1\n\n(\nξ∗\ni ⊂ Bi\n\n)} =\nn∧\n\ni=1\nM\n\n{\nξ∗\ni ⊂ Bi\n\n}\nand\n\nM\n\n{ n⋃\ni=1\n\n(\nξ∗\ni ⊂ Bi\n\n)} =\nn∨\n\ni=1\nM\n\n{\nξ∗\ni ⊂ Bi\n\n}\n\nwhere ξ∗\ni are arbitrarily chosen from\n\n{\nξi, ξ ci\n\n}\n, i = 1, 2, · · · , n, respectively.\n\nDefinition 3. [21] An uncertain set ξ is said to have a membership function μ if for any\nBorel set B of real numbers, we have\n\nM{B ⊂ ξ} = inf\nx∈Bμ(x),M{ξ ⊂ B} = 1 − sup\n\nx∈Bc\nμ(x).\n\nThe above equations will be called measures inversion formulas.\n\nRemark 1. When an uncertain set ξ does have a membership function μ, it follows\nfrom the first measure inversion formula that\n\nμ(x) = M{x ∈ ξ}.\n\nExample 2. An uncertain set ξ is called triangular if it has a membership function\n\nμ(x) =\n⎧⎨\n⎩\n\nx−a\nb−a , a ≤ x ≤ b\n\nx−c\nb−c , b ≤ x ≤ c\n\n(2)\n\ndenoted by (a, b, c) where a, b, c are real numbers with a < b < c.\n\nDefinition 4. [21]Amembership functionμ is said to be regular if there exists a point x0\nsuch that μ(x0) = 1, and μ(x) is unimodal about the mode x0. That is, μ(x) is increasing\non (−∞, x0] and decreasing on [ x0,+∞).\n\nDefinition 5. [16] Let ξ be an uncertain set. Then, the expected value of ξ is defined by\n\nE[ ξ ]=\n∫ +∞\n\n0\nM{ξ \n r}dr −\n\n∫ 0\n\n−∞\nM{ξ � r}dr\n\nprovided that at least one of the two integrals is finite and\n\nM{ξ \n r} = 1\n2\n(M{ξ ≥ r} + 1 − M{ξ < r}),\n\nM{ξ � r} = 1\n2\n(M{ξ ≤ r} + 1 − M{ξ > r}).\n\nTheorem 1. [13] Let ξ be an uncertain set with regular membership function μ. Then\n\nE[ ξ ]= x0 + 1\n2\n\n∫ +∞\n\nx0\nμ(x)dx − 1\n\n2\n\n∫ x0\n\n−∞\nμ(x)dx, (3)\n\nwhere x0 is a point such that μ(x0) = 1.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 6 of 19\n\nExample 3. Let ξ be a triangular uncertain set denoted by (a, b, c). Then, according to\nTheorem 1, we have\n\nE[ ξ ]= a + 2b + c\n4\n\n.\n\nIn fact, it follows from Equations 2 and 3 that\n\nE[ ξ ] = b + 1\n2\n\n∫ c\n\nb\n\nx − c\nb − c\n\ndx − 1\n2\n\n∫ b\n\na\n\nx − a\nb − a\n\ndx\n\n= b − 1\n4\n(b − c) − 1\n\n4\n(b − a)\n\n= a + 2b + c\n4\n\n.\n\nUncertain inference rule\n\nHere, we introduce concepts of the uncertain inference and uncertain system. Inference\nrules are the key points of the inference systems. In fuzzy systems, CRI approach [22],\nMamdani inference rules [23] and Takagi-Sugeno inference rules [24] are the most com-\nmon used inference rules. Fuzzy if-then inference rules use fuzzy sets to describe the\nantecedents and the consequents. Unlike fuzzy inference, both antecedents and conse-\nquents in uncertain inference are characterized by uncertain sets. Uncertain inference\n[16] is a process of deriving consequences from human knowledge via uncertain set\ntheory. First, we introduce the following inference rule.\n\nInference Rule 1. [16] Let X and Y be two concepts. Assume a rule ‘if X is an uncertain\nset ξ , then Y is an uncertain set η’. From X is a constant a, we infer that Y is an uncertain\nset\n\nη∗ = η|a∈ξ\n\nwhich is the conditional uncertain set of η given a ∈ ξ . The inference rule is represented by\n\nRule: If X is ξ , then Y is η\n\nFrom: X is a constant a\n\nInfer: Y is η∗ = η|a∈ξ\n\nTheorem 2. [16] Let ξ and η be independent uncertain sets with membership functions\nμ and ν, respectively. If ξ∗ is a constant a, then the Inference Rule 1 yields that η∗ has a\nmembership function\n\nν∗(y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nν(y)\nμ(a) , if ν(y) <\n\nμ(a)\n2\n\nν(y)+μ(a)−1\nμ(a) , if ν(y) > 1 − μ(a)\n\n2\n\n0.5, otherwise.\n\nBased on Inference Rule 1, Gao et al. [18] proposed the multi-input, multi-if-then-rule\ninference rules.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 7 of 19\n\nInference Rule 2. [13] Let X1,X2, · · · ,Xm,Y be concepts. Assume rules ‘if X1 is ξi1\nand · · · and Xm is ξim, then Y is ηi’ for i = 1, 2, · · · , k. From X1 is a constant a1 and · · ·\nand Xm is a constant am, we infer that\n\nη∗ =\nk∑\n\ni=1\n\nci · ηi|(a1∈ξi1)∩(a2∈ξi2)∩···∩(am∈ξim)\n\nc1 + c2 + · · · + ck\n, (4)\n\nwhere the coefficients are determined by\n\nci = M{(a1 ∈ ξi1) ∩ (a2 ∈ ξi2) ∩ · · · ∩ (am ∈ ξim)}\nfor i = 1, 2, · · · , k. The inference rule is represented by\n\nRule 1: If X1 is ξ11 and · · · and Xm is ξ1m, then Y is η1\nRule 2: If X1 is ξ21 and · · · and Xm is ξ2m, then Y is η2\n\n· · ·\nRule k: If X1 is ξk1 and · · · and Xm is ξkm, then Y is ηk\nFrom: X1 is a1 and · · · and Xm is am\nInfer: Y is determined by Eq. (4)\n\nTheorem 3. [13] Assume ξi1, ξi2, · · · , ξim, ηi are independent uncertain sets with mem-\nbership functions μi1,μi2, · · · ,μim, νi, i = 1, 2, · · · , k, respectively. If ξ∗\n\n1 , ξ∗\n2 , · · · , ξ∗\n\nm are\nconstants a1, a2, · · · , am, respectively, then the Inference Rule 2 yields\n\nη∗ =\nk∑\n\ni=1\n\nci · η∗\ni\n\nc1 + c2 + · · · + ck\n\nwhere η∗\ni are uncertain sets whose membership functions are given by\n\nν∗\ni (y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνi(y)\nci , if νi(y) < ci\n\n2\n\nνi(y)+ci−1\nμ(a) , if νi(y) > 1 − ci\n\n2\n\n0.5, otherwise\n\nand ci = min\n1≤l≤m\n\nμil(al) are constants.\n\nUncertain system\n\nUncertain system, proposed by Liu [16], is a function from its inputs to outputs based\non the uncertain inference rule. Usually, an uncertain system consists of five parts: inputs\nthat are crisp data to be fed into the uncertain system; a rule-base that contains a set of\nif-then rules provided by the experts; an uncertain inference rule that infers uncertain\nconsequents from the uncertain antecedents; an expected value operator that converts\nthe uncertain consequents to crisp values; and outputs that are crisp data yielded from\nthe expected value operator.\nNow, we consider an uncertain system with m crisp inputs α1,α2, · · · ,αm, and n crisp\n\noutputs β1,β2, · · · ,βn. We have the following if-then rules:\n\nIf X1 is ξ11 and · · · and Xm is ξ1m, then Y1 is η11 and Y2 is η12 and · · · and Yn is η1n\nIf X1 is ξ21 and · · · and Xm is ξ2m, then Y1 is η21 and Y2 is η22 and · · · and Yn is η2n\n\n· · ·\nIf X1 is ξk1 and · · · and Xm is ξkm, then Y1 is ηk1 and Y2 is ηk2 and · · · and Yn is ηkn\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 8 of 19\n\nThus, according to Inference Rule 1 and 2, we can infer that Yj(j = 1, 2, · · · , n) are\n\nη∗\nj =\n\nk∑\ni=1\n\nci · ηij|(a1∈ξi1)∩(a2∈ξi2)∩···∩(am∈ξim)\n\nc1 + c2 + · · · + ck\n,\n\nwhere ci = M{(a1 ∈ ξi1)∩ (a2 ∈ ξi2)∩· · ·∩ (am ∈ ξim)} for i = 1, 2, · · · , k. Then, by using\nthe expected value operator, we obtain\n\nβj = E\n[\nη∗\nj\n\n]\nfor j = 1, 2, · · · , n. Now, we construct a function from crisp inputs α1,α2, · · · ,αm to crisp\noutputs β1,β2, · · · ,βn, i.e.,\n\n(β1,β2, · · · ,βn) = f (α1,α2, · · · ,αm).\n\nThen, we get an uncertain system f. For the uncertain system we proposed, we have the\nfollowing theorem.\n\nTheorem 4. [13] Assume that ξi1, ξi2, · · · , ξim and ηi1, ηi2, · · · , ηin are indepen-\ndent uncertain sets with membership functions μi1,μi2, · · · ,μim, νi1, νi2, · · · , νin, i =\n1, 2, · · · , k, respectively. Then, the uncertain system from α1,α2, · · · ,αm to β1,β2, · · · ,βn is\n\nbj =\nk∑\n\ni=1\n\nci · E[ η∗\nij]\n\nc1 + c2 + · · · + ck\n,\n\nwhere j = 1, 2, · · · , n and η∗\nij are uncertain sets whose membership functions are given by\n\nν∗\nij(y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνij(y)\nci , if νij(y) < ci\n\n2\n\nνij(y)+ci−1\nμ(a) , if νij(y) > 1 − ci\n\n2\n\n0.5, otherwise\n\nand ci = min\n1≤l≤m\n\nμil(al) are constants.\n\nNext, we discuss the expected value of a special triangular uncertain set.Without loss of\ngenerality, we assume n = 1. Then the uncertain system proposed in the above becomes:\n\nb =\nk∑\n\ni=1\n\nci · E[ η∗\ni ]\n\nc1 + c2 + · · · + ck\n, (5)\n\nν∗\ni (y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνi(y)\nci , if νi(y) < ci\n\n2\n\nνi(y)+ci−1\nμ(a) , if νi(y) > 1 − ci\n\n2\n\n0.5, otherwise,\n\n(6)\n\nci = min\n1≤l≤m\n\nμil(al). (7)\n\nTheorem 5. Assume we have an uncertain system with m inputs and 1 output consist-\ning of k inference rules. The antecedents of the rules are represented by the uncertain sets ξi\nwith membership functions μi1,μi2, · · · ,μim, i = 1, 2, · · · , k. And the consequent is repre-\nsented by an triangular uncertain set ηi = (αi,βi, γi) with a membership function νi, where\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 9 of 19\n\nthe coefficients satisfy\n\nαi + γi = 2βi, i = 1, 2, · · · , k. (8)\n\nWe have\n\nE\n[\nη∗\ni\n] = βi, i = 1, 2, · · · , k.\n\nProof. Given the m input data a1, a2, · · · , am, we can calculate ci from Equation 7.\nThen, we can get the membership functions ν∗\n\ni of the consequence uncertain sets η∗\ni\n\naccording to Equation 6. Next, the computation of the expected value of uncertain\nconsequence breaks into three cases.\nCase 1: Assume ci/2 = 0.5. We can immediately have ν∗\n\ni (y) = νi(y), thus\n\nE[ η∗\ni ]=\n\nαi + 2βi + γi\n4\n\n= βi.\n\nCase 2: Assume ci/2 < 0.5. Let yi11 and yi12\n(\nyi11 < yi12\n\n)\nbe the two points that satisfy\n\nthe equation νi(y) = ci/2. Similarly, yi21 and yi22\n(\nyi21 < yi22\n\n)\nsatisfy the equation νi(y) =\n\n1 − ci/2. Since the membership function of a triangular uncertain set has a symmetry\nproperty, we have\n\nyi11 + yi12 = 2βi, yi21 + yi22 = 2βi. (9)\n\nThen, we can rewrite the membership function of ηi as follows:\n\nν∗\ni =\n\n⎧⎪⎪⎪⎪⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎪⎪⎪⎪⎩\n\nνi(y)\nci , if αi ≤ y < yi11\n\nνi(y)+ci−1\nci , if yi21 ≤ y < yi22\n\nνi(y)\nci , if yi12 ≤ y < γi\n\n0.5, otherwise.\n\n(10)\n\nAnd ν∗\ni (βi) = 1. Together with Equations 3, 8, and 9, we have\n\nE[ η∗\ni ] = βi + 1\n\n2\n\n(∫ yi22\n\nβi\n\nνi(y) + ci − 1\nci\n\ndy +\n∫ yi12\n\nyi22\n0.5dy +\n\n∫ γi\n\nyi12\n\nνi(y)\nci\n\ndy\n)\n\n−1\n2\n\n(∫ βi\n\nyi21\n\nνi(y) + ci − 1\nci\n\ndy +\n∫ yi21\n\nyi11\n0.5dy +\n\n∫ yi11\n\nαi\n\nνi(y)\nci\n\ndy\n)\n\n= βi + 1\n2\n\n(∫ yi22\n\nβi\n\nνi(y) + ci − 1\nci\n\ndy −\n∫ βi\n\nyi21\n\nνi(y) + ci − 1\nci\n\ndy\n)\n\n+1\n2\n\n(∫ γi\n\nyi12\n\nνi(y)\nci\n\ndy −\n∫ yi11\n\nαi\n\nνi(y)\nci\n\ndy\n)\n\n+1\n2\n\n(∫ yi12\n\nyi22\n0.5dy −\n\n∫ yi21\n\nyi11\n0.5dy\n\n)\n\n= βi.\n\nCase 3: Assume ci > 0.5. Similarly, we have E[ η∗\ni ]= βi. Thus, we have proved the\n\ntheorem.\n\nProblem formulation\nIn this section, we propose an extraction model to obtain uncertain inference rules.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 10 of 19\n\nLet X = (x1, x2, · · · , xn) be the decision vector, which represents a rule base consisting\nof n rules. Each rule has m antecedents which are described by Q uncertain sets and one\nconsequent which is described by R uncertain sets. Each variable xi represents a sequence\nxi1xi2 · · · ximxim+1, where xij ∈ {0, 1, 2, · · · ,Q}(i = 1, 2, · · · , n; j = 1, 2, · · · ,m) represent\nthe antecedents of the inference rule. And xim+1 ∈ {0, 1, 2, · · · ,R}(i = 1, 2, · · · , n) repre-\nsent the consequent. Thus, each variable of decision vector represents one inference rule.\nSome xij = 0 means this antecedent is not included. And some xim+1 = 0 means this\ninference rule will not be included in the rule base. For example, assume that we have one\ninference rule consists of 4 antecedents and 1 consequent. They are described by 5 uncer-\ntain sets which refer to five descriptions: very low, low, medium, high, and very high. We\nuse 1, 2, 3, 4, 5 to denote them. Thus, sequence “23045”, for example, represents the rule:\n“if input 1 is low, input 2 is medium, and input 4 is high, then the output is very high”.\nUncertain systems can be used for classification. But which uncertain system is better\n\ndepends on the rule base. Here, we try to find best rule base by comparing the mean\nabsolute errors of the origin output and the system output. That is,\n\nMAE = 1\nP\n\nP∑\ni=1\n\n|oi − ti|, (11)\n\nwhere P is the number of training data, oi, ti(i = 1, 2, · · · ,P) are the system outputs and\norigin outputs, respectively. If we find the rule base with the least mean absolute error, we\nextract the uncertain inference rules successfully. We can obtain the system outputs by\nEquation 5. However, they may not be integers. To avoid this nonsense, for a classification\nproblem with C classes, we can divide interval that covers all the system outputs into C\nsubintervals. Then, if the output from Equation 5 is in the ith subinterval, we have oi = i.\nThus, we transfer the classification problem to the following optimization model:⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨\n\n⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩\n\nmin\nX\n\nF(X) = MAE\n\ns.t.\nX = (x1, x2, · · · , xn)\nxi = xi1 · · · ximxim+1\nxij ∈ {0, 1, · · · ,Q}\nxim+1 ∈ {0, 1, · · · ,R}\ni = 1, 2, · · · , n\nj = 1, 2, · · · ,m\n\nExtractionmethod for uncertain inference rules withmutations\nIn this section, we propose the extraction method for uncertain inference rules with\nmutations by ant colony optimization algorithm.\nAs stated before, each xi is a sequence of m values in {0, 1, 2, · · · ,Q} and 1 value in\n\n{0, 1, 2, · · · ,R}. Without loss of generality, we set Q = R. Each number in {0, 1, 2, · · · ,Q}\nis a node. Let ants walking across these nodes. Ants choose the next node in probability\nbased on the pheromone levels in the Q + 1 choices at every step. Once ants movem + 1\nsteps, a candidate decision variable is generated. After repeat this process n times, we get\na candidate solution. After all ants finish their walk, update the pheromone trails. Denote\nthe pheromone trail by τi;k,j(t) associated to the node j at step k of xi in iteration t. The\nprocedures are described as follows.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 11 of 19\n\n(1) Initialization: Randomly generate a feasible solutionX0, and set the optimal solution\nX̂ = X0. Set τi;k,j(0) = τ0, i = 1, 2, · · · , n, k = 1, 2, · · · ,m + 1, j = 0, 1, 2, · · · ,Q, where τ0\nis a fixed parameter.\n(2) Ant movement: At each step k after building the sequence xi1xi2 · · · xik , select the\n\nnext node in probability following\n\npk;k+1 = τi;k+1,j(t)\nQ∑\n\nq=0\nτi;k+1,q(t)\n\n. (12)\n\nIn this way, we could get a sequence xi1xi2 · · · xim+1. To speed up the algorithm, wemutate\nthis sequence to get a new candidate sequence. The mutation is made as follows: ran-\ndomly add 1 or subtract 1 to each element xij in the sequence; if the element is 0, the\nmutated element is 1; if the element is Q, the mutated element is Q − 1. Assume X ′ is\nthe mutated solution, if \rF = F(X ′\n\n) − F(X) ≤ 0, then X ← X ′ ; otherwise, keep the\ncurrent solution. If Q is very large, we could repeat this mutation until some termination\ncondition is satisfied.\n(3) Pheromone Update: At each iteration t, let X̂ be the optimal solution found so far\n\nand Xt be the best feasible solution in the current iteration. Assume F(X̂) and F(Xt) are\nthe corresponding objective function values.\n\nIf F(Xt) < F(X̂), then X̂ ← Xt .\nReinforce the pheromone trails on nodes of X̂ and evaporate the pheromone trails on\n\nthe left nodes:\n\nτi;j,k(t) =\n{\n\n(1 − ρ)τi;j,k(t − 1) + ρg(X̂), if (k, j) ∈ X̂\n(1 − ρ)τi;j,k(t − 1), otherwise\n\n(13)\n\nwhere ρ (0 < ρ < 1) is the evaporation rate, g(x)(0 < g(x) < +∞) is a function with that\ng(x) ≥ g(y) if F(x) < F(y), for example, g(x) = L/(|F(x)| + 1) is a function satisfying the\ncondition where L > 0.\n\nLet τ0 be the initial value of pheromone trails, n be the number of decision variables,\nM be the number of ants, ρ be evaporation rate and T be the number of iterations. Now,\nwe summarize the algorithm as follows.\n\nStep 1 Initialize all pheromone trails with the same pheromone level τ0. Randomly\ngenerate a feasible solution X0, and set optimal solution X̂ = X0. Set l ← 0.\nStep 2 Ant movement in probability following Equation 12. Generate a decision variable\n\nxi afterm + 1 steps.\nStep 3 Repeat Step 2 until X = (x1, x2, · · · , xn) is generated; mutate every xi: thus, gen-\n\nerate a new decision vector X ′ = (x′\n1, x\n\n′\n2, · · · , x\n\n′\nn); if \rF = F(X ′\n\n) − F(X) ≤ 0, then\nX ← X ′ .\nStep 4 Repeat Step 2 and Step 3 for allM ants.\nStep 5 Calculate the system outputs by Equation 5. Then, calculate the objective function\n\nvalues for the M candidate solutions by Equation 11. Denote the best solution in this\niteration by Xl.\nStep 6 If F(Xl) < F(X̂), then X̂ ← Xl; update the pheromone trails according to\n\nEquation 13.\nStep 7 l ← l + 1; if l = T , terminate; otherwise, go to Step 2.\nStep 8 Report the optimal solution X̂.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 12 of 19\n\nWith this algorithm above, we obtain an uncertain rule base. Then, we successfully\ndesign an uncertain system and can use it for classification.\n\nExtractionmethod for uncertain inference rules with SA\nIn the previous section, to speed up the algorithm, we introduce a mutation operation.\nHere, we introduce the simulated annealing algorithm as the local search operation.\nSimulated annealing algorithm was initiated by Metropolis in 1953, applied to portfolio\n\noptimization by Kirkpatrick [25] in 1983. The name and inspiration come from anneal-\ning in metallurgy, a technique involving heating and controlled cooling of a material to\nincrease the size of its crystals and reduce their defects. Simulated annealing algorithm is\nexcellent at avoiding getting stuck in local optimums. It has a good robust property and is\nuniversal and easy to implement.\nFor optimization problem (1), we can use simulated annealing algorithm to search for\n\nthe optimal solution. The algorithm is as follows.\n\nStep 1 Randomly generate a initial solution x0; x ← x0; k ← 0; t0 ← tmax(initial\ntemperature);\nStep 2 If the temperature satisfies the inner cycle termination criterion, go to Step 3;\n\notherwise, randomly choose a point x′ in the neighborhood N(x), calculate \rf = f (x′\n) −\n\nf (x). If \rf ≤ 0, then x ← x′ ; otherwise, according to Metropolis acceptance criterion, if\nexp(−\rf /tk) > random(0, 1), then x ← x′ . Repeat Step 2.\nStep 3 tk+1 = d(tk) (temperature decrease); k ← k + 1; if the termination criterion is\n\nsatisfied, stop and report the optimal solution; otherwise, go to Step 2.\n\nIn this section, we combine ant colony optimization algorithm and simulated annealing\nalgorithm. In each iteration of ant colony optimization algorithm, we get a feasible solu-\ntion. Then, we use it as the initial solution of the simulated annealing algorithm to get a\nneighbor solution. This neighbor solution will be accepted in probability. And for each\ndecision vector X = (x1, x2, · · · , xn), xi = xi1xi2 · · · xim+1, we build the neighbor solution\nas follows: for each xi, for some randomly generated p and q (1 ≤ p < q ≤ m), reverse the\norder of the sequence xip · · · xiq, i.e., x′\n\ni = xi1 · · · xip−1xiqxiq−1 · · · xip+1xipxiq+1 · · · xim+1.\nFor example, assume xi is 0123456, p = 2, q = 6, and the neighbor solution x′\n\ni is 0543216.\nIn this way, we obtain a neighbor solution X ′ . If \rF = F(X ′\n\n) − F(X) ≤ 0, X ← X ′ ;\notherwise, if exp(−\rF/tk) > random(0, 1), then X ← X ′ ; otherwise, abandon this neigh-\nbor solution. Still denote the pheromone trail by τi;k,j(t). The procedure are described as\nfollows.\n\n(1) Initialization: Generate a feasible solution X0 randomly and set the optimal solution\nX̂ = X0. Set τi;k,j(0) = τ0, i = 1, 2, · · · , n, k = 1, 2, · · · ,m + 1, j = 0, 1, 2, · · · ,Q, where τ0\nis a fixed parameter.\n(2) Ant movement: At each step k after building the sequence xi1xi2 · · · xik , select the\n\nnext node in probability following Equation 12. In this way, we could get a sequence\nxi1xi2 · · · xim+1. In order to expand the search range, we use simulated annealing algo-\nrithm to search locally around the solution at this step. Assume the neighbor solution is\nX ′ . If \rF = F(X ′\n\n) − F(X) ≤ 0, X ← X ′ ; otherwise, if exp(−\rF/tk) > random(0, 1)\nwhere tk is the current temperature and tk → 0 when k → ∞, then X ← X ′ ; otherwise,\nabandon this neighbor solution and still choose the original feasible solution.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 13 of 19\n\n(3) Pheromone Update: Let X̂ be the optimal solution found so far and Xt be the best\nfeasible solution in the current iteration t. Assume F(X̂) and F(Xt) are",
      "metadata_storage_size": 707841,
      "metadata_storage_last_modified": "2023-10-25T02:11:42Z",
      "metadata_storage_name": "s40467-015-0033-9.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9rbm1pbmluZzRzdG9yYWdlYWNjb3VudC5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXJzL3M0MDQ2Ny0wMTUtMDAzMy05LnBkZg2",
      "metadata_author": null,
      "metadata_title": null,
      "metadata_creation_date": "2015-05-16T18:35:45Z",
      "people": [
        "Chen",
        "Ling Chen",
        "Yun Sun",
        "Yuanguo Zhu",
        "Herrera",
        "Zhu",
        "nacy",
        "Liu",
        "Peng",
        "Yao",
        "Gao",
        "Dorigo",
        "M",
        "Borel",
        "Mamdani",
        "Takagi",
        "Sugeno",
        "yi22",
        "yi21",
        "yi11",
        "xik",
        "Kirkpatrick",
        "anneal"
      ],
      "organizations": [
        "School of Science",
        "University of Science",
        "Technology",
        "Springer",
        "ACOA",
        "MAE",
        "ximxim+1",
        "SA",
        "Metropolis"
      ],
      "locations": [
        "Nanjing",
        "China",
        "nest",
        "path",
        "Metropolis"
      ],
      "keyphrases": [
        "Creative Commons Attribution License",
        "interesting, potentially useful patterns",
        "Ant colony optimization algorithm",
        "modern intelligent optimization methods",
        "current data analysis tools",
        "important practical significance",
        "Open Access article",
        "many successful examples",
        "many different specifiedmethods",
        "machine learning methods",
        "Uncertain inference rule",
        "RESEARCH Open Access",
        "Bayesian belief network",
        "large data sets",
        "financial data analysis",
        "data compartment analysis",
        "many different fields",
        "data mining techniques",
        "data mining algorithms",
        "mated data generation",
        "data mining methods",
        "data mining technology",
        "collection tools",
        "inference rules",
        "many methods",
        "many other",
        "Uncertainty Analysis",
        "neutral network",
        "Bayesian classification",
        "uncertain system",
        "tomany fields",
        "mining objects",
        "Extraction methods",
        "statistical methods",
        "effective methods",
        "Data classification",
        "existing data",
        "data description",
        "data dependency",
        "data regression",
        "data aggregate",
        "data prediction",
        "training data",
        "Rules extraction",
        "classification rules",
        "Yun Sun",
        "Yuanguo Zhu",
        "recent years",
        "classification problems",
        "computer networks",
        "scientific research",
        "ness decision",
        "computational process",
        "overall goal",
        "standable structure",
        "biomedical research",
        "main jobs",
        "decision trees",
        "mathematical formula",
        "original work",
        "Nanjing University",
        "business management",
        "valuable information",
        "two applications",
        "science research",
        "unrestricted use",
        "Ling Chen",
        "Journal",
        "DOI",
        "Correspondence",
        "School",
        "China",
        "Abstract",
        "increasing",
        "attention",
        "paper",
        "effectiveness",
        "superiority",
        "Keywords",
        "Introduction",
        "databases",
        "finance",
        "E-commerce",
        "logistics",
        "result",
        "amount",
        "people",
        "basis",
        "difficulty",
        "depth",
        "deficiency",
        "concepts",
        "laws",
        "modes",
        "knowledge",
        "reality",
        "instance",
        "telecommunication",
        "retail",
        "study",
        "couple",
        "models",
        "functions",
        "characteristics",
        "categories",
        "output",
        "licensee",
        "Springer",
        "terms",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "Page",
        "variety",
        "approaches",
        "start",
        "addition",
        "mutation ant colony optimization algorithm",
        "original ant colony optimization algorithm",
        "early decision trees algorithm",
        "simulated annealing algorithm",
        "global search algorithm",
        "powerful mathematical tool",
        "uncertain differential equation",
        "mature technology methods",
        "Decision trees method",
        "fuzzy rules learning",
        "many mathematical tools",
        "massive data set",
        "data mining technique",
        "neutral network algorithm",
        "product measure axiom",
        "decision tree method",
        "incremental tree structure",
        "intelligent optimization algorithms",
        "different network models",
        "random initial solutions",
        "Machine learning methods",
        "popular classification methods",
        "complex nonlinear relations",
        "classification machine learning",
        "many evolution algorithms",
        "Genetic algorithm",
        "optimization strategies",
        "bionic optimization",
        "numerical methods",
        "conceptual learning",
        "inductive learning",
        "ID3 method",
        "C4.5 method",
        "different types",
        "many cases",
        "spatial data",
        "normality axiom",
        "duality axiom",
        "subadditivity axiom",
        "pattern classification",
        "statistical principle",
        "tering problems",
        "core content",
        "simulation model",
        "complex system",
        "pattern recognition",
        "biological evolution",
        "genetic mechanism",
        "important role",
        "Mixed algorithms",
        "other algorithms",
        "slow convergence",
        "local optimums",
        "Hybrid genetic",
        "hybrid particle",
        "real world",
        "human being",
        "domain experts",
        "belief degree",
        "Many researchers",
        "theoretical work",
        "uniqueness theorem",
        "practical problems",
        "uncertainty theory",
        "probability theory",
        "number",
        "variants",
        "SLIQ",
        "clustering",
        "regression",
        "behavior",
        "ant-miner",
        "Herrera",
        "weakness",
        "reason",
        "Zhu",
        "ACOA",
        "MACO",
        "performance",
        "understanding",
        "indeterminacy",
        "samples",
        "situation",
        "choice",
        "event",
        "unlikely",
        "frequency",
        "view",
        "Liu",
        "Chen",
        "Applications",
        "lot",
        "existence",
        "stability",
        "Peng",
        "Yao",
        "two typical classification problems",
        "ant colony optimization algorithm",
        "uncertain optimal control model",
        "heuristic optimization approach",
        "two extraction methods",
        "ant transfer prob",
        "option pricing models",
        "expected value operator",
        "M feasible solutions",
        "following optimization problem",
        "uncertain inference rule",
        "pheromone evaporation rate",
        "same pheromone level",
        "optimal solution s",
        "uncertain inference controller",
        "one ant",
        "artificial ant",
        "uncertain systems",
        "uncertain sets",
        "membership functions",
        "five parts",
        "universal approximator",
        "reasonable tool",
        "inverted pendulum",
        "mutation operation",
        "data mining",
        "uncertainty sets",
        "decision variable",
        "domain D",
        "candidate solution",
        "best solution",
        "pheromone communication",
        "pheromone levels",
        "initial pheromone",
        "pheromone trails",
        "many paths",
        "bad paths",
        "good paths",
        "next section",
        "basic concepts",
        "last section",
        "objective function",
        "constraint function",
        "good ability",
        "data set",
        "food sources",
        "real ants",
        "cial ants",
        "one path",
        "walking path",
        "stocks",
        "inputs",
        "outputs",
        "rule-base",
        "Gao",
        "robustness",
        "results",
        "Preliminary",
        "Dorigo",
        "nest",
        "time",
        "parameters",
        "iterations",
        "procedures",
        "Step",
        "probability",
        "sk",
        "5",
        "⎪⎪⎪",
        "first measure inversion formula",
        "following three axiom",
        "product uncertain measureM",
        "Mamdani inference rules",
        "Takagi-Sugeno inference rules",
        "triangular uncertain set",
        "regular membership function μ",
        "inference systems",
        "fuzzy inference",
        "nonempty set",
        "optimal solution",
        "current iteration",
        "uncertainty space",
        "real numbers",
        "expected value",
        "two integrals",
        "key points",
        "fuzzy systems",
        "CRI approach",
        "human knowledge",
        "set function",
        "Amembership functionμ",
        "fuzzy sets",
        "Mk{�k",
        "M{ξ � r",
        "M{B ⊂",
        "B.",
        "⊂ B",
        "algebra",
        "L.",
        "triplet",
        "Lk",
        "Definition",
        "collection",
        "Borel",
        "Example",
        "power",
        "ξn",
        "Bi",
        "sup",
        "equations",
        "formulas",
        "Remark",
        "mode",
        "Theorem",
        "x0",
        "fact",
        "2b",
        "antecedents",
        "consequents",
        "process",
        "consequences",
        "σ",
        "∈",
        "∑",
        "∞",
        "⎪",
        "γ",
        "1",
        "∫",
        "independent uncertain sets",
        "conditional uncertain set",
        "m crisp inputs",
        "crisp data",
        "crisp values",
        "Uncertain system",
        "uncertain antecedents",
        "uncertain consequents",
        "two concepts",
        "constant a",
        "theory",
        "following",
        "X1",
        "Xm",
        "im",
        "coefficients",
        "Eq.",
        "ηi",
        "constants",
        "k∑",
        "min",
        "μil",
        "experts",
        "Y1",
        "Y2",
        "Yn",
        "Yj",
        "βj",
        "ξ",
        "ν",
        "η∗",
        "⎪⎪⎪⎪",
        "1 μ",
        "special triangular uncertain set",
        "m input data",
        "dent uncertain sets",
        "Q uncertain sets",
        "R uncertain sets",
        "consequence uncertain sets",
        "uncertain inference rules",
        "k inference rules",
        "one inference rule",
        "uncertain consequence",
        "m inputs",
        "three cases",
        "two points",
        "symmetry property",
        "Problem formulation",
        "decision vector",
        "rule base",
        "n rules",
        "m antecedents",
        "variable xi",
        "k.",
        "βn",
        "μim",
        "bj",
        "ij",
        "loss",
        "generality",
        "above",
        "1 output",
        "ing",
        "αi",
        "γi",
        "2βi",
        "Proof",
        "Equation",
        "computation",
        "dy",
        "section",
        "model",
        "example",
        "corresponding objective function values",
        "following optimization model",
        "candidate decision variable",
        "Q + 1 choices",
        "best feasible solution",
        "mean absolute error",
        "best rule base",
        "new candidate sequence",
        "m values",
        "absolute errors",
        "feasible solutionX0",
        "Uncertain systems",
        "mutated solution",
        "current solution",
        "five descriptions",
        "system outputs",
        "origin outputs",
        "C classes",
        "ith subinterval",
        "The procedures",
        "fixed parameter",
        "evaporation rate",
        "classification problem",
        "next node",
        "mutated element",
        "step k",
        "left nodes",
        "Once ants",
        "Q∑",
        "4 antecedents",
        "sets",
        "input",
        "MAE",
        "1 P",
        "P∑",
        "integers",
        "nonsense",
        "subintervals",
        "xij",
        "Extractionmethod",
        "mutations",
        "1 value",
        "R.",
        "steps",
        "walk",
        "Denote",
        "τi",
        "xik",
        "way",
        "termination",
        "condition",
        "ρg",
        "⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪",
        "inner cycle termination criterion",
        "M candidate solutions",
        "good robust property",
        "uncertain rule base",
        "local search operation",
        "Metropolis acceptance criterion",
        "Step 2 Ant movement",
        "new decision vector",
        "Simulated annealing algorithm",
        "portfolio optimization",
        "optimization problem",
        "decision variables",
        "initial value",
        "feasible solution",
        "controlled cooling",
        "initial solution",
        "neighbor solution",
        "previous section",
        "Repeat Step",
        "ants",
        "X0",
        "1 steps",
        "values",
        "Xl.",
        "classification",
        "Kirkpatrick",
        "name",
        "inspiration",
        "metallurgy",
        "technique",
        "heating",
        "material",
        "size",
        "crystals",
        "defects",
        "temperature",
        "point",
        "neighborhood",
        "order",
        "sequence",
        "procedure",
        "original feasible solution",
        "search range",
        "current temperature",
        "F(X̂",
        "Set",
        "step",
        "rithm",
        "tk",
        "τ"
      ],
      "language": "en",
      "masked_text": "\n**** et al. Journal of Uncertainty Analysis and Applications  (****) 3:9 \nDOI 10.1186/s40467-015-0033-9\n\nRESEARCH Open Access\n\nExtraction methods for uncertain inference\nrules by ant colony optimization\n*********, ******** and ***********\n\n*Correspondence:\n***********************\n*****************, Nanjing\n********************* and\nTechnology, Nanjing ******, China\n\nAbstract\n\nIn recent years, the research on data mining methods has received increasing\nattention. In this paper, we design an uncertain system with the extracted uncertain\ninference rules to solve the classification problems in data mining. And then, two\nextraction methods integrated with ant colony optimization are proposed for the\ngeneration of the uncertain inference rules. Finally, two applications are given to verify\nthe effectiveness and superiority of the proposed methods.\n\nKeywords: Uncertain inference rule; Uncertain system; Ant colony optimization\nalgorithm; Rules extraction; Data classification\n\nIntroduction\nNowadays, databases and computer networks, coupled with the use of advanced auto-\nmated data generation and collection tools, are widely used in many different fields such\nas finance, E-commerce, logistics, etc. As a result, the amount of data that people have\nto deal with is dramatically increasing. People hope to carry out scientific research, busi-\nness decision, or business management on the basis of the analysis of the existing data.\nHowever, the current data analysis tools have difficulty in processing the data in depth.\nTo compensate for this deficiency, there come the data mining techniques. Data mining is\nthe computational process of discovering some interesting, potentially useful patterns in\nlarge data sets. Those patterns can be concepts, rules, laws, and modes. The overall goal\nof data mining is to extract information from a data set and transform it into an under-\nstandable structure for further use. Data mining helps us to discover valuable information\nand knowledge. Data mining is applied tomany fields in reality. There are many successful\nexamples [1] of data mining in business and science research. For instance, data mining is\nwidely used in financial data analysis, telecommunication, retail, and biomedical research.\nTherefore, the study of data mining technology has an important practical significance.\nThe main jobs of data mining are data description, data classification, data dependency,\n\ndata compartment analysis, data regression, data aggregate, and data prediction. What\ndata classification does is to find a couple of models or functions that can accurately\ndescribe the characteristics of the data sets. Then, we can identify the categories of the\n********** unknown data. After obtaining themodels or functions from the set of training\ndata with data mining algorithms, we use many methods to describe the output such as\nclassification rules (if-then), decision trees, mathematical formula, and neutral network.\n\n© **** **** et al.; licensee ********. This is an Open Access article distributed under the terms of the Creative Commons\nAttribution License (*******************************************, which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work is properly credited.\n\nmailto: ***********************\n******************************************\n\n\n**** et al. Journal of Uncertainty Analysis and Applications  (****) 3:9 Page 2 of 19\n\nThere are a variety of approaches in data mining. For mining objects in different fields,\nmany different specifiedmethods are invented. The approaches we usually used are statis-\ntical methods, machine learning methods, and modern intelligent optimization methods.\nThe statistical methods are very effective methods from the start. In addition, many other\ndata mining methods are invented based on the statistical methods. When dealing with\nclassification problems, Bayesian classification and Bayesian belief network are important\nclassification methods that based on the statistical principle. Machine learning methods\nare mainly used to solve the conceptual learning, pattern classification, and pattern clus-\ntering problems. The core content of machine learning is inductive learning. And there\nalready exist a number of mature technology methods, such as decision tree method for\nclassification problems. Decision trees method is one of the most popular classification\nmethods. The early decision trees algorithm is ID3 method. Later, based on ID3, many\nalgorithms such as C4.5 method [2] are proposed. Besides, there are some variants of the\ndecision trees algorithm including incremental tree structure ID4, ID5, and expandable\ntree structure SLIQ for massive data set.\nIn recent years, intelligent optimization algorithms are widely applied into data min-\n\ning. Neutral network is a simulation model for complex system with nonlinear relations.\nIt is very suitable to deal with complex nonlinear relations in spatial data. Researchers\nhave already proposed different network models to realize the clustering, classification,\nregression, and pattern recognition of the data. Furthermore, many evolution algorithms\nsuch as simulated annealing algorithm are introduced into neutral network algorithm\nas the optimization strategies. Genetic algorithm is a global search algorithm that sim-\nulates the biological evolution and genetic mechanism. It plays an important role in\noptimization and classification machine learning. Mixed algorithms of genetic algorithm\nand other algorithms, such as decision trees, neutral network, have been applied to the\ndata mining technology. Ant colony optimization algorithm is a bionic optimization algo-\nrithm that simulates the behavior of the ants. Based on that, a data mining technique\nant-miner [3] was invented. And ******* [4] applied it to fuzzy rules learning. How-\never, ant colony optimization algorithm has some weakness such as slow convergence,\nrandom initial solutions. For this reason, some improved ant colony optimization algo-\nrithms are proposed. *** proposed an improved ant colony optimization algorithm\n(****) [5] and a mutation ant colony optimization algorithm (MACO) [6] to speed up\nthe algorithms and avoid the solutions getting stuck in local optimums. Hybrid genetic\nant colony optimization [7] and hybrid particle swarm ant colony optimization algo-\nrithm [8] significantly improve the performance of the original ant colony optimization\nalgorithm.\nThe real world is so complex that human being *** face different types of indetermi-\n\nnacy ********. To get a better understanding of the real world, many mathematical tools\nare created. One of them is probability theory which is used to model indeterminacy from\nsamples. However, in many cases, no samples are available to estimate a probability distri-\nbution. In this situation, we have no choice but to invite some domain experts to evaluate\nthe belief degree that each event *** occur. We cannot use probability theory to deal\nwith belief degree since human beings usually overweight unlikely events which makes\nthe belief degrees deviate far from the frequency. In view of this, *** [9] founded uncer-\ntainty theory based on normality axiom, duality axiom, subadditivity axiom, and product\nmeasure axiom. It has become a powerful mathematical tool dealing with indeterminacy.\n\n\n\n**** et al. Journal of Uncertainty Analysis and Applications  (****) 3:9 Page 3 of 19\n\nMany *********** have done a lot of theoretical work related to uncertainty theory. In\n****, *** [10] presented the uncertain differential equation. Later, the existence and\nuniqueness theorem was given [11]. And the stability of uncertain differential equation\nwas discussed [12,13]. Also, some analysis and numerical methods for solving uncertain\ndifferential equation were proposed. With uncertain differential equation describing the\nevolution of the system, we may solve some practical problems. **** and *** [14] stud-\nied an option pricing models for stocks. *** [15] proposed an uncertain optimal control\nmodel in ****.\nIn [16,17], *** proposed and studied the uncertain systems based on the concepts of\n\nuncertain sets, membership functions, and uncertain inference rules. An uncertain sys-\ntem is a function from its inputs to outputs based on the uncertain inference rule. Usually,\nan uncertain system consists of five parts: inputs, rule-base, uncertain inference rules,\nexpected value ********, and outputs. Following that, *** et **. [18] generalized uncertain\ninference rules and described uncertain systems with them. **** and **** [19] proved\nthat uncertain systems are universal approximator and then demonstrated that the uncer-\ntain controller is a reasonable tool. Gao [20] designed an uncertain inference controller\nthat successfully balanced an inverted pendulum with 5 × 5 if-then rules. What is more\nimportant is that this uncertain inference controller has a good ability of robustness.\nOn the basis of uncertainty theory, we consider two extraction methods for uncertain\n\ninference rules by ant colony optimization algorithm. In the next section, we review the\nant colony optimization algorithm and give some basic concepts about uncertain sets.\nThen, we formulate a model to extract inference rules based on data set. And then, we\npropose an extraction method for uncertain inference rules by ant colony optimization\nalgorithm with a mutation operation. Finally, we combine the ant colony optimiza-\ntion algorithm with simulated annealing algorithm to speed up the extraction method.\nIn the last section, we discuss two typical classification problems in data mining with\nour results.\n\nPreliminary\nIn this section, we review the ant colony optimization algorithm. And then, we give some\nbasic concepts on uncertainty sets.\n\nAnt colony optimization algorithm\n\nAnt colony optimization algorithm, initiated by ******, is a heuristic optimization\napproach. It simulates the behavior of real ants when they forage for food which relies on\nthe pheromone communication. In ant colony optimization algorithm, each path of artifi-\ncial ants walking from the food sources to the nest is a candidate solution to the problem.\nWhen walking on the path, the ants will release pheromone which evaporates over time.\nAnd the artificial ants will lay down more pheromone on the path corresponding to the\nbetter solution. While one ant has many paths to go, it will make a choice according to\nthe amount of the pheromone on the paths. The more pheromone there is on the path,\nthe better the solution is. As a result, bad paths will disappear since the pheromone evap-\norates over time. And good paths will be reserved since ants walking on it increases the\npheromone levels. Finally, one path which is used by most of the ants is left. Then, the\noptimal solution to the problem is obtained.\n\n\n\n**** et al. Journal of Uncertainty Analysis and Applications  (****) 3:9 Page 4 of 19\n\nConsider the following optimization problem:\n\n⎧⎪⎪⎪⎨\n⎪⎪⎪⎩\nmin f (x)\ns.t.\n\ng(x) ≥ 0\nx ∈ D\n\n(1)\n\nwhere x is the decision variable in the domain D. And f (x) is the objective function while\ng(x) is the constraint function.\nWe can use ant colony optimization algorithm to obtain the optimal solution to the\n\nproblem (1). The parameters in the algorithm are initial pheromone τ0, ant transfer prob-\nability p, number of ants M, pheromone evaporation rate ρ, and number of iterations T .\nThe procedures are as follows.\n\nStep 1 Randomly generate a feasible solution x0 and set optimal solution s = x0. Initialize\nall pheromone trails with the same pheromone level τ0. Set k ← 0.\nStep 2 The artificial ant generates a walking path x in some probability p according to\n\nthe pheromone trails. If x ∈ D, then go to Step 3; otherwise, repeat Step 2 until x ∈ D.\nStep 3 Repeat Step 2 until for each ant and generate M feasible solutions. Let sk be the\n\nbest solution in this iteration.\nStep 4 If f (sk) < f (s), then s ← sk and update the pheromone trails according to the\n\noptimal solution in the current iteration.\nStep 5 If k < T , then k ← k + 1 and go to Step 2; otherwise, terminate.\nStep 6 Report the optimal solution.\n\nUncertain set\n\nLet � be a nonempty set and L be σ -algebra over �. Each � ∈ L is called an event. For\nany �, M{�} ∈ [0, 1]. The set function M defined on L is called an uncertain measure\nif it satisfies the following three axiom: M{�} = 1; M{�} + M{�c} = 1 for any � ∈ L;\nM\n\n{⋃∞\ni=1 �i\n\n} ≤ ∑∞\ni=1M{�i} for all �1,�2, · · · ∈ L. Then, the triplet (�,L,M) is called\n\nan uncertainty space [9]. The product uncertain measureM is an uncertain measure sat-\nisfying M\n\n{∏∞\ni=1 �k\n\n} = ∞∧\ni=1\n\nMk{�k}, where �k are arbitrarily chosen events from Lk for\nk = 1, 2, · · · , respectively.\n\nDefinition 1. [16] An uncertain set is a function ξ from an uncertainty space (�,L,M)\n\nto a collection of sets of real numbers such that both {B ⊂ ξ} and {ξ ⊂ B} are events for\nany ***** set B.\n\nExample 1. Take (�,L,M) to be {γ1, γ2, γ3} with power set L. Then, the set-valued\nfunction\n\nξ(γ ) =\n\n⎧⎪⎪⎨\n⎪⎪⎩\n[ 1, 3] , if γ = γ1\n\n[ 2, 4] , if γ = γ2\n\n[ 3, 5] , if γ = γ3\n\nis an uncertain set on (�,L,M).\n\n\n\n**** et al. Journal of Uncertainty Analysis and Applications  (****) 3:9 Page 5 of 19\n\nDefinition 2. [16] The uncertain sets ξ1, ξ2, ξ3, · · · , ξn are said to be independent if for\nany Borel sets B1,B2,B3, · · · ,Bn, we have\n\nM\n\n{ n⋂\ni=1\n\n(\nξ∗\ni ⊂ Bi\n\n)} =\nn∧\n\ni=1\nM\n\n{\nξ∗\ni ⊂ Bi\n\n}\nand\n\nM\n\n{ n⋃\ni=1\n\n(\nξ∗\ni ⊂ Bi\n\n)} =\nn∨\n\ni=1\nM\n\n{\nξ∗\ni ⊂ Bi\n\n}\n\nwhere ξ∗\ni are arbitrarily chosen from\n\n{\nξi, ξ ci\n\n}\n, i = 1, 2, · · · , n, respectively.\n\nDefinition 3. [21] An uncertain set ξ is said to have a membership function μ if for any\n***** set B of real numbers, we have\n\nM{B ⊂ ξ} = inf\nx∈Bμ(x),M{ξ ⊂ B} = 1 − sup\n\nx∈Bc\nμ(x).\n\nThe above equations will be called measures inversion formulas.\n\nRemark 1. When an uncertain set ξ does have a membership function μ, it follows\nfrom the first measure inversion formula that\n\nμ(x) = M{x ∈ ξ}.\n\nExample 2. An uncertain set ξ is called triangular if it has a membership function\n\nμ(x) =\n⎧⎨\n⎩\n\nx−a\nb−a , a ≤ x ≤ b\n\nx−c\nb−c , b ≤ x ≤ c\n\n(2)\n\ndenoted by (a, b, c) where a, b, c are real numbers with a < b < c.\n\nDefinition 4. [21]Amembership functionμ is said to be regular if there exists a point x0\nsuch that μ(x0) = 1, and μ(x) is unimodal about the mode x0. That is, μ(x) is increasing\non (−∞, x0] and decreasing on [ x0,+∞).\n\nDefinition 5. [16] Let ξ be an uncertain set. Then, the expected value of ξ is defined by\n\nE[ ξ ]=\n∫ +∞\n\n0\nM{ξ \n r}dr −\n\n∫ 0\n\n−∞\nM{ξ � r}dr\n\nprovided that at least one of the two integrals is finite and\n\nM{ξ \n r} = 1\n2\n(M{ξ ≥ r} + 1 − M{ξ < r}),\n\nM{ξ � r} = 1\n2\n(M{ξ ≤ r} + 1 − M{ξ > r}).\n\nTheorem 1. [13] Let ξ be an uncertain set with regular membership function μ. Then\n\nE[ ξ ]= x0 + 1\n2\n\n∫ +∞\n\nx0\nμ(x)dx − 1\n\n2\n\n∫ x0\n\n−∞\nμ(x)dx, (3)\n\nwhere x0 is a point such that μ(x0) = 1.\n\n\n\n**** et al. Journal of Uncertainty Analysis and Applications  (****) 3:9 Page 6 of 19\n\nExample 3. Let ξ be a triangular uncertain set denoted by (a, b, c). Then, according to\nTheorem 1, we have\n\nE[ ξ ]= a + 2b + c\n4\n\n.\n\nIn fact, it follows from Equations 2 and 3 that\n\nE[ ξ ] = b + 1\n2\n\n∫ c\n\nb\n\nx − c\nb − c\n\ndx − 1\n2\n\n∫ b\n\na\n\nx − a\nb − a\n\ndx\n\n= b − 1\n4\n(b − c) − 1\n\n4\n(b − a)\n\n= a + 2b + c\n4\n\n.\n\nUncertain inference rule\n\nHere, we introduce concepts of the uncertain inference and uncertain system. Inference\nrules are the key points of the inference systems. In fuzzy systems, CRI approach [22],\n******* inference rules [23] and ******-Sugeno inference rules [24] are the most com-\n*** used inference rules. Fuzzy if-then inference rules use fuzzy sets to describe the\nantecedents and the consequents. Unlike fuzzy inference, both antecedents and conse-\nquents in uncertain inference are characterized by uncertain sets. Uncertain inference\n[16] is a process of deriving consequences from human knowledge via uncertain set\ntheory. First, we introduce the following inference rule.\n\nInference Rule 1. [16] Let X and Y be two concepts. Assume a rule ‘if X is an uncertain\nset ξ , then Y is an uncertain set η’. From X is a constant a, we infer that Y is an uncertain\nset\n\nη∗ = η|a∈ξ\n\nwhich is the conditional uncertain set of η given a ∈ ξ . The inference rule is represented by\n\nRule: If X is ξ , then Y is η\n\nFrom: X is a constant a\n\nInfer: Y is η∗ = η|a∈ξ\n\nTheorem 2. [16] Let ξ and η be independent uncertain sets with membership functions\nμ and ν, respectively. If ξ∗ is a constant a, then the Inference Rule 1 yields that η∗ has a\nmembership function\n\nν∗(y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nν(y)\nμ(a) , if ν(y) <\n\nμ(a)\n2\n\nν(y)+μ(a)−1\nμ(a) , if ν(y) > 1 − μ(a)\n\n2\n\n0.5, otherwise.\n\nBased on Inference Rule 1, *** et al. [18] proposed the multi-input, multi-if-then-rule\ninference rules.\n\n\n\n**** et al. Journal of Uncertainty Analysis and Applications  (****) 3:9 Page 7 of 19\n\nInference Rule 2. [13] Let X1,X2, · · · ,Xm,Y be concepts. Assume rules ‘if X1 is ξi1\nand · · · and Xm is ξim, then Y is ηi’ for i = 1, 2, · · · , *. From X1 is a constant a1 and · · ·\nand Xm is a constant am, we infer that\n\nη∗ =\nk∑\n\ni=1\n\nci · ηi|(a1∈ξi1)∩(a2∈ξi2)∩···∩(am∈ξim)\n\nc1 + c2 + · · · + ck\n, (4)\n\nwhere the coefficients are determined by\n\nci = M{(a1 ∈ ξi1) ∩ (a2 ∈ ξi2) ∩ · · · ∩ (am ∈ ξim)}\nfor i = 1, 2, · · · , *. The inference rule is represented by\n\nRule 1: If X1 is ξ11 and · · · and Xm is ξ1m, then Y is η1\nRule 2: If X1 is ξ21 and · · · and Xm is ξ2m, then Y is η2\n\n· · ·\nRule k: If X1 is ξk1 and · · · and Xm is ξkm, then Y is ηk\nFrom: X1 is a1 and · · · and Xm is am\nInfer: Y is determined by Eq. (4)\n\nTheorem 3. [13] Assume ξi1, ξi2, · · · , ξim, ηi are independent uncertain sets with mem-\nbership functions μi1,μi2, · · · ,μim, νi, i = 1, 2, · · · , k, respectively. If ξ∗\n\n1 , ξ∗\n2 , · · · , ξ∗\n\nm are\nconstants a1, a2, · · · , am, respectively, then the Inference Rule 2 yields\n\nη∗ =\nk∑\n\ni=1\n\nci · η∗\ni\n\nc1 + c2 + · · · + ck\n\nwhere η∗\ni are uncertain sets whose membership functions are given by\n\nν∗\ni (y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνi(y)\nci , if νi(y) < ci\n\n2\n\nνi(y)+ci−1\nμ(a) , if νi(y) > 1 − ci\n\n2\n\n0.5, otherwise\n\nand ci = min\n1≤l≤m\n\nμil(al) are constants.\n\nUncertain system\n\nUncertain system, proposed by *** [16], is a function from its inputs to outputs based\non the uncertain inference rule. Usually, an uncertain system consists of five parts: inputs\nthat are crisp data to be fed into the uncertain system; a rule-base that contains a set of\nif-then rules provided by the *******; an uncertain inference rule that infers uncertain\nconsequents from the uncertain antecedents; an expected value operator that converts\nthe uncertain consequents to crisp values; and outputs that are crisp data yielded from\nthe expected value operator.\nNow, we consider an uncertain system with m crisp inputs α1,α2, · · · ,αm, and n crisp\n\noutputs β1,β2, · · · ,βn. We have the following if-then rules:\n\nIf X1 is ξ11 and · · · and Xm is ξ1m, then Y1 is η11 and Y2 is η12 and · · · and Yn is η1n\nIf X1 is ξ21 and · · · and Xm is ξ2m, then Y1 is η21 and Y2 is η22 and · · · and Yn is η2n\n\n· · ·\nIf X1 is ξk1 and · · · and Xm is ξkm, then Y1 is ηk1 and Y2 is ηk2 and · · · and Yn is ηkn\n\n\n\n**** et al. Journal of Uncertainty Analysis and Applications  (****) 3:9 Page 8 of 19\n\nThus, according to Inference Rule 1 and 2, we can infer that Yj(j = 1, 2, · · · , n) are\n\nη∗\nj =\n\nk∑\ni=1\n\nci · ηij|(a1∈ξi1)∩(a2∈ξi2)∩···∩(am∈ξim)\n\nc1 + c2 + · · · + ck\n,\n\nwhere ci = M{(a1 ∈ ξi1)∩ (a2 ∈ ξi2)∩· · ·∩ (am ∈ ξim)} for i = 1, 2, · · · , *. Then, by using\nthe expected value operator, we obtain\n\nβj = E\n[\nη∗\nj\n\n]\nfor j = 1, 2, · · · , n. ***, we construct a function from crisp inputs α1,α2, · · · ,αm to crisp\noutputs β1,β2, · · · ,βn, i.e.,\n\n(β1,β2, · · · ,βn) = f (α1,α2, · · · ,αm).\n\nThen, we get an uncertain system f. For the uncertain system we proposed, we have the\nfollowing theorem.\n\nTheorem 4. [13] Assume that ξi1, ξi2, · · · , ξim and ηi1, ηi2, · · · , ηin are indepen-\ndent uncertain sets with membership functions μi1,μi2, · · · ,μim, νi1, νi2, · · · , νin, i =\n1, 2, · · · , k, respectively. Then, the uncertain system from α1,α2, · · · ,αm to β1,β2, · · · ,βn is\n\nbj =\nk∑\n\ni=1\n\nci · E[ η∗\nij]\n\nc1 + c2 + · · · + ck\n,\n\nwhere j = 1, 2, · · · , n and η∗\nij are uncertain sets whose membership functions are given by\n\nν∗\nij(y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνij(y)\nci , if νij(y) < ci\n\n2\n\nνij(y)+ci−1\nμ(a) , if νij(y) > 1 − ci\n\n2\n\n0.5, otherwise\n\nand ci = min\n1≤l≤m\n\nμil(al) are constants.\n\nNext, we discuss the expected value of a special triangular uncertain set.Without loss of\ngenerality, we assume n = 1. Then the uncertain system proposed in the above becomes:\n\nb =\nk∑\n\ni=1\n\nci · E[ η∗\ni ]\n\nc1 + c2 + · · · + ck\n, (5)\n\nν∗\ni (y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνi(y)\nci , if νi(y) < ci\n\n2\n\nνi(y)+ci−1\nμ(a) , if νi(y) > 1 − ci\n\n2\n\n0.5, otherwise,\n\n(6)\n\nci = min\n1≤l≤m\n\nμil(al). (7)\n\nTheorem 5. Assume we have an uncertain system with m inputs and 1 output consist-\ning of k inference rules. The antecedents of the rules are represented by the uncertain sets ξi\nwith membership functions μi1,μi2, · · · ,μim, i = 1, 2, · · · , *. And the consequent is repre-\nsented by an triangular uncertain set ηi = (αi,βi, γi) with a membership function νi, where\n\n\n\n**** et al. Journal of Uncertainty Analysis and Applications  (****) 3:9 Page 9 of 19\n\nthe coefficients satisfy\n\nαi + γi = 2βi, i = 1, 2, · · · , k. (8)\n\nWe have\n\nE\n[\nη∗\ni\n] = βi, i = 1, 2, · · · , *.\n\nProof. Given the m input data a1, a2, · · · , am, we can calculate ci from Equation 7.\nThen, we can get the membership functions ν∗\n\ni of the consequence uncertain sets η∗\ni\n\naccording to Equation 6. Next, the computation of the expected value of uncertain\nconsequence breaks into three cases.\nCase 1: Assume ci/2 = 0.5. We can immediately have ν∗\n\ni (y) = νi(y), thus\n\nE[ η∗\ni ]=\n\nαi + 2βi + γi\n4\n\n= βi.\n\nCase 2: Assume ci/2 < 0.5. Let yi11 and yi12\n(\nyi11 < yi12\n\n)\nbe the two points that satisfy\n\nthe equation νi(y) = ci/2. Similarly, yi21 and yi22\n(\nyi21 < yi22\n\n)\nsatisfy the equation νi(y) =\n\n1 − ci/2. Since the membership function of a triangular uncertain set has a symmetry\nproperty, we have\n\nyi11 + yi12 = 2βi, yi21 + yi22 = 2βi. (9)\n\nThen, we can rewrite the membership function of ηi as follows:\n\nν∗\ni =\n\n⎧⎪⎪⎪⎪⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎪⎪⎪⎪⎩\n\nνi(y)\nci , if αi ≤ y < yi11\n\nνi(y)+ci−1\nci , if yi21 ≤ y < yi22\n\nνi(y)\nci , if yi12 ≤ y < γi\n\n0.5, otherwise.\n\n(10)\n\nAnd ν∗\ni (βi) = 1. Together with Equations 3, 8, and 9, we have\n\nE[ η∗\ni ] = βi + 1\n\n2\n\n(∫ yi22\n\nβi\n\nνi(y) + ci − 1\nci\n\ndy +\n∫ yi12\n\nyi22\n0.5dy +\n\n∫ γi\n\nyi12\n\nνi(y)\nci\n\ndy\n)\n\n−1\n2\n\n(∫ βi\n\nyi21\n\nνi(y) + ci − 1\nci\n\ndy +\n∫ yi21\n\nyi11\n0.5dy +\n\n∫ yi11\n\nαi\n\nνi(y)\nci\n\ndy\n)\n\n= βi + 1\n2\n\n(∫ yi22\n\nβi\n\nνi(y) + ci − 1\nci\n\ndy −\n∫ βi\n\nyi21\n\nνi(y) + ci − 1\nci\n\ndy\n)\n\n+1\n2\n\n(∫ γi\n\nyi12\n\nνi(y)\nci\n\ndy −\n∫ yi11\n\nαi\n\nνi(y)\nci\n\ndy\n)\n\n+1\n2\n\n(∫ yi12\n\nyi22\n0.5dy −\n\n∫ yi21\n\nyi11\n0.5dy\n\n)\n\n= βi.\n\nCase 3: Assume ci > 0.5. Similarly, we have E[ η∗\ni ]= βi. Thus, we have proved the\n\ntheorem.\n\nProblem formulation\nIn this section, we propose an extraction model to obtain uncertain inference rules.\n\n\n\n**** et al. Journal of Uncertainty Analysis and Applications  (****) 3:9 Page 10 of 19\n\nLet X = (x1, x2, · · · , xn) be the decision vector, which represents a rule base consisting\nof n rules. Each rule has m antecedents which are described by Q uncertain sets and one\nconsequent which is described by R uncertain sets. Each variable xi represents a sequence\nxi1xi2 · · · ximxim+1, where xij ∈ {0, 1, 2, · · · ,Q}(i = 1, 2, · · · , n; j = 1, 2, · · · ,m) represent\nthe antecedents of the inference rule. And xim+1 ∈ {0, 1, 2, · · · ,R}(i = 1, 2, · · · , n) repre-\nsent the consequent. Thus, each variable of decision vector represents one inference rule.\nSome xij = 0 means this antecedent is not included. And some xim+1 = 0 means this\ninference rule will not be included in the rule base. For example, assume that we have one\ninference rule consists of 4 antecedents and 1 consequent. They are described by 5 uncer-\ntain sets which refer to five descriptions: very low, low, medium, high, and very high. We\nuse 1, 2, 3, 4, 5 to denote them. Thus, sequence “23045”, for example, represents the rule:\n“if input 1 is low, input 2 is medium, and input 4 is high, then the output is very high”.\nUncertain systems can be used for classification. But which uncertain system is better\n\ndepends on the rule base. Here, we try to find best rule base by comparing the mean\nabsolute errors of the origin output and the system output. That is,\n\n*** *****\n\nP∑\ni=1\n\n|oi − ti|, (11)\n\nwhere P is the number of training data, oi, ti(i = 1, 2, · · · ,P) are the system outputs and\norigin outputs, respectively. If we find the rule base with the least mean absolute error, we\nextract the uncertain inference rules successfully. We can obtain the system outputs by\nEquation 5. However, they may not be integers. To avoid this nonsense, for a classification\nproblem with C classes, we can divide interval that covers all the system outputs into C\nsubintervals. Then, if the output from Equation 5 is in the ith subinterval, we have oi = i.\nThus, we transfer the classification problem to the following optimization model:⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨\n\n⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩\n\nmin\nX\n\nF(X) = MAE\n\ns.t.\nX = (x1, x2, · · · , xn)\nxi = xi1 · · · ximxim+1\nxij ∈ {0, 1, · · · ,Q}\nxim+1 ∈ {0, 1, · · · ,R}\ni = 1, 2, · · · , n\nj = 1, 2, · · · ,m\n\nExtractionmethod for uncertain inference rules withmutations\nIn this section, we propose the extraction method for uncertain inference rules with\nmutations by ant colony optimization algorithm.\nAs stated before, each xi is a sequence of m values in {0, 1, 2, · · · ,Q} and 1 value in\n\n{0, 1, 2, · · · ,R}. Without loss of generality, we set Q = R. Each number in {0, 1, 2, · · · ,Q}\nis a node. Let ants walking across these nodes. Ants choose the next node in probability\nbased on the pheromone levels in the Q + 1 choices at every step. Once ants movem + 1\nsteps, a ********* decision variable is generated. After repeat this process n times, we get\na candidate solution. After all ants finish their walk, update the pheromone trails. Denote\nthe pheromone trail by τi;k,j(t) associated to the node j at step k of xi in iteration t. The\nprocedures are described as follows.\n\n\n\n**** et al. Journal of Uncertainty Analysis and Applications  (****) 3:9 Page 11 of 19\n\n(1) Initialization: Randomly generate a feasible solutionX0, and set the optimal solution\nX̂ = X0. Set τi;k,j(0) = τ0, i = 1, 2, · · · , n, k = 1, 2, · · · ,m + 1, j = 0, 1, 2, · · · ,Q, where τ0\nis a fixed parameter.\n(2) Ant movement: At each step k after building the sequence xi1xi2 · · · xik , select the\n\nnext node in probability following\n\npk;k+1 = τi;k+1,j(t)\nQ∑\n\nq=0\nτi;k+1,q(t)\n\n. (12)\n\nIn this way, we could get a sequence xi1xi2 · · · xim+1. To speed up the algorithm, wemutate\nthis sequence to get a new candidate sequence. The mutation is made as follows: ran-\ndomly add 1 or subtract 1 to each element xij in the sequence; if the element is 0, the\nmutated element is 1; if the element is Q, the mutated element is Q − 1. Assume X ′ is\nthe mutated solution, if \rF = F(X ′\n\n) − F(X) ≤ 0, then X ← X ′ ; otherwise, keep the\ncurrent solution. If Q is very large, we could repeat this mutation until some termination\ncondition is satisfied.\n(3) Pheromone Update: At each iteration t, let X̂ be the optimal solution found so far\n\nand Xt be the best feasible solution in the current iteration. Assume F(X̂) and F(Xt) are\nthe corresponding objective function values.\n\nIf F(Xt) < F(X̂), then X̂ ← Xt .\nReinforce the pheromone trails on nodes of X̂ and evaporate the pheromone trails on\n\nthe left nodes:\n\nτi;j,k(t) =\n{\n\n(1 − ρ)τi;j,k(t − 1) + ρg(X̂), if (k, j) ∈ X̂\n(1 − ρ)τi;j,k(t − 1), otherwise\n\n(13)\n\nwhere ρ (0 < ρ < 1) is the evaporation rate, g(x)(0 < g(x) < +∞) is a function with that\ng(x) ≥ g(y) if F(x) < F(y), for example, g(x) = L/(|F(x)| + 1) is a function satisfying the\ncondition where L > 0.\n\nLet τ0 be the initial value of pheromone trails, n be the number of decision variables,\nM be the number of ants, ρ be evaporation rate and T be the number of iterations. Now,\nwe summarize the algorithm as follows.\n\nStep 1 Initialize all pheromone trails with the same pheromone level τ0. Randomly\ngenerate a feasible solution X0, and set optimal solution X̂ = X0. Set l ← 0.\nStep 2 Ant movement in probability following Equation 12. Generate a decision variable\n\nxi afterm + 1 steps.\nStep 3 Repeat Step 2 until X = (x1, x2, · · · , xn) is generated; mutate every xi: thus, gen-\n\nerate a new decision vector X ′ = (x′\n1, x\n\n′\n2, · · · , x\n\n′\nn); if \rF = F(X ′\n\n) − F(X) ≤ 0, then\nX ← X ′ .\nStep 4 Repeat Step 2 and Step 3 for allM ants.\nStep 5 Calculate the system outputs by Equation 5. Then, calculate the objective function\n\nvalues for the M candidate solutions by Equation 11. Denote the best solution in this\niteration byXl*.\nStep 6 If F(Xl) < F(X̂), then X̂ ← Xl; update the pheromone trails according to\n\nEquation 13.\nStep 7 l ← l + 1; if l = T , terminate; otherwise, go to Step 2.\nStep 8 Report the optimal solution X̂.\n\n\n\n**** et al. Journal of Uncertainty Analysis and Applications  (****) 3:9 Page 12 of 19\n\nWith this algorithm above, we obtain an uncertain rule base. Then, we successfully\ndesign an uncertain system and can use it for classification.\n\nExtractionmethod for uncertain inference rules with **\nIn the previous section, to speed up the algorithm, we introduce a mutation operation.\nHere, we introduce the simulated annealing algorithm as the local search operation.\nSimulated annealing algorithm was initiated by ********** in ****, applied to portfolio\n\noptimization by *********** [25] in ****. The name and inspiration come from anneal-\ning in metallurgy, a technique involving heating and controlled cooling of a material to\nincrease the size of its crystals and reduce their defects. Simulated annealing algorithm is\nexcellent at avoiding getting stuck in local optimums. It has a good robust property and is\nuniversal and easy to implement.\nFor optimization problem (1), we can use simulated annealing algorithm to search for\n\nthe optimal solution. The algorithm is as follows.\n\nStep 1 Randomly generate a initial solution x0; x ← x0; k ← 0; t0 ← tmax(initial\ntemperature);\nStep 2 If the temperature satisfies the inner cycle termination criterion, go to Step 3;\n\notherwise, randomly choose a point x′ in the neighborhood N(x), calculate \rf = f (x′\n) −\n\nf (x). If \rf ≤ 0, then x ← x′ ; otherwise, according to Metropolis acceptance criterion, if\nexp(−\rf /tk) > random(0, 1), then x ← x′ . Repeat Step 2.\nStep 3 tk+1 = d(tk) (temperature decrease); k ← k + 1; if the termination criterion is\n\nsatisfied, stop and report the optimal solution; otherwise, go to Step 2.\n\nIn this section, we combine ant colony optimization algorithm and simulated annealing\nalgorithm. In each iteration of ant colony optimization algorithm, we get a feasible solu-\ntion. Then, we use it as the initial solution of the simulated annealing algorithm to get a\nneighbor solution. This neighbor solution will be accepted in probability. And for each\ndecision vector X = (x1, x2, · · · , xn), xi = xi1xi2 · · · xim+1, we build the neighbor solution\nas follows: for each xi, for some randomly generated p and q (1 ≤ p < q ≤ m), reverse the\norder of the sequence xip · · · xiq, i.e., x′\n\ni = xi1 · · · xip−1xiqxiq−1 · · · xip+1xipxiq+1 · · · xim+1.\nFor example, assume xi is *******, p = 2, q = 6, and the neighbor solution x′\n\ni is *******.\nIn this way, we obtain a neighbor solution X ′ . If \rF = F(X ′\n\n) − F(X) ≤ 0, X ← X ′ ;\notherwise, if exp(−\rF/tk) > random(0, 1), then X ← X ′ ; otherwise, abandon this neigh-\nbor solution. Still denote the pheromone trail by τi;k,j(t). The procedure are described as\nfollows.\n\n(1) Initialization: Generate a feasible solution X0 randomly and set the optimal solution\nX̂ = X0. Set τi;k,j(0) = τ0, i = 1, 2, · · · , n, k = 1, 2, · · · ,m + 1, j = 0, 1, 2, · · · ,Q, where τ0\nis a fixed parameter.\n(2) Ant movement: At each step k after building the sequence xi1xi2 · · · xik , select the\n\nnext node in probability following Equation 12. In this way, we could get a sequence\nxi1xi2 · · · xim+1. In order to expand the search range, we use simulated annealing algo-\nrithm to search locally around the solution at this step. Assume the neighbor solution is\nX ′ . If \rF = F(X ′\n\n) − F(X) ≤ 0, X ← X ′ ; otherwise, if exp(−\rF/tk) > random(0, 1)\nwhere tk is the current temperature and tk → 0 when k → ∞, then X ← X ′ ; otherwise,\nabandon this neighbor solution and still choose the original feasible solution.\n\n\n\n**** et al. Journal of Uncertainty Analysis and Applications  (****) 3:9 Page 13 of 19\n\n(3) Pheromone Update: Let X̂ be the optimal solution found so far and Xt be the best\nfeasible solution in the current iteration t. Assume F(X̂) and F(Xt) are",
      "merged_content": "\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 \nDOI 10.1186/s40467-015-0033-9\n\nRESEARCH Open Access\n\nExtraction methods for uncertain inference\nrules by ant colony optimization\nLing Chen, Yun Sun* and Yuanguo Zhu\n\n*Correspondence:\nchinalsy_881220@163.com\nSchool of Science, Nanjing\nUniversity of Science and\nTechnology, Nanjing 210094, China\n\nAbstract\n\nIn recent years, the research on data mining methods has received increasing\nattention. In this paper, we design an uncertain system with the extracted uncertain\ninference rules to solve the classification problems in data mining. And then, two\nextraction methods integrated with ant colony optimization are proposed for the\ngeneration of the uncertain inference rules. Finally, two applications are given to verify\nthe effectiveness and superiority of the proposed methods.\n\nKeywords: Uncertain inference rule; Uncertain system; Ant colony optimization\nalgorithm; Rules extraction; Data classification\n\nIntroduction\nNowadays, databases and computer networks, coupled with the use of advanced auto-\nmated data generation and collection tools, are widely used in many different fields such\nas finance, E-commerce, logistics, etc. As a result, the amount of data that people have\nto deal with is dramatically increasing. People hope to carry out scientific research, busi-\nness decision, or business management on the basis of the analysis of the existing data.\nHowever, the current data analysis tools have difficulty in processing the data in depth.\nTo compensate for this deficiency, there come the data mining techniques. Data mining is\nthe computational process of discovering some interesting, potentially useful patterns in\nlarge data sets. Those patterns can be concepts, rules, laws, and modes. The overall goal\nof data mining is to extract information from a data set and transform it into an under-\nstandable structure for further use. Data mining helps us to discover valuable information\nand knowledge. Data mining is applied tomany fields in reality. There are many successful\nexamples [1] of data mining in business and science research. For instance, data mining is\nwidely used in financial data analysis, telecommunication, retail, and biomedical research.\nTherefore, the study of data mining technology has an important practical significance.\nThe main jobs of data mining are data description, data classification, data dependency,\n\ndata compartment analysis, data regression, data aggregate, and data prediction. What\ndata classification does is to find a couple of models or functions that can accurately\ndescribe the characteristics of the data sets. Then, we can identify the categories of the\npreviously unknown data. After obtaining themodels or functions from the set of training\ndata with data mining algorithms, we use many methods to describe the output such as\nclassification rules (if-then), decision trees, mathematical formula, and neutral network.\n\n© 2015 Chen et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons\nAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work is properly credited.\n\nmailto: chinalsy_881220@163.com\nhttp://creativecommons.org/licenses/by/4.0\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 2 of 19\n\nThere are a variety of approaches in data mining. For mining objects in different fields,\nmany different specifiedmethods are invented. The approaches we usually used are statis-\ntical methods, machine learning methods, and modern intelligent optimization methods.\nThe statistical methods are very effective methods from the start. In addition, many other\ndata mining methods are invented based on the statistical methods. When dealing with\nclassification problems, Bayesian classification and Bayesian belief network are important\nclassification methods that based on the statistical principle. Machine learning methods\nare mainly used to solve the conceptual learning, pattern classification, and pattern clus-\ntering problems. The core content of machine learning is inductive learning. And there\nalready exist a number of mature technology methods, such as decision tree method for\nclassification problems. Decision trees method is one of the most popular classification\nmethods. The early decision trees algorithm is ID3 method. Later, based on ID3, many\nalgorithms such as C4.5 method [2] are proposed. Besides, there are some variants of the\ndecision trees algorithm including incremental tree structure ID4, ID5, and expandable\ntree structure SLIQ for massive data set.\nIn recent years, intelligent optimization algorithms are widely applied into data min-\n\ning. Neutral network is a simulation model for complex system with nonlinear relations.\nIt is very suitable to deal with complex nonlinear relations in spatial data. Researchers\nhave already proposed different network models to realize the clustering, classification,\nregression, and pattern recognition of the data. Furthermore, many evolution algorithms\nsuch as simulated annealing algorithm are introduced into neutral network algorithm\nas the optimization strategies. Genetic algorithm is a global search algorithm that sim-\nulates the biological evolution and genetic mechanism. It plays an important role in\noptimization and classification machine learning. Mixed algorithms of genetic algorithm\nand other algorithms, such as decision trees, neutral network, have been applied to the\ndata mining technology. Ant colony optimization algorithm is a bionic optimization algo-\nrithm that simulates the behavior of the ants. Based on that, a data mining technique\nant-miner [3] was invented. And Herrera [4] applied it to fuzzy rules learning. How-\never, ant colony optimization algorithm has some weakness such as slow convergence,\nrandom initial solutions. For this reason, some improved ant colony optimization algo-\nrithms are proposed. Zhu proposed an improved ant colony optimization algorithm\n(ACOA) [5] and a mutation ant colony optimization algorithm (MACO) [6] to speed up\nthe algorithms and avoid the solutions getting stuck in local optimums. Hybrid genetic\nant colony optimization [7] and hybrid particle swarm ant colony optimization algo-\nrithm [8] significantly improve the performance of the original ant colony optimization\nalgorithm.\nThe real world is so complex that human being may face different types of indetermi-\n\nnacy everyday. To get a better understanding of the real world, many mathematical tools\nare created. One of them is probability theory which is used to model indeterminacy from\nsamples. However, in many cases, no samples are available to estimate a probability distri-\nbution. In this situation, we have no choice but to invite some domain experts to evaluate\nthe belief degree that each event may occur. We cannot use probability theory to deal\nwith belief degree since human beings usually overweight unlikely events which makes\nthe belief degrees deviate far from the frequency. In view of this, Liu [9] founded uncer-\ntainty theory based on normality axiom, duality axiom, subadditivity axiom, and product\nmeasure axiom. It has become a powerful mathematical tool dealing with indeterminacy.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 3 of 19\n\nMany researchers have done a lot of theoretical work related to uncertainty theory. In\n2008, Liu [10] presented the uncertain differential equation. Later, the existence and\nuniqueness theorem was given [11]. And the stability of uncertain differential equation\nwas discussed [12,13]. Also, some analysis and numerical methods for solving uncertain\ndifferential equation were proposed. With uncertain differential equation describing the\nevolution of the system, we may solve some practical problems. Peng and Yao [14] stud-\nied an option pricing models for stocks. Zhu [15] proposed an uncertain optimal control\nmodel in 2010.\nIn [16,17], Liu proposed and studied the uncertain systems based on the concepts of\n\nuncertain sets, membership functions, and uncertain inference rules. An uncertain sys-\ntem is a function from its inputs to outputs based on the uncertain inference rule. Usually,\nan uncertain system consists of five parts: inputs, rule-base, uncertain inference rules,\nexpected value operator, and outputs. Following that, Gao et al. [18] generalized uncertain\ninference rules and described uncertain systems with them. Peng and Chen [19] proved\nthat uncertain systems are universal approximator and then demonstrated that the uncer-\ntain controller is a reasonable tool. Gao [20] designed an uncertain inference controller\nthat successfully balanced an inverted pendulum with 5 × 5 if-then rules. What is more\nimportant is that this uncertain inference controller has a good ability of robustness.\nOn the basis of uncertainty theory, we consider two extraction methods for uncertain\n\ninference rules by ant colony optimization algorithm. In the next section, we review the\nant colony optimization algorithm and give some basic concepts about uncertain sets.\nThen, we formulate a model to extract inference rules based on data set. And then, we\npropose an extraction method for uncertain inference rules by ant colony optimization\nalgorithm with a mutation operation. Finally, we combine the ant colony optimiza-\ntion algorithm with simulated annealing algorithm to speed up the extraction method.\nIn the last section, we discuss two typical classification problems in data mining with\nour results.\n\nPreliminary\nIn this section, we review the ant colony optimization algorithm. And then, we give some\nbasic concepts on uncertainty sets.\n\nAnt colony optimization algorithm\n\nAnt colony optimization algorithm, initiated by Dorigo, is a heuristic optimization\napproach. It simulates the behavior of real ants when they forage for food which relies on\nthe pheromone communication. In ant colony optimization algorithm, each path of artifi-\ncial ants walking from the food sources to the nest is a candidate solution to the problem.\nWhen walking on the path, the ants will release pheromone which evaporates over time.\nAnd the artificial ants will lay down more pheromone on the path corresponding to the\nbetter solution. While one ant has many paths to go, it will make a choice according to\nthe amount of the pheromone on the paths. The more pheromone there is on the path,\nthe better the solution is. As a result, bad paths will disappear since the pheromone evap-\norates over time. And good paths will be reserved since ants walking on it increases the\npheromone levels. Finally, one path which is used by most of the ants is left. Then, the\noptimal solution to the problem is obtained.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 4 of 19\n\nConsider the following optimization problem:\n\n⎧⎪⎪⎪⎨\n⎪⎪⎪⎩\nmin f (x)\ns.t.\n\ng(x) ≥ 0\nx ∈ D\n\n(1)\n\nwhere x is the decision variable in the domain D. And f (x) is the objective function while\ng(x) is the constraint function.\nWe can use ant colony optimization algorithm to obtain the optimal solution to the\n\nproblem (1). The parameters in the algorithm are initial pheromone τ0, ant transfer prob-\nability p, number of ants M, pheromone evaporation rate ρ, and number of iterations T .\nThe procedures are as follows.\n\nStep 1 Randomly generate a feasible solution x0 and set optimal solution s = x0. Initialize\nall pheromone trails with the same pheromone level τ0. Set k ← 0.\nStep 2 The artificial ant generates a walking path x in some probability p according to\n\nthe pheromone trails. If x ∈ D, then go to Step 3; otherwise, repeat Step 2 until x ∈ D.\nStep 3 Repeat Step 2 until for each ant and generate M feasible solutions. Let sk be the\n\nbest solution in this iteration.\nStep 4 If f (sk) < f (s), then s ← sk and update the pheromone trails according to the\n\noptimal solution in the current iteration.\nStep 5 If k < T , then k ← k + 1 and go to Step 2; otherwise, terminate.\nStep 6 Report the optimal solution.\n\nUncertain set\n\nLet � be a nonempty set and L be σ -algebra over �. Each � ∈ L is called an event. For\nany �, M{�} ∈ [0, 1]. The set function M defined on L is called an uncertain measure\nif it satisfies the following three axiom: M{�} = 1; M{�} + M{�c} = 1 for any � ∈ L;\nM\n\n{⋃∞\ni=1 �i\n\n} ≤ ∑∞\ni=1M{�i} for all �1,�2, · · · ∈ L. Then, the triplet (�,L,M) is called\n\nan uncertainty space [9]. The product uncertain measureM is an uncertain measure sat-\nisfying M\n\n{∏∞\ni=1 �k\n\n} = ∞∧\ni=1\n\nMk{�k}, where �k are arbitrarily chosen events from Lk for\nk = 1, 2, · · · , respectively.\n\nDefinition 1. [16] An uncertain set is a function ξ from an uncertainty space (�,L,M)\n\nto a collection of sets of real numbers such that both {B ⊂ ξ} and {ξ ⊂ B} are events for\nany Borel set B.\n\nExample 1. Take (�,L,M) to be {γ1, γ2, γ3} with power set L. Then, the set-valued\nfunction\n\nξ(γ ) =\n\n⎧⎪⎪⎨\n⎪⎪⎩\n[ 1, 3] , if γ = γ1\n\n[ 2, 4] , if γ = γ2\n\n[ 3, 5] , if γ = γ3\n\nis an uncertain set on (�,L,M).\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 5 of 19\n\nDefinition 2. [16] The uncertain sets ξ1, ξ2, ξ3, · · · , ξn are said to be independent if for\nany Borel sets B1,B2,B3, · · · ,Bn, we have\n\nM\n\n{ n⋂\ni=1\n\n(\nξ∗\ni ⊂ Bi\n\n)} =\nn∧\n\ni=1\nM\n\n{\nξ∗\ni ⊂ Bi\n\n}\nand\n\nM\n\n{ n⋃\ni=1\n\n(\nξ∗\ni ⊂ Bi\n\n)} =\nn∨\n\ni=1\nM\n\n{\nξ∗\ni ⊂ Bi\n\n}\n\nwhere ξ∗\ni are arbitrarily chosen from\n\n{\nξi, ξ ci\n\n}\n, i = 1, 2, · · · , n, respectively.\n\nDefinition 3. [21] An uncertain set ξ is said to have a membership function μ if for any\nBorel set B of real numbers, we have\n\nM{B ⊂ ξ} = inf\nx∈Bμ(x),M{ξ ⊂ B} = 1 − sup\n\nx∈Bc\nμ(x).\n\nThe above equations will be called measures inversion formulas.\n\nRemark 1. When an uncertain set ξ does have a membership function μ, it follows\nfrom the first measure inversion formula that\n\nμ(x) = M{x ∈ ξ}.\n\nExample 2. An uncertain set ξ is called triangular if it has a membership function\n\nμ(x) =\n⎧⎨\n⎩\n\nx−a\nb−a , a ≤ x ≤ b\n\nx−c\nb−c , b ≤ x ≤ c\n\n(2)\n\ndenoted by (a, b, c) where a, b, c are real numbers with a < b < c.\n\nDefinition 4. [21]Amembership functionμ is said to be regular if there exists a point x0\nsuch that μ(x0) = 1, and μ(x) is unimodal about the mode x0. That is, μ(x) is increasing\non (−∞, x0] and decreasing on [ x0,+∞).\n\nDefinition 5. [16] Let ξ be an uncertain set. Then, the expected value of ξ is defined by\n\nE[ ξ ]=\n∫ +∞\n\n0\nM{ξ \n r}dr −\n\n∫ 0\n\n−∞\nM{ξ � r}dr\n\nprovided that at least one of the two integrals is finite and\n\nM{ξ \n r} = 1\n2\n(M{ξ ≥ r} + 1 − M{ξ < r}),\n\nM{ξ � r} = 1\n2\n(M{ξ ≤ r} + 1 − M{ξ > r}).\n\nTheorem 1. [13] Let ξ be an uncertain set with regular membership function μ. Then\n\nE[ ξ ]= x0 + 1\n2\n\n∫ +∞\n\nx0\nμ(x)dx − 1\n\n2\n\n∫ x0\n\n−∞\nμ(x)dx, (3)\n\nwhere x0 is a point such that μ(x0) = 1.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 6 of 19\n\nExample 3. Let ξ be a triangular uncertain set denoted by (a, b, c). Then, according to\nTheorem 1, we have\n\nE[ ξ ]= a + 2b + c\n4\n\n.\n\nIn fact, it follows from Equations 2 and 3 that\n\nE[ ξ ] = b + 1\n2\n\n∫ c\n\nb\n\nx − c\nb − c\n\ndx − 1\n2\n\n∫ b\n\na\n\nx − a\nb − a\n\ndx\n\n= b − 1\n4\n(b − c) − 1\n\n4\n(b − a)\n\n= a + 2b + c\n4\n\n.\n\nUncertain inference rule\n\nHere, we introduce concepts of the uncertain inference and uncertain system. Inference\nrules are the key points of the inference systems. In fuzzy systems, CRI approach [22],\nMamdani inference rules [23] and Takagi-Sugeno inference rules [24] are the most com-\nmon used inference rules. Fuzzy if-then inference rules use fuzzy sets to describe the\nantecedents and the consequents. Unlike fuzzy inference, both antecedents and conse-\nquents in uncertain inference are characterized by uncertain sets. Uncertain inference\n[16] is a process of deriving consequences from human knowledge via uncertain set\ntheory. First, we introduce the following inference rule.\n\nInference Rule 1. [16] Let X and Y be two concepts. Assume a rule ‘if X is an uncertain\nset ξ , then Y is an uncertain set η’. From X is a constant a, we infer that Y is an uncertain\nset\n\nη∗ = η|a∈ξ\n\nwhich is the conditional uncertain set of η given a ∈ ξ . The inference rule is represented by\n\nRule: If X is ξ , then Y is η\n\nFrom: X is a constant a\n\nInfer: Y is η∗ = η|a∈ξ\n\nTheorem 2. [16] Let ξ and η be independent uncertain sets with membership functions\nμ and ν, respectively. If ξ∗ is a constant a, then the Inference Rule 1 yields that η∗ has a\nmembership function\n\nν∗(y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nν(y)\nμ(a) , if ν(y) <\n\nμ(a)\n2\n\nν(y)+μ(a)−1\nμ(a) , if ν(y) > 1 − μ(a)\n\n2\n\n0.5, otherwise.\n\nBased on Inference Rule 1, Gao et al. [18] proposed the multi-input, multi-if-then-rule\ninference rules.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 7 of 19\n\nInference Rule 2. [13] Let X1,X2, · · · ,Xm,Y be concepts. Assume rules ‘if X1 is ξi1\nand · · · and Xm is ξim, then Y is ηi’ for i = 1, 2, · · · , k. From X1 is a constant a1 and · · ·\nand Xm is a constant am, we infer that\n\nη∗ =\nk∑\n\ni=1\n\nci · ηi|(a1∈ξi1)∩(a2∈ξi2)∩···∩(am∈ξim)\n\nc1 + c2 + · · · + ck\n, (4)\n\nwhere the coefficients are determined by\n\nci = M{(a1 ∈ ξi1) ∩ (a2 ∈ ξi2) ∩ · · · ∩ (am ∈ ξim)}\nfor i = 1, 2, · · · , k. The inference rule is represented by\n\nRule 1: If X1 is ξ11 and · · · and Xm is ξ1m, then Y is η1\nRule 2: If X1 is ξ21 and · · · and Xm is ξ2m, then Y is η2\n\n· · ·\nRule k: If X1 is ξk1 and · · · and Xm is ξkm, then Y is ηk\nFrom: X1 is a1 and · · · and Xm is am\nInfer: Y is determined by Eq. (4)\n\nTheorem 3. [13] Assume ξi1, ξi2, · · · , ξim, ηi are independent uncertain sets with mem-\nbership functions μi1,μi2, · · · ,μim, νi, i = 1, 2, · · · , k, respectively. If ξ∗\n\n1 , ξ∗\n2 , · · · , ξ∗\n\nm are\nconstants a1, a2, · · · , am, respectively, then the Inference Rule 2 yields\n\nη∗ =\nk∑\n\ni=1\n\nci · η∗\ni\n\nc1 + c2 + · · · + ck\n\nwhere η∗\ni are uncertain sets whose membership functions are given by\n\nν∗\ni (y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνi(y)\nci , if νi(y) < ci\n\n2\n\nνi(y)+ci−1\nμ(a) , if νi(y) > 1 − ci\n\n2\n\n0.5, otherwise\n\nand ci = min\n1≤l≤m\n\nμil(al) are constants.\n\nUncertain system\n\nUncertain system, proposed by Liu [16], is a function from its inputs to outputs based\non the uncertain inference rule. Usually, an uncertain system consists of five parts: inputs\nthat are crisp data to be fed into the uncertain system; a rule-base that contains a set of\nif-then rules provided by the experts; an uncertain inference rule that infers uncertain\nconsequents from the uncertain antecedents; an expected value operator that converts\nthe uncertain consequents to crisp values; and outputs that are crisp data yielded from\nthe expected value operator.\nNow, we consider an uncertain system with m crisp inputs α1,α2, · · · ,αm, and n crisp\n\noutputs β1,β2, · · · ,βn. We have the following if-then rules:\n\nIf X1 is ξ11 and · · · and Xm is ξ1m, then Y1 is η11 and Y2 is η12 and · · · and Yn is η1n\nIf X1 is ξ21 and · · · and Xm is ξ2m, then Y1 is η21 and Y2 is η22 and · · · and Yn is η2n\n\n· · ·\nIf X1 is ξk1 and · · · and Xm is ξkm, then Y1 is ηk1 and Y2 is ηk2 and · · · and Yn is ηkn\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 8 of 19\n\nThus, according to Inference Rule 1 and 2, we can infer that Yj(j = 1, 2, · · · , n) are\n\nη∗\nj =\n\nk∑\ni=1\n\nci · ηij|(a1∈ξi1)∩(a2∈ξi2)∩···∩(am∈ξim)\n\nc1 + c2 + · · · + ck\n,\n\nwhere ci = M{(a1 ∈ ξi1)∩ (a2 ∈ ξi2)∩· · ·∩ (am ∈ ξim)} for i = 1, 2, · · · , k. Then, by using\nthe expected value operator, we obtain\n\nβj = E\n[\nη∗\nj\n\n]\nfor j = 1, 2, · · · , n. Now, we construct a function from crisp inputs α1,α2, · · · ,αm to crisp\noutputs β1,β2, · · · ,βn, i.e.,\n\n(β1,β2, · · · ,βn) = f (α1,α2, · · · ,αm).\n\nThen, we get an uncertain system f. For the uncertain system we proposed, we have the\nfollowing theorem.\n\nTheorem 4. [13] Assume that ξi1, ξi2, · · · , ξim and ηi1, ηi2, · · · , ηin are indepen-\ndent uncertain sets with membership functions μi1,μi2, · · · ,μim, νi1, νi2, · · · , νin, i =\n1, 2, · · · , k, respectively. Then, the uncertain system from α1,α2, · · · ,αm to β1,β2, · · · ,βn is\n\nbj =\nk∑\n\ni=1\n\nci · E[ η∗\nij]\n\nc1 + c2 + · · · + ck\n,\n\nwhere j = 1, 2, · · · , n and η∗\nij are uncertain sets whose membership functions are given by\n\nν∗\nij(y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνij(y)\nci , if νij(y) < ci\n\n2\n\nνij(y)+ci−1\nμ(a) , if νij(y) > 1 − ci\n\n2\n\n0.5, otherwise\n\nand ci = min\n1≤l≤m\n\nμil(al) are constants.\n\nNext, we discuss the expected value of a special triangular uncertain set.Without loss of\ngenerality, we assume n = 1. Then the uncertain system proposed in the above becomes:\n\nb =\nk∑\n\ni=1\n\nci · E[ η∗\ni ]\n\nc1 + c2 + · · · + ck\n, (5)\n\nν∗\ni (y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνi(y)\nci , if νi(y) < ci\n\n2\n\nνi(y)+ci−1\nμ(a) , if νi(y) > 1 − ci\n\n2\n\n0.5, otherwise,\n\n(6)\n\nci = min\n1≤l≤m\n\nμil(al). (7)\n\nTheorem 5. Assume we have an uncertain system with m inputs and 1 output consist-\ning of k inference rules. The antecedents of the rules are represented by the uncertain sets ξi\nwith membership functions μi1,μi2, · · · ,μim, i = 1, 2, · · · , k. And the consequent is repre-\nsented by an triangular uncertain set ηi = (αi,βi, γi) with a membership function νi, where\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 9 of 19\n\nthe coefficients satisfy\n\nαi + γi = 2βi, i = 1, 2, · · · , k. (8)\n\nWe have\n\nE\n[\nη∗\ni\n] = βi, i = 1, 2, · · · , k.\n\nProof. Given the m input data a1, a2, · · · , am, we can calculate ci from Equation 7.\nThen, we can get the membership functions ν∗\n\ni of the consequence uncertain sets η∗\ni\n\naccording to Equation 6. Next, the computation of the expected value of uncertain\nconsequence breaks into three cases.\nCase 1: Assume ci/2 = 0.5. We can immediately have ν∗\n\ni (y) = νi(y), thus\n\nE[ η∗\ni ]=\n\nαi + 2βi + γi\n4\n\n= βi.\n\nCase 2: Assume ci/2 < 0.5. Let yi11 and yi12\n(\nyi11 < yi12\n\n)\nbe the two points that satisfy\n\nthe equation νi(y) = ci/2. Similarly, yi21 and yi22\n(\nyi21 < yi22\n\n)\nsatisfy the equation νi(y) =\n\n1 − ci/2. Since the membership function of a triangular uncertain set has a symmetry\nproperty, we have\n\nyi11 + yi12 = 2βi, yi21 + yi22 = 2βi. (9)\n\nThen, we can rewrite the membership function of ηi as follows:\n\nν∗\ni =\n\n⎧⎪⎪⎪⎪⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎪⎪⎪⎪⎩\n\nνi(y)\nci , if αi ≤ y < yi11\n\nνi(y)+ci−1\nci , if yi21 ≤ y < yi22\n\nνi(y)\nci , if yi12 ≤ y < γi\n\n0.5, otherwise.\n\n(10)\n\nAnd ν∗\ni (βi) = 1. Together with Equations 3, 8, and 9, we have\n\nE[ η∗\ni ] = βi + 1\n\n2\n\n(∫ yi22\n\nβi\n\nνi(y) + ci − 1\nci\n\ndy +\n∫ yi12\n\nyi22\n0.5dy +\n\n∫ γi\n\nyi12\n\nνi(y)\nci\n\ndy\n)\n\n−1\n2\n\n(∫ βi\n\nyi21\n\nνi(y) + ci − 1\nci\n\ndy +\n∫ yi21\n\nyi11\n0.5dy +\n\n∫ yi11\n\nαi\n\nνi(y)\nci\n\ndy\n)\n\n= βi + 1\n2\n\n(∫ yi22\n\nβi\n\nνi(y) + ci − 1\nci\n\ndy −\n∫ βi\n\nyi21\n\nνi(y) + ci − 1\nci\n\ndy\n)\n\n+1\n2\n\n(∫ γi\n\nyi12\n\nνi(y)\nci\n\ndy −\n∫ yi11\n\nαi\n\nνi(y)\nci\n\ndy\n)\n\n+1\n2\n\n(∫ yi12\n\nyi22\n0.5dy −\n\n∫ yi21\n\nyi11\n0.5dy\n\n)\n\n= βi.\n\nCase 3: Assume ci > 0.5. Similarly, we have E[ η∗\ni ]= βi. Thus, we have proved the\n\ntheorem.\n\nProblem formulation\nIn this section, we propose an extraction model to obtain uncertain inference rules.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 10 of 19\n\nLet X = (x1, x2, · · · , xn) be the decision vector, which represents a rule base consisting\nof n rules. Each rule has m antecedents which are described by Q uncertain sets and one\nconsequent which is described by R uncertain sets. Each variable xi represents a sequence\nxi1xi2 · · · ximxim+1, where xij ∈ {0, 1, 2, · · · ,Q}(i = 1, 2, · · · , n; j = 1, 2, · · · ,m) represent\nthe antecedents of the inference rule. And xim+1 ∈ {0, 1, 2, · · · ,R}(i = 1, 2, · · · , n) repre-\nsent the consequent. Thus, each variable of decision vector represents one inference rule.\nSome xij = 0 means this antecedent is not included. And some xim+1 = 0 means this\ninference rule will not be included in the rule base. For example, assume that we have one\ninference rule consists of 4 antecedents and 1 consequent. They are described by 5 uncer-\ntain sets which refer to five descriptions: very low, low, medium, high, and very high. We\nuse 1, 2, 3, 4, 5 to denote them. Thus, sequence “23045”, for example, represents the rule:\n“if input 1 is low, input 2 is medium, and input 4 is high, then the output is very high”.\nUncertain systems can be used for classification. But which uncertain system is better\n\ndepends on the rule base. Here, we try to find best rule base by comparing the mean\nabsolute errors of the origin output and the system output. That is,\n\nMAE = 1\nP\n\nP∑\ni=1\n\n|oi − ti|, (11)\n\nwhere P is the number of training data, oi, ti(i = 1, 2, · · · ,P) are the system outputs and\norigin outputs, respectively. If we find the rule base with the least mean absolute error, we\nextract the uncertain inference rules successfully. We can obtain the system outputs by\nEquation 5. However, they may not be integers. To avoid this nonsense, for a classification\nproblem with C classes, we can divide interval that covers all the system outputs into C\nsubintervals. Then, if the output from Equation 5 is in the ith subinterval, we have oi = i.\nThus, we transfer the classification problem to the following optimization model:⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨\n\n⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩\n\nmin\nX\n\nF(X) = MAE\n\ns.t.\nX = (x1, x2, · · · , xn)\nxi = xi1 · · · ximxim+1\nxij ∈ {0, 1, · · · ,Q}\nxim+1 ∈ {0, 1, · · · ,R}\ni = 1, 2, · · · , n\nj = 1, 2, · · · ,m\n\nExtractionmethod for uncertain inference rules withmutations\nIn this section, we propose the extraction method for uncertain inference rules with\nmutations by ant colony optimization algorithm.\nAs stated before, each xi is a sequence of m values in {0, 1, 2, · · · ,Q} and 1 value in\n\n{0, 1, 2, · · · ,R}. Without loss of generality, we set Q = R. Each number in {0, 1, 2, · · · ,Q}\nis a node. Let ants walking across these nodes. Ants choose the next node in probability\nbased on the pheromone levels in the Q + 1 choices at every step. Once ants movem + 1\nsteps, a candidate decision variable is generated. After repeat this process n times, we get\na candidate solution. After all ants finish their walk, update the pheromone trails. Denote\nthe pheromone trail by τi;k,j(t) associated to the node j at step k of xi in iteration t. The\nprocedures are described as follows.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 11 of 19\n\n(1) Initialization: Randomly generate a feasible solutionX0, and set the optimal solution\nX̂ = X0. Set τi;k,j(0) = τ0, i = 1, 2, · · · , n, k = 1, 2, · · · ,m + 1, j = 0, 1, 2, · · · ,Q, where τ0\nis a fixed parameter.\n(2) Ant movement: At each step k after building the sequence xi1xi2 · · · xik , select the\n\nnext node in probability following\n\npk;k+1 = τi;k+1,j(t)\nQ∑\n\nq=0\nτi;k+1,q(t)\n\n. (12)\n\nIn this way, we could get a sequence xi1xi2 · · · xim+1. To speed up the algorithm, wemutate\nthis sequence to get a new candidate sequence. The mutation is made as follows: ran-\ndomly add 1 or subtract 1 to each element xij in the sequence; if the element is 0, the\nmutated element is 1; if the element is Q, the mutated element is Q − 1. Assume X ′ is\nthe mutated solution, if \rF = F(X ′\n\n) − F(X) ≤ 0, then X ← X ′ ; otherwise, keep the\ncurrent solution. If Q is very large, we could repeat this mutation until some termination\ncondition is satisfied.\n(3) Pheromone Update: At each iteration t, let X̂ be the optimal solution found so far\n\nand Xt be the best feasible solution in the current iteration. Assume F(X̂) and F(Xt) are\nthe corresponding objective function values.\n\nIf F(Xt) < F(X̂), then X̂ ← Xt .\nReinforce the pheromone trails on nodes of X̂ and evaporate the pheromone trails on\n\nthe left nodes:\n\nτi;j,k(t) =\n{\n\n(1 − ρ)τi;j,k(t − 1) + ρg(X̂), if (k, j) ∈ X̂\n(1 − ρ)τi;j,k(t − 1), otherwise\n\n(13)\n\nwhere ρ (0 < ρ < 1) is the evaporation rate, g(x)(0 < g(x) < +∞) is a function with that\ng(x) ≥ g(y) if F(x) < F(y), for example, g(x) = L/(|F(x)| + 1) is a function satisfying the\ncondition where L > 0.\n\nLet τ0 be the initial value of pheromone trails, n be the number of decision variables,\nM be the number of ants, ρ be evaporation rate and T be the number of iterations. Now,\nwe summarize the algorithm as follows.\n\nStep 1 Initialize all pheromone trails with the same pheromone level τ0. Randomly\ngenerate a feasible solution X0, and set optimal solution X̂ = X0. Set l ← 0.\nStep 2 Ant movement in probability following Equation 12. Generate a decision variable\n\nxi afterm + 1 steps.\nStep 3 Repeat Step 2 until X = (x1, x2, · · · , xn) is generated; mutate every xi: thus, gen-\n\nerate a new decision vector X ′ = (x′\n1, x\n\n′\n2, · · · , x\n\n′\nn); if \rF = F(X ′\n\n) − F(X) ≤ 0, then\nX ← X ′ .\nStep 4 Repeat Step 2 and Step 3 for allM ants.\nStep 5 Calculate the system outputs by Equation 5. Then, calculate the objective function\n\nvalues for the M candidate solutions by Equation 11. Denote the best solution in this\niteration by Xl.\nStep 6 If F(Xl) < F(X̂), then X̂ ← Xl; update the pheromone trails according to\n\nEquation 13.\nStep 7 l ← l + 1; if l = T , terminate; otherwise, go to Step 2.\nStep 8 Report the optimal solution X̂.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 12 of 19\n\nWith this algorithm above, we obtain an uncertain rule base. Then, we successfully\ndesign an uncertain system and can use it for classification.\n\nExtractionmethod for uncertain inference rules with SA\nIn the previous section, to speed up the algorithm, we introduce a mutation operation.\nHere, we introduce the simulated annealing algorithm as the local search operation.\nSimulated annealing algorithm was initiated by Metropolis in 1953, applied to portfolio\n\noptimization by Kirkpatrick [25] in 1983. The name and inspiration come from anneal-\ning in metallurgy, a technique involving heating and controlled cooling of a material to\nincrease the size of its crystals and reduce their defects. Simulated annealing algorithm is\nexcellent at avoiding getting stuck in local optimums. It has a good robust property and is\nuniversal and easy to implement.\nFor optimization problem (1), we can use simulated annealing algorithm to search for\n\nthe optimal solution. The algorithm is as follows.\n\nStep 1 Randomly generate a initial solution x0; x ← x0; k ← 0; t0 ← tmax(initial\ntemperature);\nStep 2 If the temperature satisfies the inner cycle termination criterion, go to Step 3;\n\notherwise, randomly choose a point x′ in the neighborhood N(x), calculate \rf = f (x′\n) −\n\nf (x). If \rf ≤ 0, then x ← x′ ; otherwise, according to Metropolis acceptance criterion, if\nexp(−\rf /tk) > random(0, 1), then x ← x′ . Repeat Step 2.\nStep 3 tk+1 = d(tk) (temperature decrease); k ← k + 1; if the termination criterion is\n\nsatisfied, stop and report the optimal solution; otherwise, go to Step 2.\n\nIn this section, we combine ant colony optimization algorithm and simulated annealing\nalgorithm. In each iteration of ant colony optimization algorithm, we get a feasible solu-\ntion. Then, we use it as the initial solution of the simulated annealing algorithm to get a\nneighbor solution. This neighbor solution will be accepted in probability. And for each\ndecision vector X = (x1, x2, · · · , xn), xi = xi1xi2 · · · xim+1, we build the neighbor solution\nas follows: for each xi, for some randomly generated p and q (1 ≤ p < q ≤ m), reverse the\norder of the sequence xip · · · xiq, i.e., x′\n\ni = xi1 · · · xip−1xiqxiq−1 · · · xip+1xipxiq+1 · · · xim+1.\nFor example, assume xi is 0123456, p = 2, q = 6, and the neighbor solution x′\n\ni is 0543216.\nIn this way, we obtain a neighbor solution X ′ . If \rF = F(X ′\n\n) − F(X) ≤ 0, X ← X ′ ;\notherwise, if exp(−\rF/tk) > random(0, 1), then X ← X ′ ; otherwise, abandon this neigh-\nbor solution. Still denote the pheromone trail by τi;k,j(t). The procedure are described as\nfollows.\n\n(1) Initialization: Generate a feasible solution X0 randomly and set the optimal solution\nX̂ = X0. Set τi;k,j(0) = τ0, i = 1, 2, · · · , n, k = 1, 2, · · · ,m + 1, j = 0, 1, 2, · · · ,Q, where τ0\nis a fixed parameter.\n(2) Ant movement: At each step k after building the sequence xi1xi2 · · · xik , select the\n\nnext node in probability following Equation 12. In this way, we could get a sequence\nxi1xi2 · · · xim+1. In order to expand the search range, we use simulated annealing algo-\nrithm to search locally around the solution at this step. Assume the neighbor solution is\nX ′ . If \rF = F(X ′\n\n) − F(X) ≤ 0, X ← X ′ ; otherwise, if exp(−\rF/tk) > random(0, 1)\nwhere tk is the current temperature and tk → 0 when k → ∞, then X ← X ′ ; otherwise,\nabandon this neighbor solution and still choose the original feasible solution.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 13 of 19\n\n(3) Pheromone Update: Let X̂ be the optimal solution found so far and Xt be the best\nfeasible solution in the current iteration t. Assume F(X̂) and F(Xt) are",
      "text": [
        "Published online: 14 May 2015"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"Published online: 14 May 2015\",\"lines\":[{\"boundingBox\":[{\"x\":4,\"y\":15},{\"x\":892,\"y\":16},{\"x\":892,\"y\":74},{\"x\":4,\"y\":71}],\"text\":\"Published online: 14 May 2015\"}],\"words\":[{\"boundingBox\":[{\"x\":4,\"y\":15},{\"x\":272,\"y\":15},{\"x\":271,\"y\":72},{\"x\":4,\"y\":70}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":292,\"y\":15},{\"x\":501,\"y\":15},{\"x\":499,\"y\":73},{\"x\":291,\"y\":72}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":513,\"y\":15},{\"x\":576,\"y\":15},{\"x\":574,\"y\":74},{\"x\":511,\"y\":73}],\"text\":\"14\"},{\"boundingBox\":[{\"x\":595,\"y\":15},{\"x\":729,\"y\":16},{\"x\":727,\"y\":74},{\"x\":593,\"y\":74}],\"text\":\"May\"},{\"boundingBox\":[{\"x\":745,\"y\":16},{\"x\":886,\"y\":16},{\"x\":884,\"y\":75},{\"x\":743,\"y\":74}],\"text\":\"2015\"}]}"
      ],
      "imageTags": [
        "font",
        "typography",
        "text"
      ],
      "imageCaption": [
        "{\"tags\":[\"text\",\"logo\"],\"captions\":[{\"text\":\"text, logo\",\"confidence\":0.82914352416992188}]}"
      ],
      "pii_entities": [
        {
          "text": "Chen",
          "type": "Person",
          "subtype": null,
          "offset": 1,
          "length": 4,
          "score": 0.94
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 64,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Ling Chen",
          "type": "Person",
          "subtype": null,
          "offset": 204,
          "length": 9,
          "score": 0.99
        },
        {
          "text": "Yun Sun",
          "type": "Person",
          "subtype": null,
          "offset": 215,
          "length": 7,
          "score": 0.99
        },
        {
          "text": "Yuanguo Zhu",
          "type": "Person",
          "subtype": null,
          "offset": 228,
          "length": 11,
          "score": 0.98
        },
        {
          "text": "chinalsy_881220@163.com",
          "type": "Email",
          "subtype": null,
          "offset": 258,
          "length": 23,
          "score": 0.8
        },
        {
          "text": "School of Science",
          "type": "Organization",
          "subtype": null,
          "offset": 282,
          "length": 17,
          "score": 0.69
        },
        {
          "text": "University of Science",
          "type": "Organization",
          "subtype": null,
          "offset": 309,
          "length": 21,
          "score": 0.87
        },
        {
          "text": "210094",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 355,
          "length": 6,
          "score": 0.8
        },
        {
          "text": "210094",
          "type": "Address",
          "subtype": null,
          "offset": 355,
          "length": 6,
          "score": 0.92
        },
        {
          "text": "previously",
          "type": "DateTime",
          "subtype": null,
          "offset": 2695,
          "length": 10,
          "score": 0.8
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 2963,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Chen",
          "type": "Person",
          "subtype": null,
          "offset": 2968,
          "length": 4,
          "score": 0.95
        },
        {
          "text": "Springer",
          "type": "Organization",
          "subtype": null,
          "offset": 2990,
          "length": 8,
          "score": 0.63
        },
        {
          "text": "http://creativecommons.org/licenses/by/4.0)",
          "type": "URL",
          "subtype": null,
          "offset": 3104,
          "length": 43,
          "score": 0.8
        },
        {
          "text": "chinalsy_881220@163.com",
          "type": "Email",
          "subtype": null,
          "offset": 3285,
          "length": 23,
          "score": 0.8
        },
        {
          "text": "http://creativecommons.org/licenses/by/4.0",
          "type": "URL",
          "subtype": null,
          "offset": 3309,
          "length": 42,
          "score": 0.8
        },
        {
          "text": "Chen",
          "type": "Person",
          "subtype": null,
          "offset": 3354,
          "length": 4,
          "score": 0.94
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 3417,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Herrera",
          "type": "Person",
          "subtype": null,
          "offset": 5804,
          "length": 7,
          "score": 0.99
        },
        {
          "text": "Zhu",
          "type": "Person",
          "subtype": null,
          "offset": 6049,
          "length": 3,
          "score": 0.92
        },
        {
          "text": "ACOA",
          "type": "Organization",
          "subtype": null,
          "offset": 6109,
          "length": 4,
          "score": 0.96
        },
        {
          "text": "may",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6507,
          "length": 3,
          "score": 0.8
        },
        {
          "text": "everyday",
          "type": "DateTime",
          "subtype": "Set",
          "offset": 6552,
          "length": 8,
          "score": 0.8
        },
        {
          "text": "may",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6941,
          "length": 3,
          "score": 0.8
        },
        {
          "text": "Liu",
          "type": "Person",
          "subtype": null,
          "offset": 7146,
          "length": 3,
          "score": 0.98
        },
        {
          "text": "Chen",
          "type": "Person",
          "subtype": null,
          "offset": 7346,
          "length": 4,
          "score": 0.94
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 7409,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "researchers",
          "type": "PersonType",
          "subtype": null,
          "offset": 7438,
          "length": 11,
          "score": 0.97
        },
        {
          "text": "2008",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 7520,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Liu",
          "type": "Person",
          "subtype": null,
          "offset": 7526,
          "length": 3,
          "score": 0.98
        },
        {
          "text": "Peng",
          "type": "Person",
          "subtype": null,
          "offset": 7934,
          "length": 4,
          "score": 0.97
        },
        {
          "text": "Yao",
          "type": "Person",
          "subtype": null,
          "offset": 7943,
          "length": 3,
          "score": 0.93
        },
        {
          "text": "Zhu",
          "type": "Person",
          "subtype": null,
          "offset": 7999,
          "length": 3,
          "score": 0.91
        },
        {
          "text": "2010",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8055,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Liu",
          "type": "Person",
          "subtype": null,
          "offset": 8073,
          "length": 3,
          "score": 0.98
        },
        {
          "text": "operator",
          "type": "PersonType",
          "subtype": null,
          "offset": 8431,
          "length": 8,
          "score": 0.56
        },
        {
          "text": "Gao",
          "type": "Person",
          "subtype": null,
          "offset": 8470,
          "length": 3,
          "score": 0.98
        },
        {
          "text": "al",
          "type": "Person",
          "subtype": null,
          "offset": 8477,
          "length": 2,
          "score": 0.51
        },
        {
          "text": "Peng",
          "type": "Person",
          "subtype": null,
          "offset": 8567,
          "length": 4,
          "score": 0.86
        },
        {
          "text": "Chen",
          "type": "Person",
          "subtype": null,
          "offset": 8576,
          "length": 4,
          "score": 0.92
        },
        {
          "text": "Dorigo",
          "type": "Person",
          "subtype": null,
          "offset": 9868,
          "length": 6,
          "score": 0.98
        },
        {
          "text": "Chen",
          "type": "Person",
          "subtype": null,
          "offset": 10836,
          "length": 4,
          "score": 0.94
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 10899,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Borel",
          "type": "Person",
          "subtype": null,
          "offset": 12896,
          "length": 5,
          "score": 0.54
        },
        {
          "text": "Chen",
          "type": "Person",
          "subtype": null,
          "offset": 13118,
          "length": 4,
          "score": 0.94
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 13181,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Borel",
          "type": "Person",
          "subtype": null,
          "offset": 13646,
          "length": 5,
          "score": 0.53
        },
        {
          "text": "Chen",
          "type": "Person",
          "subtype": null,
          "offset": 14863,
          "length": 4,
          "score": 0.94
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 14926,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Mamdani",
          "type": "Person",
          "subtype": null,
          "offset": 15460,
          "length": 7,
          "score": 0.7
        },
        {
          "text": "Takagi",
          "type": "Person",
          "subtype": null,
          "offset": 15493,
          "length": 6,
          "score": 0.59
        },
        {
          "text": "mon",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 15546,
          "length": 3,
          "score": 0.8
        },
        {
          "text": "Gao",
          "type": "Person",
          "subtype": null,
          "offset": 16660,
          "length": 3,
          "score": 0.96
        },
        {
          "text": "Chen",
          "type": "Person",
          "subtype": null,
          "offset": 16741,
          "length": 4,
          "score": 0.94
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 16804,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "k",
          "type": "Person",
          "subtype": null,
          "offset": 16975,
          "length": 1,
          "score": 0.59
        },
        {
          "text": "k",
          "type": "Person",
          "subtype": null,
          "offset": 17252,
          "length": 1,
          "score": 0.59
        },
        {
          "text": "Liu",
          "type": "Person",
          "subtype": null,
          "offset": 18153,
          "length": 3,
          "score": 0.99
        },
        {
          "text": "experts",
          "type": "PersonType",
          "subtype": null,
          "offset": 18425,
          "length": 7,
          "score": 0.88
        },
        {
          "text": "Chen",
          "type": "Person",
          "subtype": null,
          "offset": 19121,
          "length": 4,
          "score": 0.94
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 19184,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "k",
          "type": "Person",
          "subtype": null,
          "offset": 19456,
          "length": 1,
          "score": 0.59
        },
        {
          "text": "Now",
          "type": "DateTime",
          "subtype": null,
          "offset": 19556,
          "length": 3,
          "score": 0.8
        },
        {
          "text": "k",
          "type": "Person",
          "subtype": null,
          "offset": 21032,
          "length": 1,
          "score": 0.59
        },
        {
          "text": "Chen",
          "type": "Person",
          "subtype": null,
          "offset": 21159,
          "length": 4,
          "score": 0.94
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 21222,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "k",
          "type": "Person",
          "subtype": null,
          "offset": 21357,
          "length": 1,
          "score": 0.59
        },
        {
          "text": "Chen",
          "type": "Person",
          "subtype": null,
          "offset": 22996,
          "length": 4,
          "score": 0.94
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 23059,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "MAE",
          "type": "Organization",
          "subtype": null,
          "offset": 24430,
          "length": 3,
          "score": 0.61
        },
        {
          "text": "= 1\nP",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 24434,
          "length": 5,
          "score": 0.8
        },
        {
          "text": "candidate",
          "type": "PersonType",
          "subtype": null,
          "offset": 25860,
          "length": 9,
          "score": 0.52
        },
        {
          "text": "Chen",
          "type": "Person",
          "subtype": null,
          "offset": 26170,
          "length": 4,
          "score": 0.94
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 26233,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Chen",
          "type": "Person",
          "subtype": null,
          "offset": 28991,
          "length": 4,
          "score": 0.94
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 29054,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "SA",
          "type": "Organization",
          "subtype": null,
          "offset": 29277,
          "length": 2,
          "score": 0.75
        },
        {
          "text": "Metropolis",
          "type": "Organization",
          "subtype": "Sports",
          "offset": 29498,
          "length": 10,
          "score": 0.87
        },
        {
          "text": "1953",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 29512,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Kirkpatrick",
          "type": "Person",
          "subtype": null,
          "offset": 29556,
          "length": 11,
          "score": 0.98
        },
        {
          "text": "1983",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 29576,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "0123456",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 31337,
          "length": 7,
          "score": 0.8
        },
        {
          "text": "0543216",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 31395,
          "length": 7,
          "score": 0.8
        },
        {
          "text": "Chen",
          "type": "Person",
          "subtype": null,
          "offset": 32510,
          "length": 4,
          "score": 0.94
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 32573,
          "length": 4,
          "score": 0.8
        }
      ]
    },
    {
      "@search.score": 4.65639,
      "content": "\nComputational Visual Media\nhttps://doi.org/10.1007/s41095-020-0189-1 Vol. 7, No. 2, June 2021, 159–167\n\nReview Article\n\nMachine learning for digital try-on: Challenges and progress\n\nJunbang Liang1 (�), Ming C. Lin1\n\nc© The Author(s) 2020.\n\nAbstract Digital try-on systems for e-commerce have\nthe potential to change people’s lives and provide notable\neconomic benefits. However, their development is limited\nby practical constraints, such as accurate sizing of the\nbody and realism of demonstrations. We enumerate three\nopen challenges remaining for a complete and easy-to-use\ntry-on system that recent advances in machine learning\nmake increasingly tractable. For each, we describe\nthe problem, introduce state-of-the-art approaches, and\nprovide future directions.\n\nKeywords machine learning; digital try-on; garment\nmodeling; human body estimation; material\nmodeling\n\n1 Introduction\nE-commerce has grown at a rapid pace in recent\nyears. Consumers today are more likely to shop\nonline than to visit a retail store. The situation is\nmuch more complicated, however, when it comes to\nbuying clothes. People need to know how a garment\nfits on them, how it looks, and how it feels. Digital\ntry-on systems can potentially satisfy these needs,\nproviding a direct visual impression, and possibly\ncustomized clothes sizing as well. Therefore, it has\ndrawn much attention as an attractive alternative to\nimprove the user experience and popularize online\nfashion shopping.\n\nHowever, the technology is still far from practical,\neasy-to-use, and adequate to replace physical try-on.\nCurrently, most try-on systems rely on either image-\nediting, copy-pasting, or template demonstrations,\n\n1 University of Maryland, College Park, MD 20785, USA.\nE-mail: J. Liang, liangjb@cs.umd.edu (�); M. C. Lin,\nlin@cs.umd.edu.\n\nManuscript received: 2020-06-24; accepted: 2020-07-21\n\nwhile the ultimate goal is a fast and realistic try-on\nsystem adaptive to each customer’s body. There is\nstill a substantial technological gap between modeling\nand demonstrating garment fitting in the digital and\nreal worlds, including fast and realistic demonstration,\naccurate modeling of human body and garments,\nfaithful modeling of garment material, and lossless\ntransformation of garments between virtual and\nphysical worlds.\n\nIn this paper, we present some open research issues\nthat contribute to this technological gap, including:\n1. accurate estimation of human shapes and sizes\n\nusing consumer devices,\n2. faithful recovery of garment materials via (online)\n\nimages, and\n3. ease of design and manipulation of sewing patterns\n\nand garment pieces by end-users.\nAlthough traditional methods have made important\n\nprogress on these under-constrained problems,\nlearning-based approaches have shown tremendous\npotential to make a notable impact. Compared to\ntraditional methods, machine-learning algorithms are\nusually much faster since training and optimization\nare performed offline. They are also good at\ngeneralizing to unseen images without the need for\ntedious data pre-processing. While extensive research\nexists on 2D image learning, machine learning of\nhighly variable 3D human body shapes is still far\nfrom mature, which is the reason why the open issues\ndescribed above remain elusive.\n\nFor each problem listed above, we motivate its\nimportance, provide a problem description, and\npresent state-of-the-art approaches with potential\nfor improvements. We believe that solutions to\nthese challenging problems will lead to significant\nadvances in digital try-on, as well as other areas of\ne-commerce.\n\n159\n\n\n\n\n\n\n\n160 J. Liang, M. C. Lin\n\n2 Open problems\nIn this section we first introduce three major\nchallenges that limit digital try-on technology from\nbeing widely adopted and accepted by shoppers.\nThere are several reasons why shoppers still prefer\nphysical try-on. Firstly, consumers are unsure if what\nthey buy online will fit them well. Although general\nsizing systems exist, their lack of consistency and\nstandardization across different brands and garment\nmaterials can often make it difficult to size clothes,\nespecially for persons with non-standard body shapes\nand proportion. Accurate estimation of human body\nshape is the key to successful digital try-on. Secondly,\nfabric is usually a key consideration when shopping\nfor clothes. Different fabric affects how garments\nlook and fit, how consumers would wear them, and\nwhether or not they would buy them. However, the\ncorrespondence between the actual material and its\ndigital representation are not well understood. It is\nalso challenging to acquire a full fabric digital model\nfrom real-world examples.\n\nFor the customers, appearance is as critical as other\nfactors. There are two approaches to displaying\ngarments: 2D image-based, and 3D mesh-based\nwith photo-realistic rendering. They have different\nadvantages and drawbacks, but both need a large\ngarment database for support. While creating a 3D\ngarment takes considerable effort, 2D images often\nsuffer from a lack of variation and are much more\ndifficult to customize. In either case, the try-on\nsystem needs a user-friendly design and manipulation\nbackend. Last, but not least, a fast and realistic\nanimation of the garments in motion along with\nbody movements greatly improves the user experience.\nAlthough it is not so critical as other factors, it\nwould effectively reduce the perceptual gap between\nthe real and digital worlds for online shopping.\nPrevious work has proposed using cloud computing\nto improve the animation speed, but there is still a\nnotable technology gap for high-quality, interactive\n3D animation of clothes.\n\n3 Human shape estimation\nAs noted, accurate human shape estimation is key to\nenabling digital try-on. Human body reconstruction,\nconsisting of pose and shape estimation, has been\nwidely studied in a variety of areas, including digital\n\nsurveillance, computer animation, special effects, and\nvirtual and augmented environments. Yet, it remains\na challenging and popular topic of interest. While\ndirect 3D body scanning can provide excellent and\naccurate results, its adoption is somewhat limited by\nthe required specialized hardware. RGB images are\nwidely available for input to digital try-on and can\nbe easily captured using commodity mobile devices.\nAlthough purely image-based try-on methods have\nbeen proposed [1], learning-based 3D body estimation\nis more widely applicable in that the 3D body can be\narticulated and so re-posed and re-targeted.\n\nWe define the human-body reconstruction problem\ninformally as, given one or more RGB images, to\nestimate the human body geometry and size, and\noutput (preferably) a 3D humanoid mesh. Traditional\nalgorithms often formulate it as an optimization\nproblem, in which the silhouette difference is a major\npart of the objective function [2]. Therefore, these\nmethods either require the human to wear tight\nclothes, or alternatively relax the target function\nto be unilateral on uncovered body parts [3], or\nto point correspondences [4]. The use of machine\nlearning methods in this problem has led to significant\nadvances. Firstly, it has moved the algorithm from\nonline to offline, significantly reducing response time.\nSecond, by using a parametric human model [5],\none can easily construct a regression network for\nthe parameters while the losses needed can also be\ninferred from them. While early works proposed\nnetwork models for only 2D/3D body skeletons [6–\n8], more recent works have introduced techniques to\nperform regression for the entire human body—either\nusing a parametric human model [9, 10] or a voxel-\nbased representation [11–13]. As annotations in most\nreal-world datasets contain only joint positions, the\nlearning process has been refined in various ways [14–\n17]. The current state of the art is the recent work\nby Ref. [18] �. It emphasizes shape learning, while\nmany other works often focus on body-joint losses,\nbut neglect the effect of body shapes.\n\nThe key contribution of Ref. [18] is a multi-view,\nmulti-stage framework to address ambiguity caused by\ncamera projection (see Fig. 1). Their model performs\nseveral stages of error correction. Each of the image\ninputs is passed on step by step; at each step, a shared-\n\n� Liang and Lin’s data and code are available at https://gamma.umd.edu/\nresearchdirections/virtualtryon/humanmultiview\n\n\n\n\n\nMachine learning for digital try-on: Challenges and progress 161\n\nFig. 1 Network structure from Ref. [18]. By using an iterative value correction structure, visual information from different views is effectively\nintegrated to provide a unified human shape. Reproduced with permission from Ref. [18], c© The Author(s) 2019.\n\nparameter prediction block computes the correction\nbased on the image feature and the input guesses.\nThe camera and the human body parameters are\nestimated at the same time, projecting the predicted\n3D joints back to 2D for loss computation. The\nestimated pose and shape parameters are shared\namong all views, while each view maintains its own\ncamera calibration and global orientation. Their\nproposed framework uses a recurrent structure,\nmaking it a universal model applicable to any number\nof views. At the same time, it couples shareable\ninformation across different views so that the human\nbody pose and shape are optimized using image\nfeatures from all views. Unlike static multi-view\nCNNs which have a fixed number of inputs, they\nmake use of the RNN-like structure in a cyclic form to\naccept any number of views, and prevent the gradient\nvanishing by predicting corrective values instead of\nupdating parameters in each regression block.\n\nExperiments have shown that, after training, this\nmodel can form a single view image, provide equally\ngood pose estimation as the state of the art, and\nprovide considerably improved pose estimation when\nusing multi-view inputs, leading to better shape\nestimation across all datasets. An example is\ndemonstrated in Fig. 2. Moreover, a physically-based\nsynthetic data generation pipeline is introduced to\nenrich the training data, which is very helpful for\n\nshape estimation and regularization in cases that\ntraditional datasets do not capture. While synthetic\ndata improves the diversity of human bodies with\nground-truth parameters, a larger garment dataset\nand a more convenient registration process are needed\nto minimize the performance gap between real-world\nimages and synthetic data. In addition, other\nvariables such as hair, skin color, and 3D backgrounds\nare subtle elements that can influence the perceived\nrealism of the synthetic data at the higher expense of\na more complex data generation pipeline. With the\nrecent progress in image style transfer using GAN, a\n\nFig. 2 Prediction results using the state of the art [18]. The model\ncaptures the shape of the human body by learning from synthetic\ndata. The recovered legs and chest are close to those of the person\nin the image. Reproduced with permission from Ref. [18], c© The\nAuthor(s) 2019.\n\n\n\n\n\n\n\n162 J. Liang, M. C. Lin\n\npromising direction is to transfer the synthetic result\nto more realistic images to further improve the result.\n\n4 Garment material modeling\n4.1 Introduction\nGarment material plays an important role in digital\ntry-on systems. Physical recreation of the fabric not\nonly gives a compelling visual simulation of the cloth,\nbut also affects how the garment feels and fits on\nthe body. However, fabric modeling is a challenging\ntask: the appearance and physical properties of\nthe garment are determined not only by the type\nof materials the clothes are made of, but also by\nsewing and weave. Thus, researchers often focus on\nthe physical behaviour, rather than the underlying\nsemantic primitives.\n\nHence, we state the garment material modeling\nproblem as follows. Given a sufficient amount of data,\nmodel the material’s physical behavior and physical\nproperties, so that visual effects the same as or similar\nto those of the real material can be reproduced by a\ncomputer. This has two implications: firstly, we need\nto define a physical model of the material, and then\nwe must estimate the parameters in the model.\n\nThere are many ways to model clothes, including\nspring–mass systems and finite elements. The latter is\nthe most popular model since it can produce realistic\nresults. While one can use isotropic properties such\nas Young’s modulus and Poisson ratio, an anisotropic\nmodel is a better choice since it can support different\nbehaviors caused by the weave of the material.\n\n4.2 Learning-based estimation\nWhile traditional optimization methods [19] often\ntake a long time to compute material parameters,\nmachine-learning methods can make predictions in\nreal time by a simple feed-forward operation, which\nis more useful in applications that need fast feedback,\nsuch as garment prototyping. The state-of-the-art\nmodel from Yang et al. [20] � uses CNNs combined\nwith LSTM to recover material parameters from\nvideos. To constrain both the input and solution\nspace, they choose one of the materials as a basis;\nthe material sub-space is constructed by multiplying\nthis material basis with a positive coefficient. To\nconstruct an optimal material parameter sub-space, a\n\n� Yang et al.’s data and code are available at http://gamma.cs.unc.edu/\nVideoCloth\n\nmaterial parameter sensitivity analysis is conducted\nto examine the sensitivity of the material parameters\nκ with respect to the amount of deformation D(κ).\nPhysically based cloth simulations are used to\ngenerate a much larger number of data samples within\nthese sub-spaces, which would otherwise be difficult\nor time-consuming to capture. The cloth meshes are\ngenerated through physically-based simulation, and\nthen rendered as 2D images with a randomly assigned\ntexture. Using the data samples, they combine the\nimage signal feature extraction method, a CNN, with\nthe temporal sequence learning method, LSTM, to\nlearn the mapping from visual appearance to material.\nAs shown in Fig. 3, the CNN layer is used to extract\nboth low- and high-level visual features, while the\nLSTM layer focuses on learning the mapping between\nthe material properties of the cloth and its consequent\nmovement.\n\nThey demonstrated the proposed framework with\nthe application of “material cloning”. With the\ntrained deep neural network model being able to\ncapture the cloth motion (Fig. 4), the material type\ncan be inferred from a video recording of the motion\nof the cloth in a fairly small amount of time. The\nrecovered material type can be “cloned” onto another\npiece of cloth or garment as shown in Fig. 5.\n\nIn this work, the videos contain only a single piece\nof cloth which does not interact with any other object.\nWhile this is not applicable to all real-world scenarios,\nthis method provides new insights into addressing this\nchallenging problem. A natural extension would be to\nlearn from videos of clothing directly interacting with\nthe human body, under varying lighting conditions\nand partial occlusion.\n\n4.3 Optimization using differentiable physics\nAnother approach to modeling the fabric is to measure\ngeometric differences directly during parameter\n\nFig. 3 Network model from Ref. [20]. The material is modeled\nby learning motion patterns of image features given by CNNs.\nReproduced with permission from Ref. [20], c© The Author(s) 2017.\n\n\n\n\n\nMachine learning for digital try-on: Challenges and progress 163\n\nFig. 4 Learned CNN conv5-layer activation visualization from\nRef. [20]. Experiments show that the trained model is able to capture\nmoving parts of the cloth even in an unseen video. Reproduced with\npermission from Ref. [20], c© The Author(s) 2017.\n\nFig. 5 Yang et al. [20] modeled clothes materials in input videos (left),\nand applied those materials to a simulated skirt (right). Reproduced\nwith permission from Ref. [20], c© The Author(s) 2017.\n\noptimization. Assuming that the environment is\nknown to the system, computation of the estimated\nmotion and its gradient with respect to the material\nparameters can be achieved using differentiable\nsimulation. A typical usage of differentiable\nsimulation is motion control (see Fig. 6), where the\ndifference to the target is measured and the loss\nbackpropagated to the network. Similar processes\ncan be applied to material parameter estimation as\nwell. By measuring the distance to the target as the\nloss and computing corresponding gradients, either in\npixel space or in 3D space, the material parameters\n\ncan be learned or optimized to achieve the desired\ncloth motion or visual effect. Recent differentiable\nphysics work covers rigid bodies [22, 23], cloth [24],\nand particle-grid systems [25, 26]. The state-of-\nthe-art is Ref. [24] �, which proposes a method for\ndifferentiable cloth simulation. It is the first work\nto tackle a high dimensional simulation problem\nand to propose a general differentiable collision\nhandling algorithm. Later, a follow-up work [21]\nextended the algorithm to be applicable to coupled\ndynamics with rigid bodies. Overall, they follow\nthe computational flow of the common approach\nto cloth simulation: discretization using the finite\nelement method, integration using an implicit Euler\nmethod, and collision response on impact zones. They\nuse implicit differentiation in the linear solver and\noptimization in order to compute the gradient with\nrespect to the input parameters. The discontinuity\nintroduced by collision response is negligible because\nthe discontinuous states constitute a zero-measure\nset. During backpropagation in the optimization,\ngradient values can be directly computed after QR\ndecomposition of the constraint matrix. Their\npipeline contains several techniques that can be\nemployed in other differentiable simulations.\n4.3.1 Derivatives of the physical solution\nIn modern simulation algorithms, an implicit Euler\nmethod is often used for stable integration results.\nThus the mass matrix M often includes the Jacobian\nof the forces, and is denoted as M̂ to indicate this\ndifference. A linear solver is needed to compute the\nacceleration since it is time-consuming to compute\nM̂−1. Implicit differentiation is used to compute the\ngradients of the linear solution. Given an equation\nM̂a = f with a solution z and propagated gradient\n∂L/∂a|a=z, where L is the task-specific loss function,\nimplicit differentiation is used to derive the gradients.\nWe refer readers to the original paper [24] for more\ndetails.\n4.3.2 Derivatives of the collision response\nA general approach using LCP to integrate collision\nconstraints into physics simulations has been\nproposed, but constructing a static LCP is often\nimpractical in cloth simulation due to the high\ndimensionality. Collisions and contacts which happen\nat each step are very sparse compared to the complete\n\n� Liang et al.’s data and code are available at https://gamma.umd.edu/\nresearchdirections/virtualtryon/differentiablecloth\n\n\n\n\n\n\n\n164 J. Liang, M. C. Lin\n\nFig. 6 Differentiable simulation embedding example from Ref. [21]. The loss can be backpropagated through the physics simulator to the\nneural network, enabling learning tasks such as material modeling and motion control.\n\ndata. Therefore, a dynamic approach is used that\nincorporates collision detection and response.\n\nCollision handling in their implementation is based\non impact zone optimization. It finds all colliding\ninstances using continuous collision detection and\nsets up the constraints for all collisions. In order\nto introduce minimum change to the original mesh\nstate, a QP problem is developed to determine the\nconstraints. Since the signed distance function is\nlinear in x, the optimization takes a quadratic form,\nas shown originally in Ref. [24]:\n\nminimize\nz\n\n1\n2\n\n(z − x)TW (z − x),\n\nsubject to Gz + h � 0\nwhere W is a constant diagonal weight matrix related\nto the mass of each vertex, and G and h are constraint\nparameters. The numbers of variables and constraints\nare n and m, i.e. x ∈ R\n\nn, h ∈ R\nm, and G ∈ R\n\nm×n.\nNote that this optimization problem has inputs x,\nG, and h, and output z. The goal here is to derive\n∂L/∂x, ∂L/∂G, and ∂L/∂h given ∂L/∂z, where L\nis the loss function.\n\nWhen computing the gradient using implicit\ndifferentiation, the dimensionality of the linear system\ncan be very high. Their key observation here is that\nn >> m > rank(G), since one contact often involves 4\nvertices (thus 12 variables) and some contacts may be\nlinearly dependent (e.g., multiple adjacent collision\n\npairs). They minimize the size of the linear equation\nbased on the QR decomposition of G, which is the key\nto accelerating backpropagation of high dimensional\nQP problems.\n\nOne of their experiments shows its ability to\noptimize material parameters from observation. The\nscene features a piece of cloth hanging under gravity\nand subjected to a constant wind force. The material\nmodel consists of three parts: density d, stretching\nstiffness S, and bending stiffness B. The stretching\nstiffness quantifies the reaction force when the cloth\nis stretched; the bending stiffness models how easily\nthe cloth can be bent and folded. Table 1 shows\nresults. They achieve a much smaller error in most\nmeasurements in comparison to the baselines; the\nlinear part of the stiffness matrix is modeled well.\nWith the computed gradient using their model, one\ncan effectively optimize the unknown parameters that\ndominate cloth movement to fit the observed data.\n\nIn follow-up work, Qiao et al. extended the\ndifferentiable simulation pipeline to couple with\nrigid body dynamics, formulated using generalized\ncoordinates:\n\nd\ndt\n\n⎛\n⎝ q\n\nq̇\n\n⎞\n⎠ =\n\n⎛\n⎝ q̇\n\nq̈\n\n⎞\n⎠ =\n\n⎛\n⎝ q̇\n\nM−1f(q, q̇)\n\n⎞\n⎠\n\nand update the optimization formulation for collision\nresponse accordingly (see Ref. [21] for details):\n\nTable 1 Material parameter estimation results from Ref. [24]. Their proposed method runs faster than L-BFGS. Values of material parameters\nare Frobenius norms of the difference normalized by the Frobenius norm of the target. Values of the simulated result are the average pairwise\nvertex distances normalized by the size of the cloth. The gradient-based method yields much smaller errors than the baselines\n\nMethod\nRuntime\n\n(sec/step/iter)\n\nDensity\n\nerror (%)\n\nLinear stretching\n\nstiffness error (%)\n\nBending stiffness\n\nerror (%)\n\nSimulation\n\nerror (%)\n\nBaseline — 68 ± 46 160 ± 119 70 ± 42 12 ± 3.0\n\nL-BFGS 2.89 ± 0.02 4.2 ± 5.6 72 ± 90 70 ± 43 4.9 ± 3.3\n\nLiang et al. [24] 2.03 ± 0.06 1.8 ± 2.0 45 ± 41 77 ± 36 1.6 ± 1.4\n\n\n\n\n\nMachine learning for digital try-on: Challenges and progress 165\n\nminimize\nq′\n\n1\n2\n\n(q − q′)TM̂(q − q′)\n\nsubject to Gf(q′) + h � 0\n\nDue to the inclusion of rigid bodies, the constraints\nused in the optimization are no longer linear. When\ncomputing gradients, they linearize the constraints\naround a neighborhood as an approximation to enable\nQR decomposition for acceleration as previously\nmentioned.\n\n5 Garment modeling and design\nRealistic apparel model generation has become\nincreasingly popular, due to the rapid changes in\nfashion trends and the growing need for garment\nmodels in different applications such as virtual try-\non. It is already used even for state-of-the-art\ninteractive apparel design systems [27]. Application\nrequirements mean that it is important to have a\ngeneral cloth model that can represent a diverse set\nof garments. However, there are many challenges\nin automatic garment model generation. Firstly,\ngarments usually have different types of topology,\nespecially for fashion apparel, that makes it difficult\nto design a universal pipeline. Moreover, it is often\nnot straightforward for general garments design to\nbe retargeted onto another body shape, making\ncustomization difficult.\n\nPrevious work has addressed this problem to some\nextent. Huang et al. [28] proposed a realistic 3D\ngarment generation algorithm based on front and\nback image sketches, but it cannot readily retarget\ngenerated garments to other body shapes. Wang et\nal. [29] proposed an algorithm which can conveniently\nperform retargeting, but permits limited topology\nlike T-shirts or skirts. There is no recent work that\naddresses these two problems at the same time.\n\nWe introduce a learning-based parametric\ngenerative model to overcome the above difficulties,\ngiven garment sewing patterns and human body\nshapes as input. One possible approach would be to\ncompute a displacement image on the U–V space\nof the human body as a unified representation of\nthe garment mesh. Different topology and sizes\nof the garment are represented by different values\nin the image. The 2D displacement image, as the\nrepresentation of the 3D garment mesh data, can\n\nthen be fed into a conditional generative adversarial\nnetwork (cGAN) for latent space learning. The 2D\nrepresentation for the garment mesh can transfer\nthe irregular 3D mesh data to regular image data\nwhere a traditional CNN can easily learn. It can also\nextract relative geometric information with respect\nto the human body, enabling garment retargeting to\na different person.\n\n6 Conclusions\nAlthough virtual reality and digital try-on have\nexcellent potential and are rapidly developing, there\nremain open problems before online try-on systems\ncan be widely adopted. We have listed three major\nchallenges, all of which can be addressed or further\nimproved using machine learning algorithms. For\ngarment material prediction, state-of-the-art methods\nare still limited in that the training data is highly\nconstrained: the scenario contains only a piece\nof cloth floating in the wind. To improve its\napplicability to daily tasks, it is necessary to focus\non solving the problem on a more diverse set of\ninputs. Predicting the material from a garment\non a fixed human body could be a good start,\nbefore generalizing to arbitrary human motions and\npredicting multiple garments on the same body. In\nthe area of human shape estimation, it would be\ninteresting to learn how external constraints could\nimprove estimation accuracy. For example, the shape\nand size of the garment are hard constraints to\nwhich the predicted body should conform. While\noptimization-based methods can integrate these\nconstraints fairly easily, doing so remains elusive\nfor learning-based approaches. One possibility is\nto jointly estimate body and garment together and\nintroduce an intersection loss. This approach would\nrequire a new solution to the open problem of unified\ndeep garment representation, if we do not want to\ntrain one model for every garment type, which could\nbe even more challenging. We believe that substantial\nbreakthroughs in digital try-on are achievable with\nmore investigation in these directions.\n\nAcknowledgements\nThis research was supported in part by the Iribe\nProfessorship and the National Science Foundation.\n\n\n\n166 J. Liang, M. C. Lin\n\nReferences\n\n[1] Zheng, Z. H.; Zhang, H. T.; Zhang, F. L.; Mu, T. J.\nImage-based clothes changing system. Computational\nVisual Media Vol. 3, No. 4, 337–347, 2017.\n\n[2] Dibra, E.; Jain, H.; Öztireli, C.; Ziegler, R.; Gross,\nM. HS-Nets: Estimating human body shape from\nsilhouettes with convolutional neural networks. In:\nProceedings of the 4th International Conference on\n3D Vision, 108–117, 2016.\n\n[3] Bălan, A. O.; Black, M. J. The naked truth: Estimating\nbody shape under clothing. In: Computer Vision –\nECCV 2008. Lecture Notes in Computer Science, Vol.\n5303. Forsyth, D.; Torr, P.; Zisserman, A. Eds. Springer\nBerlin, 15–29, 2008.\n\n[4] Lassner, C.; Romero, J.; Kiefel, M.; Bogo, F.; Black,\nM. J.; Gehler, P. V. Unite the people: Closing the\nloop between 3D and 2D human representations. In:\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 4704–4713, 2017.\n\n[5] Loper, M.; Mahmood, N.; Romero, J.; Pons-Moll, G.;\nBlack, M. J. SMPL: A skinned multi-person linear\nmodel. ACM Transactions on Graphics Vol. 34, No. 6,\nArticle No. 248, 2015.\n\n[6] Wei, S.-E.; Ramakrishna, V.; Kanade, T.; Sheikh, Y.\nConvolutional pose machines. In: Proceedings of the\nIEEE conference on Computer Vision and Pattern\nRecognition, 4724–4732, 2016.\n\n[7] Cao, Z.; Simon, T.; Wei, S.; Sheikh, Y. Realtime multi-\nperson 2D pose estimation using part affinity fields.\nIn: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 1302–1310, 2017.\n\n[8] Mehta, D.; Sridhar, S.; Sotnychenko, O.; Rhodin, H.;\nShafiei, M.; Seidel, H.-P.; Xu, W.; Casas, D.; Theobalt,\nC. VNect: Realtime 3D human pose estimation with\na single RGB camera. ACM Transactions on Graphics\nVol. 36, No. 4, Article No. 44, 2017.\n\n[9] Alldieck, T.; Magnor, M.; Xu, W.; Theobalt, C.; Pons-\nMoll, G. Video based reconstruction of 3D people\nmodels. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 8387–8397,\n2018.\n\n[10] Kanazawa, A.; Black, M. J.; Jacobs, D. W.; Malik, J.\nEnd-to-end recovery of human shape and pose. In:\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 7122–7131, 2018.\n\n[11] Varol, G.; Ceylan, D.; Russell, B.; Yang, J.; Yumer,\nE.; Laptev, I. Bodynet: Volumetric inference of 3D\nhuman body shapes. In: Proceedings of the European\nConference on Computer Vision, 20–36, 2018.\n\n[12] Zheng, Z.; Yu, T.; Wei, Y.; Dai, Q.; Liu, Y. Deephuman:\n3D human reconstruction from a single image. In:\n\nProceedings of the IEEE International Conference on\nComputer Vision, 7739–7749, 2019.\n\n[13] Saito, S.; Huang, Z.; Natsume, R.; Morishima, S.;\nKanazawa, A.; Li, H. PIFu: Pixel-aligned implicit\nfunction for high-resolution clothed human digitization.\nIn: Proceedings of the IEEE International Conference\non Computer Vision, 2304–2314, 2019.\n\n[14] Xu, Y.; Zhu, S.-C.; Tung, T. Denserac: Joint 3D pose\nand shape estimation by dense render-and-compare. In:\nProceedings of the IEEE International Conference on\nComputer Vision, 7760–7770, 2019.\n\n[15] Smith, D.; Loper, M.; Hu, X.; Mavroidis, P.; Romero,\nJ. FACSIMILE: Fast and accurate scans from an image\nin less than a second. In: Proceedings of the IEEE\nInternational Conference on Computer Vision, 5329–\n5338, 2019.\n\n[16] Alldieck, T.; Magnor, M.; Bhatnagar, B. L.; Theobalt,\nC.; Pons-Moll, G. Learning to reconstruct people in\nclothing from a single RGB camera. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern\nRecognition, 1175–1186, 2019.\n\n[17] Kolotouros, N.; Pavlakos, G.; Black, M. J.; Daniilidis,\nK. Learning to reconstruct 3D human pose and shape\nvia modelfitting in the loop. In: Proceedings of the\nIEEE International Conference on Computer Vision,\n2252–2261, 2019.\n\n[18] Liang, J.; Lin, M. C. Shape-aware human pose and\nshape reconstruction using multi-view images. In:\nProceedings of the IEEE International Conference on\nComputer Vision, 4352–4362, 2019.\n\n[19] Yang, S.; Pan, Z. R.; Amert, T.; Wang, K.; Yu, L.\nC.; Berg, T.; Lin, M. C. Physics-inspired garment\nrecovery from a single-view image. ACM Transactions\non Graphics Vol. 37, No. 5, Article No. 170, 2018.\n\n[20] Yang, S.; Liang, J.; Lin, M. C.; Learning-based cloth\nmaterial recovery from video. In: Proceedings of the\nIEEE International Conference on Computer Vision,\n4383–4393, 2017.\n\n[21] Qiao, Y. L.; Liang, J. B.; Koltun, V.; Lin, M. C.\nScalable differentiable physics for learning and control.\narXiv preprint arXiv:2007.02168, 2020.\n\n[22] De Avila Belbute-Peres, F.; Smith, K. A.; Allen, K.;\nTenenbaum, J.; Kolter, J. Z. End-to-end differentiable\nphysics for learning and control. In: Proceedings of the\nAdvances in Neural Information Processing Systems,\n2018.\n\n[23] Degrave, J.; Hermans, M.; Dambre, J.; Wyffels, F.\nA differentiable physics engine for deep learning in\nrobotics. Frontiers in Neurorobotics Vol. 13, 6, 2019.\n\n[24] Liang, J.; Lin, M.; Koltun, V. Differentiable cloth\nsimulation for inverse problems. In: Proceedings of\nthe 33rd Conference on Neural Information Processing\nSystems, 2019.\n\n\n\nMachine learning for digital try-on: Challenges and progress 167\n\n[25] Hu, Y.; Liu, J.; Spielberg, A.; Tenenbaum, J.\nB.; Freeman, W. T.; Wu, J.; Rus, D.; Matusik,\nW. ChainQueen: A real-time differentiable physical\nsimulator for soft robotics. In: Proceedings of the\nInternational Conference on Robotics and Automation,\n6265–6271, 2019.\n\n[26] Hu, Y. M.; Anderson, L.; Li, T. M.; Sun, Q.; Carr, N.;\nRagan-Kelley, J.; Durand, F. DiffTaichi: Differentiable\nprogramming for physical simulation. arXiv preprint\narXiv:1910.00935, 2019.\n\n[27] Liu, K. X.; Zeng, X. Y.; Bruniaux, P.; Tao, X. Y.; Yao,\nX. F.; Li, V.; Wang, J. 3D interactive garment pattern-\nmaking technology. Computer-Aided Design Vol. 104,\n113–124, 2018.\n\n[28] Huang, P.; Yao, J.; Zhao, H. Automatic realistic\n3D garment generation based on two images. In:\nProceedings of the International Conference on Virtual\nReality and Visualization, 250–257, 2016.\n\n[29] Wang, T. Y.; Ceylan, D.; Popović, J.; Mitra, N. J.\nLearning a shared shape space for multimodal garment\ndesign. ACM Transactions on Graphics Vol. 37, No. 6",
      "metadata_storage_size": 3040236,
      "metadata_storage_last_modified": "2023-10-25T02:14:30Z",
      "metadata_storage_name": "Liang-Lin2021_Article_MachineLearningForDigitalTry-o.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9rbm1pbmluZzRzdG9yYWdlYWNjb3VudC5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXJzL0xpYW5nLUxpbjIwMjFfQXJ0aWNsZV9NYWNoaW5lTGVhcm5pbmdGb3JEaWdpdGFsVHJ5LW8ucGRm0",
      "metadata_author": "Administrator",
      "metadata_title": "01-CVM0189.pdf",
      "metadata_creation_date": "2021-04-14T08:29:06Z",
      "people": [
        "Junbang Liang1",
        "Ming C. Lin1",
        "J. Liang",
        "M. C. Lin",
        "Liang",
        "Lin",
        "Yang",
        "Euler",
        "Qiao",
        "Frobenius",
        "Huang",
        "Wang",
        "Zheng, Z. H.",
        "Zhang, H. T.",
        "Zhang, F. L.",
        "Mu, T. J.",
        "Dibra, E.",
        "Jain, H.",
        "Öztireli, C.",
        "Ziegler, R",
        "Gross",
        "M. HS-Nets",
        "Bălan, A. O.",
        "Black, M. J.",
        "Forsyth, D.",
        "Torr, P",
        "Zisserman,",
        "Lassner, C.",
        "Romero, J.",
        "Kiefel, M",
        "Bogo, F.",
        "Black",
        "M. J.",
        "Gehler, P. V",
        "Loper, M",
        "Mahmood, N",
        "Pons-Moll, G.",
        "Wei, S",
        "Ramakrishna, V",
        "Kanade, T.",
        "Sheikh, Y",
        "Cao, Z",
        "Simon, T.",
        "Wei, S.",
        "Sheikh, Y.",
        "Mehta, D",
        "Sridhar, S.",
        "Sotnychenko,",
        "Rhodin, H.",
        "Shafiei, M",
        "Seidel",
        "Xu, W.",
        "Casas, D.",
        "Theobalt",
        "C. VNect",
        "Alldieck, T",
        "Magnor, M.",
        "Theobalt, C.",
        "Pons",
        "Moll, G",
        "Kanazawa, A.",
        "Jacobs, D. W.",
        "Malik, J.",
        "Varol, G.",
        "Ceylan, D.",
        "Russell, B.",
        "Yang, J.",
        "Yumer",
        "Laptev, I",
        "Zheng, Z.",
        "Yu, T.",
        "Wei, Y.",
        "Dai, Q",
        "Liu, Y. Deephuman",
        "Saito, S",
        "Huang, Z.",
        "Natsume, R.",
        "Morishima, S",
        "Li",
        "Xu, Y",
        "Zhu, S.-C.",
        "Tung",
        "T. Denserac",
        "Smith, D.",
        "Hu",
        "Mavroidis, P",
        "Romero",
        "Alldieck, T.",
        "Bhatnagar, B. L.",
        "Pons-Moll, G",
        "Kolotouros",
        "Pavlakos, G.",
        "Daniilidis",
        "K",
        "Liang, J.",
        "Lin, M. C.",
        "Yang, S.",
        "Pan, Z. R.",
        "Amert, T.",
        "Wang, K.",
        "Yu, L.",
        "Berg, T.",
        "Qiao, Y. L.",
        "Liang, J. B.",
        "Koltun",
        "De Avila Belbute-Peres, F.",
        "Smith, K. A.",
        "Allen, K.",
        "Tenenbaum, J.",
        "Kolter, J. Z.",
        "Degrave, J.",
        "Hermans, M.",
        "Dambre, J.",
        "Wyffels, F",
        "Lin, M.",
        "Hu, Y.",
        "Liu, J.",
        "Spielberg, A.",
        "Tenenbaum, J",
        "Freeman, W. T.",
        "Wu, J.",
        "Rus, D.",
        "Matusik",
        "W. ChainQueen",
        "Hu, Y. M.",
        "Anderson, L.",
        "Li, T. M.",
        "Sun, Q.",
        "Carr, N.",
        "Ragan-Kelley, J.",
        "Durand, F.",
        "Liu, K. X.",
        "Zeng, X. Y.",
        "Bruniaux, P",
        "Tao, X. Y.",
        "X. F",
        "Wang, J.",
        "Huang, P.",
        "Yao, J.",
        "Zhao, H.",
        "Wang, T. Y.",
        "Popović, J.",
        "Mitra, N. J."
      ],
      "organizations": [
        "University of Maryland",
        "Springer UNIVERSITY PRESS",
        "RNN",
        "VideoCloth",
        "CNN",
        "LSTM",
        "dt",
        "Iribe",
        "National Science Foundation",
        "ECCV",
        "Springer",
        "ACM",
        "IEEE",
        "arXiv",
        "Systems"
      ],
      "locations": [
        "retail store",
        "USA",
        "TSINGHUA",
        "neighborhood",
        "Berlin"
      ],
      "keyphrases": [
        "variable 3D human body shapes",
        "TSINGHUA Springer UNIVERSITY PRESS",
        "Computational Visual Media",
        "Ming C. Lin1",
        "The Author(s",
        "direct visual impression",
        "M. C. Lin",
        "2D image learning",
        "substantial technological gap",
        "human body estimation",
        "open research issues",
        "three major challenges",
        "human shapes",
        "open issues",
        "accurate estimation",
        "extensive research",
        "open challenges",
        "2 Open problems",
        "Machine learning",
        "Review Article",
        "Junbang Liang1",
        "economic benefits",
        "practical constraints",
        "accurate sizing",
        "art approaches",
        "future directions",
        "rapid pace",
        "recent years",
        "retail store",
        "attractive alternative",
        "user experience",
        "fashion shopping",
        "physical try",
        "image- editing",
        "College Park",
        "J. Liang",
        "ultimate goal",
        "real worlds",
        "physical worlds",
        "consumer devices",
        "faithful recovery",
        "sewing patterns",
        "traditional methods",
        "constrained problems",
        "learning-based approaches",
        "machine-learning algorithms",
        "tedious data",
        "challenging problems",
        "other areas",
        "several reasons",
        "material modeling",
        "accurate modeling",
        "faithful modeling",
        "garment material",
        "garment pieces",
        "recent advances",
        "template demonstrations",
        "realistic demonstration",
        "notable impact",
        "unseen images",
        "garment modeling",
        "digital try",
        "Abstract Digital",
        "tremendous potential",
        "problem description",
        "1 University",
        "Vol.",
        "June",
        "progress",
        "systems",
        "commerce",
        "people",
        "lives",
        "development",
        "realism",
        "complete",
        "state",
        "Keywords",
        "1 Introduction",
        "Consumers",
        "situation",
        "clothes",
        "needs",
        "attention",
        "technology",
        "Maryland",
        "USA",
        "mail",
        "umd",
        "Manuscript",
        "fast",
        "customer",
        "garments",
        "lossless",
        "transformation",
        "virtual",
        "paper",
        "sizes",
        "ease",
        "design",
        "manipulation",
        "end-users",
        "training",
        "optimization",
        "processing",
        "importance",
        "present",
        "improvements",
        "solutions",
        "significant",
        "section",
        "shoppers",
        "direct 3D body scanning",
        "full fabric digital model",
        "accurate human shape estimation",
        "parametric human model",
        "commodity mobile devices",
        "standard body shapes",
        "uncovered body parts",
        "2D/3D body skeletons",
        "Human body reconstruction",
        "human body geometry",
        "entire human body",
        "notable technology gap",
        "voxel- based representation",
        "successful digital try",
        "3D humanoid mesh",
        "human-body reconstruction problem",
        "3 Human shape estimation",
        "many other works",
        "machine learning methods",
        "Accurate estimation",
        "digital representation",
        "body movements",
        "shape learning",
        "accurate results",
        "perceptual gap",
        "3D mesh-based",
        "digital worlds",
        "3D animation",
        "digital surveillance",
        "early works",
        "recent works",
        "learning process",
        "sizing systems",
        "Different fabric",
        "actual material",
        "real-world examples",
        "two approaches",
        "2D image-based",
        "photo-realistic rendering",
        "considerable effort",
        "2D images",
        "user-friendly design",
        "realistic animation",
        "Previous work",
        "cloud computing",
        "animation speed",
        "high-quality, interactive",
        "computer animation",
        "special effects",
        "augmented environments",
        "popular topic",
        "specialized hardware",
        "RGB images",
        "optimization problem",
        "silhouette difference",
        "objective function",
        "target function",
        "response time",
        "network models",
        "real-world datasets",
        "joint positions",
        "various ways",
        "current state",
        "different brands",
        "garment materials",
        "garment database",
        "key consideration",
        "other factors",
        "online shopping",
        "regression network",
        "body-joint losses",
        "lack",
        "consistency",
        "standardization",
        "persons",
        "proportion",
        "consumers",
        "correspondence",
        "customers",
        "appearance",
        "advantages",
        "drawbacks",
        "large",
        "support",
        "variation",
        "case",
        "backend",
        "motion",
        "pose",
        "variety",
        "areas",
        "challenging",
        "interest",
        "excellent",
        "adoption",
        "input",
        "one",
        "size",
        "output",
        "algorithms",
        "major",
        "tight",
        "advances",
        "parameters",
        "techniques",
        "annotations",
        "most",
        "Ref.",
        "complex data generation pipeline",
        "Regression Regression Regression View",
        "iterative value correction structure",
        "01,1 Regression Regression Regression Block",
        "synthetic data generation pipeline",
        "Multi-View Multi-Stage Regression Network",
        "convenient registration process",
        "larger garment dataset",
        "4 Garment material modeling",
        "parameter prediction block",
        "image style transfer",
        "single view image",
        "good pose estimation",
        "2 C1,2 Block 2,2 Block",
        "unified human shape",
        "human body parameters",
        "Network structure",
        "static multi-view",
        "multi-stage framework",
        "recurrent structure",
        "RNN-like structure",
        "No C",
        "body pose",
        "error correction",
        "multi-view inputs",
        "estimated pose",
        "body shapes",
        "human bodies",
        "Recovered body",
        "Block A",
        "key contribution",
        "several stages",
        "image feature",
        "input guesses",
        "same time",
        "3D joints",
        "loss computation",
        "global orientation",
        "cyclic form",
        "corrective values",
        "ground-truth parameters",
        "performance gap",
        "real-world images",
        "other variables",
        "skin color",
        "3D backgrounds",
        "subtle elements",
        "higher expense",
        "Image Encoder",
        "dreamstime dreamstie",
        "Input image",
        "promising direction",
        "realistic images",
        "important role",
        "Physical recreation",
        "synthetic result",
        "training data",
        "camera projection",
        "image inputs",
        "shape parameters",
        "camera calibration",
        "shape estimation",
        "visual information",
        "traditional datasets",
        "recent progress",
        "162 J. Liang",
        "different views",
        "fixed number",
        "universal model",
        "2,1 Block",
        "effect",
        "ambiguity",
        "Fig.",
        "step",
        "gamma",
        "researchdirections",
        "humanmultiview",
        "Challenges",
        "permission",
        "2D",
        "features",
        "CNNs",
        "use",
        "gradient",
        "Experiments",
        "art",
        "example",
        "regularization",
        "cases",
        "diversity",
        "addition",
        "hair",
        "GAN",
        "results",
        "legs",
        "chest",
        "person",
        "fn",
        "Introduction",
        "fabric",
        "image signal feature extraction method",
        "temporal sequence learning method",
        "deep neural network model",
        "material parameter sensitivity analysis",
        "garment material modeling problem",
        "spring–mass systems",
        "simple feed-forward operation",
        "varying lighting conditions",
        "compelling visual simulation",
        "high-level visual features",
        "optimal material parameter",
        "traditional optimization methods",
        "image features",
        "challenging problem",
        "fabric modeling",
        "visual effects",
        "machine-learning methods",
        "based simulation",
        "physical model",
        "popular model",
        "anisotropic model",
        "art model",
        "challenging task",
        "physical properties",
        "physical behaviour",
        "semantic primitives",
        "physical behavior",
        "real material",
        "two implications",
        "many ways",
        "finite elements",
        "realistic results",
        "isotropic properties",
        "Poisson ratio",
        "different behaviors",
        "4.2 Learning-based estimation",
        "fast feedback",
        "solution space",
        "material sub-space",
        "positive coefficient",
        "larger number",
        "visual appearance",
        "material properties",
        "consequent movement",
        "material cloning",
        "video recording",
        "other object",
        "real-world scenarios",
        "new insights",
        "natural extension",
        "partial occlusion",
        "differentiable physics",
        "geometric differences",
        "garment prototyping",
        "material parameters",
        "sufficient amount",
        "long time",
        "real time",
        "material type",
        "small amount",
        "motion patterns",
        "material basis",
        "data samples",
        "CNN layer",
        "single piece",
        "human body",
        "cloth simulations",
        "cloth meshes",
        "LSTM layer",
        "cloth motion",
        "materials",
        "researchers",
        "underlying",
        "computer",
        "Young",
        "choice",
        "weave",
        "predictions",
        "applications",
        "Yang",
        "videos",
        "code",
        "VideoCloth",
        "respect",
        "deformation",
        "sub-spaces",
        "texture",
        "mapping",
        "framework",
        "clothing",
        "approach",
        "CNN CNN CNN Material type LSTM cell",
        "CNN conv5-layer activation visualization",
        "1 Classifier 1 Tu Tv2 Tv",
        "high dimensional simulation problem",
        "material parameter estimation",
        "finite element method",
        "modern simulation algorithms",
        "stable integration results",
        "other differentiable simulations",
        "Differentiable simulation embedding",
        "implicit Euler method",
        "task-specific loss function",
        "general differentiable collision",
        "differentiable cloth simulation",
        "high dimensionality",
        "M̂a",
        "general approach",
        "physics simulations",
        "implicit differentiation",
        "collision response",
        "collision constraints",
        "Concatenate op",
        "unseen video",
        "input videos",
        "typical usage",
        "Similar processes",
        "pixel space",
        "3D space",
        "visual effect",
        "physics work",
        "particle-grid systems",
        "first work",
        "follow-up work",
        "computational flow",
        "common approach",
        "impact zones",
        "linear solver",
        "input parameters",
        "discontinuous states",
        "constraint matrix",
        "several techniques",
        "physical solution",
        "mass matrix",
        "linear solution",
        "solution z",
        "original paper",
        "complete � Liang",
        "164 J. Liang",
        "motion control",
        "clothes materials",
        "handling algorithm",
        "static LCP",
        "corresponding gradients",
        "gradient values",
        "model",
        "parts",
        "simulated",
        "skirt",
        "environment",
        "difference",
        "target",
        "network",
        "distance",
        "computing",
        "bodies",
        "dynamics",
        "discretization",
        "order",
        "discontinuity",
        "zero-measure",
        "backpropagation",
        "QR",
        "decomposition",
        "pipeline",
        "Derivatives",
        "Jacobian",
        "forces",
        "acceleration",
        "equation",
        "readers",
        "details",
        "Collisions",
        "contacts",
        "data",
        "edu",
        "virtualtryon",
        "differentiablecloth",
        "3.1",
        "3.2",
        "Control Simulation Observations Signals Time Collision Results",
        "multiple adjacent collision pairs",
        "constant diagonal weight matrix",
        "Material parameter estimation results",
        "differentiable simulation pipeline",
        "Differentiable Simulation Layer",
        "continuous collision detection",
        "constant wind force",
        "Trainable Network Layers",
        "original mesh state",
        "impact zone optimization",
        "bending stiffness models",
        "baselines Method Runtime",
        "Integration Response Loss",
        "Collision handling",
        "stiffness matrix",
        "neural network",
        "physics simulator",
        "dynamic approach",
        "minimum change",
        "QP problem",
        "distance function",
        "quadratic form",
        "constraint parameters",
        "loss function",
        "linear system",
        "one contact",
        "linear equation",
        "QR decomposition",
        "high dimensional",
        "three parts",
        "stiffness S",
        "stiffness B.",
        "reaction force",
        "most measurements",
        "linear part",
        "unknown parameters",
        "body dynamics",
        "generalized coordinates",
        "Frobenius norms",
        "simulated result",
        "gradient-based method",
        "smaller errors",
        "computing gradients",
        "optimization formulation",
        "New Observation",
        "The scene",
        "vertex distances",
        "Linear stretching",
        "stiffness error",
        "cloth movement",
        "key observation",
        "tasks",
        "implementation",
        "instances",
        "constraints",
        "collisions",
        "Gz",
        "mass",
        "numbers",
        "variables",
        "inputs",
        "goal",
        "rank",
        "4 vertices",
        "experiments",
        "ability",
        "piece",
        "gravity",
        "density",
        "Table 1",
        "comparison",
        "Qiao",
        "dt",
        "L-BFGS.",
        "Values",
        "iter",
        "Liang",
        "velocity",
        "digital",
        "TM",
        "Gf",
        "inclusion",
        "−",
        "conditional generative adversarial network",
        "learning-based parametric generative model",
        "irregular 3D mesh data",
        "Realistic apparel model generation",
        "automatic garment model generation",
        "interactive apparel design systems",
        "3D garment mesh data",
        "U–V space",
        "latent space learning",
        "relative geometric information",
        "machine learning algorithms",
        "National Science Foundation",
        "regular image data",
        "arbitrary human motions",
        "garment sewing patterns",
        "general cloth model",
        "garment generation algorithm",
        "2D displacement image",
        "deep garment representation",
        "One possible approach",
        "The 2D representation",
        "garment material prediction",
        "other body shapes",
        "human shape estimation",
        "realistic 3D",
        "one model",
        "fashion apparel",
        "many challenges",
        "estimation accuracy",
        "One possibility",
        "image sketches",
        "Garment modeling",
        "garment models",
        "garment type",
        "rapid changes",
        "fashion trends",
        "growing need",
        "different applications",
        "Application requirements",
        "diverse set",
        "different types",
        "universal pipeline",
        "recent work",
        "two problems",
        "different values",
        "traditional CNN",
        "different person",
        "virtual reality",
        "excellent potential",
        "open problems",
        "daily tasks",
        "good start",
        "optimization-based methods",
        "intersection loss",
        "new solution",
        "substantial breakthroughs",
        "Iribe Professorship",
        "166 J. Liang",
        "general garments",
        "same body",
        "unified representation",
        "limited topology",
        "Different topology",
        "external constraints",
        "hard constraints",
        "art methods",
        "multiple garments",
        "neighborhood",
        "approximation",
        "customization",
        "extent",
        "Huang",
        "front",
        "Wang",
        "al.",
        "retargeting",
        "T-shirts",
        "skirts",
        "difficulties",
        "cGAN",
        "6 Conclusions",
        "scenario",
        "wind",
        "applicability",
        "area",
        "investigation",
        "directions",
        "Acknowledgements",
        "research",
        "part",
        "5",
        "Y. Realtime multi- person 2D pose estimation",
        "Realtime 3D human pose estimation",
        "Image-based clothes changing system",
        "2D human representations",
        "Convolutional pose machines",
        "Shape-aware human pose",
        "Joint 3D pose",
        "convolutional neural networks",
        "multi-person linear model",
        "part affinity fields",
        "single RGB camera",
        "human body shapes",
        "4th International Conference",
        "IEEE International Conference",
        "A. Eds. Springer",
        "Visual Media Vol.",
        "3D human reconstruction",
        "M. J. SMPL",
        "IEEE Conference",
        "human shape",
        "human digitization",
        "3D Vision",
        "European Conference",
        "Y. Deephuman",
        "single image",
        "naked truth",
        "Computer Vision",
        "Lecture Notes",
        "Computer Science",
        "ACM Transactions",
        "Article No.",
        "Pons- Moll",
        "end recovery",
        "I. Bodynet",
        "Volumetric inference",
        "high-resolution clothed",
        "accurate scans",
        "shape reconstruction",
        "multi-view images",
        "3D people",
        "a second",
        "Öztireli",
        "H.-P.",
        "H. PIFu",
        "J. FACSIMILE",
        "M. HS-Nets",
        "F. L.",
        "B. L.",
        "A. O.",
        "C. VNect",
        "L. C.",
        "T. Denserac",
        "S.-E.",
        "S.-C.",
        "Z. H.",
        "M. C.",
        "H. T.",
        "T. J.",
        "P. V.",
        "Z. R.",
        "D. W.",
        "References",
        "Zhang",
        "Computational",
        "Dibra",
        "Jain",
        "Ziegler",
        "Gross",
        "silhouettes",
        "Proceedings",
        "Black",
        "ECCV",
        "Forsyth",
        "Torr",
        "Zisserman",
        "Berlin",
        "Romero",
        "Kiefel",
        "Bogo",
        "Gehler",
        "loop",
        "Recognition",
        "Loper",
        "Mahmood",
        "N.",
        "Pons-Moll",
        "G.",
        "Graphics",
        "Wei",
        "Ramakrishna",
        "Kanade",
        "Sheikh",
        "Cao",
        "Simon",
        "Mehta",
        "Sridhar",
        "Sotnychenko",
        "Rhodin",
        "Shafiei",
        "Seidel",
        "Xu",
        "Casas",
        "Theobalt",
        "Alldieck",
        "Magnor",
        "Video",
        "models",
        "Kanazawa",
        "Jacobs",
        "Malik",
        "Varol",
        "Ceylan",
        "Russell",
        "Yumer",
        "Laptev",
        "Zheng",
        "Dai",
        "Q.",
        "Liu",
        "Saito",
        "Natsume",
        "Morishima",
        "Pixel-aligned",
        "function",
        "Zhu",
        "Tung",
        "Smith",
        "X.",
        "Mavroidis",
        "Fast",
        "less",
        "Bhatnagar",
        "Kolotouros",
        "Pavlakos",
        "Daniilidis",
        "K.",
        "modelfitting",
        "Pan",
        "Amert",
        "Berg",
        "Neural Information Processing Systems",
        "real-time differentiable physical simulator",
        "V. Differentiable cloth simulation",
        "De Avila Belbute-Peres",
        "shared shape space",
        "Scalable differentiable physics",
        "differentiable physics engine",
        "3D garment generation",
        "multimodal garment design",
        "physical simulation",
        "Learning-based cloth",
        "Physics-inspired garment",
        "Computer-Aided Design",
        "33rd Conference",
        "single-view image",
        "Graphics Vol.",
        "inverse problems",
        "W. T.",
        "W. ChainQueen",
        "arXiv preprint",
        "two images",
        "Virtual Reality",
        "X. Y.",
        "T. Y.",
        "T. M.",
        "K. X.",
        "Y. M.",
        "material recovery",
        "deep learning",
        "F. DiffTaichi",
        "X. F.",
        "Y. L.",
        "soft robotics",
        "J. B.",
        "J. Z.",
        "K. A.",
        "N. J.",
        "Lin",
        "S.",
        "video",
        "Koltun",
        "control",
        "Allen",
        "Tenenbaum",
        "Kolter",
        "Advances",
        "Degrave",
        "Hermans",
        "Dambre",
        "Wyffels",
        "Frontiers",
        "Neurorobotics",
        "Hu",
        "Spielberg",
        "Freeman",
        "Wu",
        "Rus",
        "D.",
        "Matusik",
        "Automation",
        "Anderson",
        "Sun",
        "Carr",
        "Ragan-Kelley",
        "Durand",
        "programming",
        "Zeng",
        "Bruniaux",
        "P.",
        "Tao",
        "Yao",
        "Zhao",
        "H.",
        "Visualization",
        "Popović",
        "Mitra"
      ],
      "language": "en",
      "masked_text": "\nComputational Visual Media\n***************************************** Vol. 7, No. ************, 159–167\n\nReview Article\n\nMachine learning for digital try-on: Challenges and progress\n\n************** (�), ************\n\nc© **********(s) ****.\n\nAbstract Digital try-on systems for e-commerce have\nthe potential to change people’s lives and provide notable\neconomic benefits. However, their development is limited\nby practical constraints, such as accurate sizing of the\nbody and realism of demonstrations. We enumerate three\nopen challenges remaining for a complete and easy-to-use\ntry-on system that recent advances in machine learning\nmake increasingly tractable. For each, we describe\nthe problem, introduce state-of-the-art approaches, and\nprovide future directions.\n\nKeywords machine learning; digital try-on; garment\nmodeling; human body estimation; material\nmodeling\n\n1 Introduction\nE-commerce has grown at a rapid pace in recent\nyears. Consumers ***** are more likely to shop\nonline than to visit a retail store. The situation is\nmuch more complicated, however, when it comes to\nbuying clothes. People need to know how a garment\nfits on them, how it looks, and how it feels. Digital\ntry-on systems can potentially satisfy these needs,\nproviding a direct visual impression, and possibly\ncustomized clothes sizing as well. Therefore, it has\ndrawn much attention as an attractive alternative to\nimprove the user experience and popularize online\nfashion shopping.\n\nHowever, the technology is still far from practical,\neasy-to-use, and adequate to replace physical try-on.\nCurrently, most try-on systems rely on either image-\nediting, copy-pasting, or template demonstrations,\n\n1 University of Maryland, ***************************.\nE-mail: ********, ****************** (�); *********,\n**************.\n\nManuscript received: **********; accepted: **********\n\nwhile the ultimate goal is a fast and realistic try-on\nsystem adaptive to each ********’s body. There is\nstill a substantial technological gap between modeling\nand demonstrating garment fitting in the digital and\nreal worlds, including fast and realistic demonstration,\naccurate modeling of human body and garments,\nfaithful modeling of garment material, and lossless\ntransformation of garments between virtual and\nphysical worlds.\n\nIn this paper, we present some open research issues\nthat contribute to this technological gap, including:\n1. accurate estimation of human shapes and sizes\n\nusing consumer devices,\n2. faithful recovery of garment materials via (online)\n\nimages, and\n3. ease of design and manipulation of sewing patterns\n\nand garment pieces by *********.\nAlthough traditional methods have made important\n\nprogress on these under-constrained problems,\nlearning-based approaches have shown tremendous\npotential to make a notable impact. Compared to\ntraditional methods, machine-learning algorithms are\nusually much faster since training and optimization\nare performed offline. They are also good at\ngeneralizing to unseen images without the need for\ntedious data pre-processing. While extensive research\nexists on ** image learning, machine learning of\nhighly variable ** human body shapes is still far\nfrom mature, which is the reason why the open issues\ndescribed above remain elusive.\n\nFor each problem listed above, we motivate its\nimportance, provide a problem description, and\npresent state-of-the-art approaches with potential\nfor improvements. We believe that solutions to\nthese challenging problems will lead to significant\nadvances in digital try-on, as well as other areas of\ne-commerce.\n\n159\n\n  \n\n ********************************** \n\n\n\n160 ********, *********\n\n2 Open problems\nIn this section we first introduce three major\nchallenges that limit digital try-on technology from\nbeing widely adopted and accepted by ********.\nThere are several reasons why shoppers still prefer\nphysical try-on. Firstly, consumers are unsure if what\nthey buy online will fit them well. Although general\nsizing systems exist, their lack of consistency and\nstandardization across different brands and garment\nmaterials can often make it difficult to size clothes,\nespecially for persons with non-standard body shapes\nand proportion. Accurate estimation of human body\nshape is the key to successful digital try-on. Secondly,\nfabric is usually a key consideration when shopping\nfor clothes. Different fabric affects how garments\nlook and fit, how consumers would wear them, and\nwhether or not they would buy them. However, the\ncorrespondence between the actual material and its\ndigital representation are not well understood. It is\nalso challenging to acquire a full fabric digital model\nfrom real-world examples.\n\nFor the *********, appearance is as critical as other\nfactors. There are two approaches to displaying\ngarments: ** image-based, and ** mesh-based\nwith photo-realistic rendering. They have different\nadvantages and drawbacks, but both need a large\ngarment database for support. While creating a **\ngarment takes considerable effort, ** images often\nsuffer from a lack of variation and are much more\ndifficult to customize. In either case, the try-on\nsystem needs a user-friendly design and manipulation\nbackend. Last, but not least, a fast and realistic\nanimation of the garments in motion along with\nbody movements greatly improves the **** experience.\nAlthough it is not so critical as other factors, it\nwould effectively reduce the perceptual gap between\nthe real and digital worlds for online shopping.\nPrevious work has proposed using cloud computing\nto improve the animation speed, but there is still a\nnotable technology gap for high-quality, interactive\n** animation of clothes.\n\n3 Human shape estimation\nAs noted, accurate human shape estimation is key to\nenabling digital try-on. Human body reconstruction,\nconsisting of pose and shape estimation, has been\nwidely studied in a variety of areas, including digital\n\nsurveillance, computer animation, special effects, and\nvirtual and augmented environments. Yet, it remains\na challenging and popular topic of interest. While\ndirect ** body scanning can provide excellent and\naccurate results, its adoption is somewhat limited by\nthe required specialized hardware. RGB images are\nwidely available for input to digital try-on and can\nbe easily captured using commodity mobile devices.\nAlthough purely image-based try-on methods have\nbeen proposed [1], learning-based ** body estimation\nis more widely applicable in that the ** body can be\narticulated and so re-posed and re-targeted.\n\nWe define the human-body reconstruction problem\ninformally as, given one or more RGB images, to\nestimate the human body geometry and size, and\noutput (preferably) a ** humanoid mesh. Traditional\nalgorithms often formulate it as an optimization\nproblem, in which the silhouette difference is a major\npart of the objective function [2]. Therefore, these\nmethods either require the human to wear tight\nclothes, or alternatively relax the target function\nto be unilateral on uncovered body parts [3], or\nto point correspondences [4]. The use of machine\nlearning methods in this problem has led to significant\nadvances. Firstly, it has moved the algorithm from\nonline to offline, significantly reducing response time.\nSecond, by using a parametric human model [5],\none can easily construct a regression network for\nthe parameters while the losses needed can also be\ninferred from them. While early works proposed\nnetwork models for only **/3D body skeletons [6–\n8], more recent works have introduced techniques to\nperform regression for the entire human body—either\nusing a parametric human model [9, 10] or a voxel-\nbased representation [11–13]. As annotations in most\nreal-world datasets contain only joint positions, the\nlearning process has been refined in various ways [14–\n17]. The current state of the art is the recent work\nby Ref. [18] �. It emphasizes shape learning, while\nmany other works often focus on body-joint losses,\nbut neglect the effect of body shapes.\n\nThe key contribution of Ref. [18] is a multi-view,\nmulti-stage framework to address ambiguity caused by\ncamera projection (see Fig. 1). Their model performs\nseveral stages of error correction. Each of the image\ninputs is passed on step by step; at each step, a shared-\n\n� ***** and ***’s data and code are available at **********************\nresearchdirections/virtualtryon/humanmultiview\n\n ********************************** \n\n\n\nMachine learning for digital try-on: Challenges and progress 161\n\nFig. 1 Network structure from Ref. [18]. By using an iterative value correction structure, visual information from different views is effectively\nintegrated to provide a unified human shape. Reproduced with permission from Ref. [18], c© The Author(s) ****.\n\nparameter prediction block computes the correction\nbased on the image feature and the input guesses.\nThe camera and the human body parameters are\nestimated at the same time, projecting the predicted\n** joints back to ** for loss computation. The\nestimated pose and shape parameters are shared\namong all views, while each view maintains its own\ncamera calibration and global orientation. Their\nproposed framework uses a recurrent structure,\nmaking it a universal model applicable to any number\nof views. At the same time, it couples shareable\ninformation across different views so that the human\nbody pose and shape are optimized using image\nfeatures from all views. Unlike static multi-view\n**** which have a fixed number of inputs, they\nmake use of the RNN-like structure in a cyclic form to\naccept any number of views, and prevent the gradient\nvanishing by predicting corrective values instead of\nupdating parameters in each regression block.\n\nExperiments have shown that, after training, this\nmodel can form a single view image, provide equally\ngood pose estimation as the state of the art, and\nprovide considerably improved pose estimation when\nusing multi-view inputs, leading to better shape\nestimation across all datasets. An example is\ndemonstrated in Fig. 2. Moreover, a physically-based\nsynthetic data generation pipeline is introduced to\nenrich the training data, which is very helpful for\n\nshape estimation and regularization in cases that\ntraditional datasets do not capture. While synthetic\ndata improves the diversity of human bodies with\nground-truth parameters, a larger garment dataset\nand a more convenient registration process are needed\nto minimize the performance gap between real-world\nimages and synthetic data. In addition, other\nvariables such as hair, skin color, and ** backgrounds\nare subtle elements that can influence the perceived\nrealism of the synthetic data at the higher expense of\na more complex data generation pipeline. With the\nrecent progress in image style transfer using GAN, a\n\nFig. 2 Prediction results using the state of the art [18]. The model\ncaptures the shape of the human body by learning from synthetic\ndata. The recovered legs and chest are close to those of the person\nin the image. Reproduced with permission from Ref. [18], c© The\n******(s) ****.\n\n A 1,1 102,1 3,1 b b b fi f1 f1 f1 I View 1 01,1 Regression Regression Regression Block **,1 Block 3,1 Block (1,1) c (2,1) C (3,1) - - 103.2 - b @ 2,2 b b - f2 f2 f2 f2 Image Encoder Regression Regression Regression - View 2 C1,2 Block 2,2 Block 3.2 Block No C (1,2) C (2,2) C (3,2) - - ... 1 1 b b b ... ... fn fn fn - Regression Regression Regression View n 1,77 Block A2,n Block 3,n Block (1,n) (2,n) C (3,n) C - - fn @3,1 1 b b b Stage 1 Stage 2 Stage 3 Multi-View Multi-Stage Regression Network \n\n dreamstime dreamstie (a) Input image (b) Recovered body \n\n\n\n162 ********, *********\n\npromising direction is to transfer the synthetic result\nto more realistic images to further improve the result.\n\n4 Garment material modeling\n4.1 Introduction\nGarment material plays an important role in digital\ntry-on systems. Physical recreation of the fabric not\nonly gives a compelling visual simulation of the cloth,\nbut also affects how the garment feels and fits on\nthe body. However, fabric modeling is a challenging\ntask: the appearance and physical properties of\nthe garment are determined not only by the type\nof materials the clothes are made of, but also by\nsewing and weave. Thus, *********** often focus on\nthe physical behaviour, rather than the underlying\nsemantic primitives.\n\nHence, we state the garment material modeling\nproblem as follows. Given a sufficient amount of data,\nmodel the material’s physical behavior and physical\nproperties, so that visual effects the same as or similar\nto those of the real material can be reproduced by a\ncomputer. This has two implications: firstly, we need\nto define a physical model of the material, and then\nwe must estimate the parameters in the model.\n\nThere are many ways to model clothes, including\n******–mass systems and finite elements. The latter is\nthe most popular model since it can produce realistic\nresults. While one can use isotropic properties such\nas *****’s modulus and Poisson ratio, an anisotropic\nmodel is a better choice since it can support different\nbehaviors caused by the weave of the material.\n\n4.2 Learning-based estimation\nWhile traditional optimization methods [19] often\ntake a long time to compute material parameters,\nmachine-learning methods can make predictions in\nreal time by a simple feed-forward operation, which\nis more useful in applications that need fast feedback,\nsuch as garment prototyping. The state-of-the-art\nmodel from **** et al. [20] � uses CNNs combined\nwith LSTM to recover material parameters from\nvideos. To constrain both the input and solution\nspace, they choose one of the materials as a basis;\nthe material sub-space is constructed by multiplying\nthis material basis with a positive coefficient. To\nconstruct an optimal material parameter sub-space, a\n\n� **** et al.’s data and code are available at ************************\nVideoCloth\n\nmaterial parameter sensitivity analysis is conducted\nto examine the sensitivity of the material parameters\nκ with respect to the amount of deformation D(κ).\nPhysically based cloth simulations are used to\ngenerate a much larger number of data samples within\nthese sub-spaces, which would otherwise be difficult\nor time-consuming to capture. The cloth meshes are\ngenerated through physically-based simulation, and\nthen rendered as ** images with a randomly assigned\ntexture. Using the data samples, they combine the\nimage signal feature extraction method, a ***, with\nthe temporal sequence learning method, LSTM, to\nlearn the mapping from visual appearance to material.\nAs shown in Fig. 3, the CNN layer is used to extract\nboth low- and high-level visual features, while the\nLSTM layer focuses on learning the mapping between\nthe material properties of the cloth and its consequent\nmovement.\n\nThey demonstrated the proposed framework with\nthe application of “material cloning”. With the\ntrained deep neural network model being able to\ncapture the cloth motion (Fig. 4), the material type\ncan be inferred from a video recording of the motion\nof the cloth in a fairly small amount of time. The\nrecovered material type can be “cloned” onto another\npiece of cloth or garment as shown in Fig. 5.\n\nIn this work, the videos contain only a single piece\nof cloth which does not interact with any other object.\nWhile this is not applicable to all real-world scenarios,\nthis method provides new insights into addressing this\nchallenging problem. A natural extension would be to\nlearn from videos of clothing directly interacting with\nthe human body, under varying lighting conditions\nand partial occlusion.\n\n4.3 Optimization using differentiable physics\nAnother approach to modeling the fabric is to measure\ngeometric differences directly during parameter\n\nFig. 3 Network model from Ref. [20]. The material is modeled\nby learning motion patterns of image features given by ****.\nReproduced with permission from Ref. [20], c© The Author(s) ****.\n\n = *****- 2.2 hn-1 (xl r.2 \". . . , x) 1 Classifier 1 Tu Tv2 Tv\" CNN CNN CNN Material type LSTM cell = Concatenate op T1 T2 rgb Tn rgb rgb \n\n\n\nMachine learning for digital try-on: Challenges and progress 163\n\nFig. 4 Learned CNN conv5-layer activation visualization from\nRef. [20]. Experiments show that the trained model is able to capture\nmoving parts of the cloth even in an unseen video. Reproduced with\npermission from Ref. [20], c© The Author(s) ****.\n\nFig. 5 **** et al. [20] modeled clothes materials in input videos (left),\nand applied those materials to a simulated skirt (right). Reproduced\nwith permission from Ref. [20], c© The ******(s) ****.\n\noptimization. Assuming that the environment is\nknown to the system, computation of the estimated\nmotion and its gradient with respect to the material\nparameters can be achieved using differentiable\nsimulation. A typical usage of differentiable\nsimulation is motion control (see Fig. 6), where the\ndifference to the target is measured and the loss\nbackpropagated to the network. Similar processes\ncan be applied to material parameter estimation as\nwell. By measuring the distance to the target as the\nloss and computing corresponding gradients, either in\npixel space or ***** space, the material parameters\n\ncan be learned or optimized to achieve the desired\ncloth motion or visual effect. Recent differentiable\nphysics work covers rigid bodies [22, 23], cloth [24],\nand particle-grid systems [25, 26]. The state-of-\nthe-art is Ref. [24] �, which proposes a method for\ndifferentiable cloth simulation. It is the first work\nto tackle a high dimensional simulation problem\nand to propose a general differentiable collision\nhandling algorithm. Later, a follow-up work [21]\nextended the algorithm to be applicable to coupled\ndynamics with rigid bodies. Overall, they follow\nthe computational flow of the common approach\nto cloth simulation: discretization using the finite\nelement method, integration using an implicit Euler\nmethod, and collision response on impact zones. They\nuse implicit differentiation in the linear solver and\noptimization in order to compute the gradient with\nrespect to the input parameters. The discontinuity\nintroduced by collision response is negligible because\nthe discontinuous states constitute a zero-measure\nset. During backpropagation in the optimization,\ngradient values can be directly computed after QR\ndecomposition of the constraint matrix. Their\npipeline contains several techniques that can be\nemployed in other differentiable simulations.\n4.3.1 Derivatives of the physical solution\nIn modern simulation algorithms, an implicit *****\nmethod is often used for stable integration results.\nThus the mass matrix M often includes the Jacobian\nof the forces, and is denoted as M̂ to indicate this\ndifference. A linear solver is needed to compute the\nacceleration since it is time-consuming to compute\nM̂−1. Implicit differentiation is used to compute the\ngradients of the linear solution. Given an equation\nM̂a = f with a solution z and propagated gradient\n∂L/∂a|a=z, where L is the task-specific loss function,\nimplicit differentiation is used to derive the gradients.\nWe refer readers to the original paper [24] for more\ndetails.\n4.3.2 Derivatives of the collision response\nA general approach using LCP to integrate collision\nconstraints into physics simulations has been\nproposed, but constructing a static LCP is often\nimpractical in cloth simulation due to the high\ndimensionality. Collisions and contacts which happen\nat each step are very sparse compared to the complete\n\n� ***** et al.’s data and code are available at **********************\nresearchdirections/virtualtryon/differentiablecloth\n\n F-1 F-25 F-3 F-105 F-1 F-25 F-38 F-105 1 F-1 F-3 0.9 F-4 0.8 0.7 0.6 0.5 0.4 0.3 0.2 F-1 F-2 F-4 0.1 F-3 0 \n\n  \n\n\n\n164 ********, *********\n\nFig. 6 Differentiable simulation embedding example from Ref. [21]. The loss can be backpropagated through the physics simulator to the\nneural network, enabling learning tasks such as material modeling and motion control.\n\ndata. Therefore, a dynamic approach is used that\nincorporates collision detection and response.\n\nCollision handling in their implementation is based\non impact zone optimization. It finds all colliding\ninstances using continuous collision detection and\nsets up the constraints for all collisions. In order\nto introduce minimum change to the original mesh\nstate, a QP problem is developed to determine the\nconstraints. Since the signed distance function is\nlinear in x, the optimization takes a quadratic form,\nas shown originally in Ref. [24]:\n\nminimize\nz\n\n1\n2\n\n(z − x)TW (z − x),\n\nsubject to Gz + h � 0\nwhere W is a constant diagonal weight matrix related\nto the mass of each vertex, and G and h are constraint\nparameters. The numbers of variables and constraints\nare n and m, i.e. x ∈ R\n\nn, h ∈ R\nm, and G ∈ R\n\nm×n.\nNote that this optimization problem has inputs x,\nG, and h, and output z. The goal here is to derive\n∂L/∂x, ∂L/∂G, and ∂L/∂h given ∂L/∂z, where L\nis the loss function.\n\nWhen computing the gradient using implicit\ndifferentiation, the dimensionality of the linear system\ncan be very high. Their key observation here is that\nn >> m > rank(G), since one contact often involves 4\nvertices (******* variables) and some contacts may be\nlinearly dependent (e.g., multiple adjacent collision\n\npairs). They minimize the size of the linear equation\nbased on the QR decomposition of G, which is the key\nto accelerating backpropagation of high dimensional\nQP problems.\n\nOne of their experiments shows its ability to\noptimize material parameters from observation. The\nscene features a piece of cloth hanging under gravity\nand subjected to a constant wind force. The material\nmodel consists of three parts: density d, stretching\nstiffness S, and bending stiffness B. The stretching\nstiffness quantifies the reaction force when the cloth\nis stretched; the bending stiffness models how easily\nthe cloth can be bent and folded. Table 1 shows\nresults. They achieve a much smaller error in most\nmeasurements in comparison to the baselines; the\nlinear part of the stiffness matrix is modeled well.\nWith the computed gradient using their model, one\ncan effectively optimize the unknown parameters that\ndominate cloth movement to fit the observed data.\n\nIn follow-up work, **** et al. extended the\ndifferentiable simulation pipeline to couple with\nrigid body dynamics, formulated using generalized\ncoordinates:\n\nd\ndt\n\n⎛\n⎝ q\n\nq̇\n\n⎞\n⎠ =\n\n⎛\n⎝ q̇\n\nq̈\n\n⎞\n⎠ =\n\n⎛\n⎝ q̇\n\nM−1f(q, q̇)\n\n⎞\n⎠\n\nand update the optimization formulation for collision\nresponse accordingly (see Ref. [21] for details):\n\nTable 1 Material parameter estimation results from Ref. [24]. Their proposed method runs faster than L-BFGS. Values of material parameters\nare Frobenius norms of the difference normalized by the Frobenius norm of the target. Values of the simulated result are the average pairwise\nvertex distances normalized by the size of the cloth. The gradient-based method yields much smaller errors than the baselines\n\nMethod\nRuntime\n\n(sec/step/iter)\n\nDensity\n\nerror (%)\n\nLinear stretching\n\nstiffness error (%)\n\nBending stiffness\n\nerror (%)\n\nSimulation\n\nerror (%)\n\nBaseline — 68 ± 46 160 ± 119 70 ± 42 12 ± 3.0\n\nL-BFGS 2.89 ± 0.02 4.2 ± 5.6 72 ± 90 70 ± 43 4.9 ± 3.3\n\n***** et al. [24] 2.03 ± 0.06 1.8 ± 2.0 45 ± 41 77 ± 36 1.6 ± 1.4\n\n New Observation ******** Control Simulation Observations Signals Time Collision Results Integration Response Loss State: velocity, position, material, force ... Trainable Network Layers Differentiable Simulation Layer \n\n\n\nMachine learning for digital try-on: Challenges and progress 165\n\nminimize\nq′\n\n1\n2\n\n(q − q′)TM̂(q − q′)\n\nsubject to Gf(q′) + h � 0\n\nDue to the inclusion of rigid bodies, the constraints\nused in the optimization are no longer linear. When\ncomputing gradients, they linearize the constraints\naround a neighborhood as an approximation to enable\nQR decomposition for acceleration as **********\nmentioned.\n\n5 Garment modeling and design\nRealistic apparel model generation has become\nincreasingly popular, due to the rapid changes in\nfashion trends and the growing need for garment\nmodels in different applications such as virtual try-\non. It is already used even for state-of-the-art\ninteractive apparel design systems [27]. Application\nrequirements mean that it is important to have a\ngeneral cloth model that can represent a diverse set\nof garments. However, there are many challenges\nin automatic garment model generation. Firstly,\ngarments usually have different types of topology,\nespecially for fashion apparel, that makes it difficult\nto design a universal pipeline. Moreover, it is often\nnot straightforward for general garments design to\nbe retargeted onto another body shape, making\ncustomization difficult.\n\nPrevious work has addressed this problem to some\nextent. ***** et al. [28] proposed a realistic **\ngarment generation algorithm based on front and\nback image sketches, but it cannot readily retarget\ngenerated garments to other body shapes. **** et\nal. [29] proposed an algorithm which can conveniently\nperform retargeting, but permits limited topology\nlike T-shirts or skirts. There is no recent work that\naddresses these two problems at the same time.\n\nWe introduce a learning-based parametric\ngenerative model to overcome the above difficulties,\ngiven garment sewing patterns and human body\nshapes as input. One possible approach would be to\ncompute a displacement image on the U–V space\nof the human body as a unified representation of\nthe garment mesh. Different topology and sizes\nof the garment are represented by different values\nin the image. The ** displacement image, as the\nrepresentation of the ** garment mesh data, can\n\nthen be fed into a conditional generative adversarial\nnetwork (cGAN) for latent space learning. The **\nrepresentation for the garment mesh can transfer\nthe irregular ** mesh data to regular image data\nwhere a traditional *** can easily learn. It can also\nextract relative geometric information with respect\nto the human body, enabling garment retargeting to\na different person.\n\n6 Conclusions\nAlthough virtual reality and digital try-on have\nexcellent potential and are rapidly developing, there\nremain open problems before online try-on systems\ncan be widely adopted. We have listed three major\nchallenges, all of which can be addressed or further\nimproved using machine learning algorithms. For\ngarment material prediction, state-of-the-art methods\nare still limited in that the training data is highly\nconstrained: the scenario contains only a piece\nof cloth floating in the wind. To improve its\napplicability to ***** tasks, it is necessary to focus\non solving the problem on a more diverse set of\ninputs. Predicting the material from a garment\non a fixed human body could be a good start,\nbefore generalizing to arbitrary human motions and\npredicting multiple garments on the same body. In\nthe area of human shape estimation, it would be\ninteresting to learn how external constraints could\nimprove estimation accuracy. For example, the shape\nand size of the garment are hard constraints to\nwhich the predicted body should conform. While\noptimization-based methods can integrate these\nconstraints fairly easily, doing so remains elusive\nfor learning-based approaches. One possibility is\nto jointly estimate body and garment together and\nintroduce an intersection loss. This approach would\nrequire a new solution to the open problem of unified\ndeep garment representation, if we do not want to\ntrain one model for every garment type, which could\nbe even more challenging. We believe that substantial\nbreakthroughs in digital try-on are achievable with\nmore investigation in these directions.\n\nAcknowledgements\nThis research was supported in part by the *****\nProfessorship and the ***************************.\n\n\n\n166 ********, *********\n\nReferences\n\n[1] ****** *****; ************; ************; *********\nImage-based clothes changing system. Computational\nVisual Media Vol. 3, No. 4, 337–347, ****.\n\n[2] *********; ********; *************; ***********; *****,\n*. HS-Nets: Estimating human body shape from\nsilhouettes with convolutional neural networks. In:\nProceedings of the 4th International Conference on\n** Vision, 108–117, ****.\n\n[3] ************.; ************ The naked truth: Estimating\nbody shape under clothing. In: Computer Vision –\n**** ****. Lecture Notes in Computer Science, Vol.\n5303. ***********; ********; *********, *. Eds. ********\nBerlin, 15–29, ****.\n\n[4] ***********; **********; **********; ********; Black,\n****.; ************. Unite the people: Closing the\nloop between ** and ** human representations. In:\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 4704–4713, ****.\n\n[5] *********; ***********; **********; *****Moll, **;\n***********. SMPL: A skinned multi-person linear\nmodel. *** Transactions on Graphics Vol. 34, No. 6,\nArticle No. 248, ****.\n\n[6] ***, *****; ***************; **********; **********\nConvolutional pose machines. In: Proceedings of the\n**** conference on Computer Vision and Pattern\nRecognition, 4724–4732, ****.\n\n[7] *******; *********; *******; ********** Realtime multi-\nperson ** pose estimation using part affinity fields.\nIn: Proceedings of thIEEE** Conference on Computer\nVision and Pattern Recognition, 1302–1310, ****.\n\n[8] *********; ***********; ***************; **********;\n***********; *************; ******; *********; ********,\n*. VNect: Realtime ** human pose estimation with\na single RGB camera. *** Transactions on Graphics\nVol. 36, No. 4, Article No. 44, ****.\n\n[9] ************; **********; ******; ************; Pons-\n******** Video based reconstruction of ** people\nmodels. In: Proceedings of thIEEE** Conference on\nComputer Vision and Pattern Recognition, 8387–8397,\n****.\n\n[10] ************; ************; *************; *********\nEnd-to-end recovery of human shape and pose. In:\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 7122–7131, ****.\n\n[11] *********; **********; ***********; ********; *****,\nE.; ********** Bodynet: Volumetric inference of **\nhuman body shapes. In: Proceedings of the European\nConference on Computer Vision, 20–36, ****.\n\n[12] *********; ******; *******; *******; *****************:\n** human reconstruction from a single image. In:\n\nProceedings of the **** International Conference on\nComputer Vision, 7739–7749, ****.\n\n[13] *********; *********; ***********; *************;\n************; ***********: Pixel-aligned implicit\nfunction for high-resolution clothed human digitization.\nIn: Proceedings of the **** International Conference\non Computer Vision, 2304–2314, ****.\n\n[14] ******; Zhu, S.-C.; ****, ***********: Joint ** pose\nand shape estimation by dense render-and-compare. In:\nProceedings of the **** International Conference on\nComputer Vision, 7760–7770, ****.\n\n[15] *********; *********; ******; *************; ******,\n************: Fast and accurate scans from an image\n*********************. In: Proceedings of the ****\nInternational Conference on Computer Vision, 5329–\n5338, ****.\n\n[16] ************; **********; ****************; ********,\nC.; ************* Learning to reconstruct people in\nclothing from a single RGB camera. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern\nRecognition, 1175–1186, ****.\n\n[17] **********, **; ************; ************; **********,\n*. Learning to reconstruct ** human pose and shape\nvia modelfitting in the loop. In: Proceedings of the\nIEEE International Conference on Computer Vision,\n2252–2261, ****.\n\n[18] *********; ********** Shape-aware human pose and\nshape reconstruction using multi-view images. In:\nProceedings of the **** International Conference on\nComputer Vision, 4352–4362, ****.\n\n[19] ********; **********; *********; ********; ******\nC.; ********; *********. Physics-inspired garment\nrecovery from a single-view image. *** Transactions\non Graphics Vol. 37, No. 5, Article No. 170, ****.\n\n[20] ********; *********; **********; Learning-based cloth\nmaterial recovery from video. In: Proceedings of the\nIEEE International Conference on Computer Vision,\n4383–4393, ****.\n\n[21] ***********; ************; **********; **********\nScalable differentiable physics for learning and control.\narXiv preprint arXiv:2007.02168, ****.\n\n[22] **************************; ************; *********;\n*************; ************. End-to-end differentiable\nphysics for learning and control. In: Proceedings of the\nAdvances in Neural Information Processing Systems,\n****.\n\n[23] ***********; ***********; **********; ***********\nA differentiable physics engine for deep learning in\nrobotics. Frontiers in Neurorobotics Vol. 13, 6, ****.\n\n[24] *********; *******; ********** Differentiable cloth\nsimulation for inverse problems. In: Proceedings of\nthe 33rd Conference on Neural Information Processing\nSystems, ****.\n\n\n\nMachine learning for digital try-on: Challenges and progress 167\n\n[25] ******; *******; *************; *************\nB.; **************; ******; *******; *******,\n*************: A real-time differentiable physical\nsimulator for soft robotics. In: Proceedings of the\nInternational Conference on Robotics and Automation,\n6265–6271, ****.\n\n[26] *********; ************; *********; *******; ********;\n****************; *********************: Differentiable\nprogramming for physical simulation. arXiv preprint\narXiv:1910.00935, ****.\n\n[27] **********; ***********; ************; **********; ***,\n*. F.; ******; ******** ** interactive garment pattern-\nmaking technology. Computer-Aided Design Vol. 104,\n113–124, ****.\n\n[28] *********; *******; ******** Automatic realistic\n** garment generation based on two images. In:\nProceedings of the International Conference on Virtual\nReality and Visualization, 250–257, ****.\n\n[29] ***********; **********; ************; ***********.\nLearning a shared shape space for multimodal garment\ndesign. *** Transactions on Graphics Vol. 37, No. 6",
      "merged_content": "\nComputational Visual Media\nhttps://doi.org/10.1007/s41095-020-0189-1 Vol. 7, No. 2, June 2021, 159–167\n\nReview Article\n\nMachine learning for digital try-on: Challenges and progress\n\nJunbang Liang1 (�), Ming C. Lin1\n\nc© The Author(s) 2020.\n\nAbstract Digital try-on systems for e-commerce have\nthe potential to change people’s lives and provide notable\neconomic benefits. However, their development is limited\nby practical constraints, such as accurate sizing of the\nbody and realism of demonstrations. We enumerate three\nopen challenges remaining for a complete and easy-to-use\ntry-on system that recent advances in machine learning\nmake increasingly tractable. For each, we describe\nthe problem, introduce state-of-the-art approaches, and\nprovide future directions.\n\nKeywords machine learning; digital try-on; garment\nmodeling; human body estimation; material\nmodeling\n\n1 Introduction\nE-commerce has grown at a rapid pace in recent\nyears. Consumers today are more likely to shop\nonline than to visit a retail store. The situation is\nmuch more complicated, however, when it comes to\nbuying clothes. People need to know how a garment\nfits on them, how it looks, and how it feels. Digital\ntry-on systems can potentially satisfy these needs,\nproviding a direct visual impression, and possibly\ncustomized clothes sizing as well. Therefore, it has\ndrawn much attention as an attractive alternative to\nimprove the user experience and popularize online\nfashion shopping.\n\nHowever, the technology is still far from practical,\neasy-to-use, and adequate to replace physical try-on.\nCurrently, most try-on systems rely on either image-\nediting, copy-pasting, or template demonstrations,\n\n1 University of Maryland, College Park, MD 20785, USA.\nE-mail: J. Liang, liangjb@cs.umd.edu (�); M. C. Lin,\nlin@cs.umd.edu.\n\nManuscript received: 2020-06-24; accepted: 2020-07-21\n\nwhile the ultimate goal is a fast and realistic try-on\nsystem adaptive to each customer’s body. There is\nstill a substantial technological gap between modeling\nand demonstrating garment fitting in the digital and\nreal worlds, including fast and realistic demonstration,\naccurate modeling of human body and garments,\nfaithful modeling of garment material, and lossless\ntransformation of garments between virtual and\nphysical worlds.\n\nIn this paper, we present some open research issues\nthat contribute to this technological gap, including:\n1. accurate estimation of human shapes and sizes\n\nusing consumer devices,\n2. faithful recovery of garment materials via (online)\n\nimages, and\n3. ease of design and manipulation of sewing patterns\n\nand garment pieces by end-users.\nAlthough traditional methods have made important\n\nprogress on these under-constrained problems,\nlearning-based approaches have shown tremendous\npotential to make a notable impact. Compared to\ntraditional methods, machine-learning algorithms are\nusually much faster since training and optimization\nare performed offline. They are also good at\ngeneralizing to unseen images without the need for\ntedious data pre-processing. While extensive research\nexists on 2D image learning, machine learning of\nhighly variable 3D human body shapes is still far\nfrom mature, which is the reason why the open issues\ndescribed above remain elusive.\n\nFor each problem listed above, we motivate its\nimportance, provide a problem description, and\npresent state-of-the-art approaches with potential\nfor improvements. We believe that solutions to\nthese challenging problems will lead to significant\nadvances in digital try-on, as well as other areas of\ne-commerce.\n\n159\n\n  \n\n TSINGHUA Springer UNIVERSITY PRESS \n\n\n\n160 J. Liang, M. C. Lin\n\n2 Open problems\nIn this section we first introduce three major\nchallenges that limit digital try-on technology from\nbeing widely adopted and accepted by shoppers.\nThere are several reasons why shoppers still prefer\nphysical try-on. Firstly, consumers are unsure if what\nthey buy online will fit them well. Although general\nsizing systems exist, their lack of consistency and\nstandardization across different brands and garment\nmaterials can often make it difficult to size clothes,\nespecially for persons with non-standard body shapes\nand proportion. Accurate estimation of human body\nshape is the key to successful digital try-on. Secondly,\nfabric is usually a key consideration when shopping\nfor clothes. Different fabric affects how garments\nlook and fit, how consumers would wear them, and\nwhether or not they would buy them. However, the\ncorrespondence between the actual material and its\ndigital representation are not well understood. It is\nalso challenging to acquire a full fabric digital model\nfrom real-world examples.\n\nFor the customers, appearance is as critical as other\nfactors. There are two approaches to displaying\ngarments: 2D image-based, and 3D mesh-based\nwith photo-realistic rendering. They have different\nadvantages and drawbacks, but both need a large\ngarment database for support. While creating a 3D\ngarment takes considerable effort, 2D images often\nsuffer from a lack of variation and are much more\ndifficult to customize. In either case, the try-on\nsystem needs a user-friendly design and manipulation\nbackend. Last, but not least, a fast and realistic\nanimation of the garments in motion along with\nbody movements greatly improves the user experience.\nAlthough it is not so critical as other factors, it\nwould effectively reduce the perceptual gap between\nthe real and digital worlds for online shopping.\nPrevious work has proposed using cloud computing\nto improve the animation speed, but there is still a\nnotable technology gap for high-quality, interactive\n3D animation of clothes.\n\n3 Human shape estimation\nAs noted, accurate human shape estimation is key to\nenabling digital try-on. Human body reconstruction,\nconsisting of pose and shape estimation, has been\nwidely studied in a variety of areas, including digital\n\nsurveillance, computer animation, special effects, and\nvirtual and augmented environments. Yet, it remains\na challenging and popular topic of interest. While\ndirect 3D body scanning can provide excellent and\naccurate results, its adoption is somewhat limited by\nthe required specialized hardware. RGB images are\nwidely available for input to digital try-on and can\nbe easily captured using commodity mobile devices.\nAlthough purely image-based try-on methods have\nbeen proposed [1], learning-based 3D body estimation\nis more widely applicable in that the 3D body can be\narticulated and so re-posed and re-targeted.\n\nWe define the human-body reconstruction problem\ninformally as, given one or more RGB images, to\nestimate the human body geometry and size, and\noutput (preferably) a 3D humanoid mesh. Traditional\nalgorithms often formulate it as an optimization\nproblem, in which the silhouette difference is a major\npart of the objective function [2]. Therefore, these\nmethods either require the human to wear tight\nclothes, or alternatively relax the target function\nto be unilateral on uncovered body parts [3], or\nto point correspondences [4]. The use of machine\nlearning methods in this problem has led to significant\nadvances. Firstly, it has moved the algorithm from\nonline to offline, significantly reducing response time.\nSecond, by using a parametric human model [5],\none can easily construct a regression network for\nthe parameters while the losses needed can also be\ninferred from them. While early works proposed\nnetwork models for only 2D/3D body skeletons [6–\n8], more recent works have introduced techniques to\nperform regression for the entire human body—either\nusing a parametric human model [9, 10] or a voxel-\nbased representation [11–13]. As annotations in most\nreal-world datasets contain only joint positions, the\nlearning process has been refined in various ways [14–\n17]. The current state of the art is the recent work\nby Ref. [18] �. It emphasizes shape learning, while\nmany other works often focus on body-joint losses,\nbut neglect the effect of body shapes.\n\nThe key contribution of Ref. [18] is a multi-view,\nmulti-stage framework to address ambiguity caused by\ncamera projection (see Fig. 1). Their model performs\nseveral stages of error correction. Each of the image\ninputs is passed on step by step; at each step, a shared-\n\n� Liang and Lin’s data and code are available at https://gamma.umd.edu/\nresearchdirections/virtualtryon/humanmultiview\n\n TSINGHUA Springer UNIVERSITY PRESS \n\n\n\nMachine learning for digital try-on: Challenges and progress 161\n\nFig. 1 Network structure from Ref. [18]. By using an iterative value correction structure, visual information from different views is effectively\nintegrated to provide a unified human shape. Reproduced with permission from Ref. [18], c© The Author(s) 2019.\n\nparameter prediction block computes the correction\nbased on the image feature and the input guesses.\nThe camera and the human body parameters are\nestimated at the same time, projecting the predicted\n3D joints back to 2D for loss computation. The\nestimated pose and shape parameters are shared\namong all views, while each view maintains its own\ncamera calibration and global orientation. Their\nproposed framework uses a recurrent structure,\nmaking it a universal model applicable to any number\nof views. At the same time, it couples shareable\ninformation across different views so that the human\nbody pose and shape are optimized using image\nfeatures from all views. Unlike static multi-view\nCNNs which have a fixed number of inputs, they\nmake use of the RNN-like structure in a cyclic form to\naccept any number of views, and prevent the gradient\nvanishing by predicting corrective values instead of\nupdating parameters in each regression block.\n\nExperiments have shown that, after training, this\nmodel can form a single view image, provide equally\ngood pose estimation as the state of the art, and\nprovide considerably improved pose estimation when\nusing multi-view inputs, leading to better shape\nestimation across all datasets. An example is\ndemonstrated in Fig. 2. Moreover, a physically-based\nsynthetic data generation pipeline is introduced to\nenrich the training data, which is very helpful for\n\nshape estimation and regularization in cases that\ntraditional datasets do not capture. While synthetic\ndata improves the diversity of human bodies with\nground-truth parameters, a larger garment dataset\nand a more convenient registration process are needed\nto minimize the performance gap between real-world\nimages and synthetic data. In addition, other\nvariables such as hair, skin color, and 3D backgrounds\nare subtle elements that can influence the perceived\nrealism of the synthetic data at the higher expense of\na more complex data generation pipeline. With the\nrecent progress in image style transfer using GAN, a\n\nFig. 2 Prediction results using the state of the art [18]. The model\ncaptures the shape of the human body by learning from synthetic\ndata. The recovered legs and chest are close to those of the person\nin the image. Reproduced with permission from Ref. [18], c© The\nAuthor(s) 2019.\n\n A 1,1 102,1 3,1 b b b fi f1 f1 f1 I View 1 01,1 Regression Regression Regression Block Q2,1 Block 3,1 Block (1,1) c (2,1) C (3,1) - - 103.2 - b @ 2,2 b b - f2 f2 f2 f2 Image Encoder Regression Regression Regression - View 2 C1,2 Block 2,2 Block 3.2 Block No C (1,2) C (2,2) C (3,2) - - ... 1 1 b b b ... ... fn fn fn - Regression Regression Regression View n 1,77 Block A2,n Block 3,n Block (1,n) (2,n) C (3,n) C - - fn @3,1 1 b b b Stage 1 Stage 2 Stage 3 Multi-View Multi-Stage Regression Network \n\n dreamstime dreamstie (a) Input image (b) Recovered body \n\n\n\n162 J. Liang, M. C. Lin\n\npromising direction is to transfer the synthetic result\nto more realistic images to further improve the result.\n\n4 Garment material modeling\n4.1 Introduction\nGarment material plays an important role in digital\ntry-on systems. Physical recreation of the fabric not\nonly gives a compelling visual simulation of the cloth,\nbut also affects how the garment feels and fits on\nthe body. However, fabric modeling is a challenging\ntask: the appearance and physical properties of\nthe garment are determined not only by the type\nof materials the clothes are made of, but also by\nsewing and weave. Thus, researchers often focus on\nthe physical behaviour, rather than the underlying\nsemantic primitives.\n\nHence, we state the garment material modeling\nproblem as follows. Given a sufficient amount of data,\nmodel the material’s physical behavior and physical\nproperties, so that visual effects the same as or similar\nto those of the real material can be reproduced by a\ncomputer. This has two implications: firstly, we need\nto define a physical model of the material, and then\nwe must estimate the parameters in the model.\n\nThere are many ways to model clothes, including\nspring–mass systems and finite elements. The latter is\nthe most popular model since it can produce realistic\nresults. While one can use isotropic properties such\nas Young’s modulus and Poisson ratio, an anisotropic\nmodel is a better choice since it can support different\nbehaviors caused by the weave of the material.\n\n4.2 Learning-based estimation\nWhile traditional optimization methods [19] often\ntake a long time to compute material parameters,\nmachine-learning methods can make predictions in\nreal time by a simple feed-forward operation, which\nis more useful in applications that need fast feedback,\nsuch as garment prototyping. The state-of-the-art\nmodel from Yang et al. [20] � uses CNNs combined\nwith LSTM to recover material parameters from\nvideos. To constrain both the input and solution\nspace, they choose one of the materials as a basis;\nthe material sub-space is constructed by multiplying\nthis material basis with a positive coefficient. To\nconstruct an optimal material parameter sub-space, a\n\n� Yang et al.’s data and code are available at http://gamma.cs.unc.edu/\nVideoCloth\n\nmaterial parameter sensitivity analysis is conducted\nto examine the sensitivity of the material parameters\nκ with respect to the amount of deformation D(κ).\nPhysically based cloth simulations are used to\ngenerate a much larger number of data samples within\nthese sub-spaces, which would otherwise be difficult\nor time-consuming to capture. The cloth meshes are\ngenerated through physically-based simulation, and\nthen rendered as 2D images with a randomly assigned\ntexture. Using the data samples, they combine the\nimage signal feature extraction method, a CNN, with\nthe temporal sequence learning method, LSTM, to\nlearn the mapping from visual appearance to material.\nAs shown in Fig. 3, the CNN layer is used to extract\nboth low- and high-level visual features, while the\nLSTM layer focuses on learning the mapping between\nthe material properties of the cloth and its consequent\nmovement.\n\nThey demonstrated the proposed framework with\nthe application of “material cloning”. With the\ntrained deep neural network model being able to\ncapture the cloth motion (Fig. 4), the material type\ncan be inferred from a video recording of the motion\nof the cloth in a fairly small amount of time. The\nrecovered material type can be “cloned” onto another\npiece of cloth or garment as shown in Fig. 5.\n\nIn this work, the videos contain only a single piece\nof cloth which does not interact with any other object.\nWhile this is not applicable to all real-world scenarios,\nthis method provides new insights into addressing this\nchallenging problem. A natural extension would be to\nlearn from videos of clothing directly interacting with\nthe human body, under varying lighting conditions\nand partial occlusion.\n\n4.3 Optimization using differentiable physics\nAnother approach to modeling the fabric is to measure\ngeometric differences directly during parameter\n\nFig. 3 Network model from Ref. [20]. The material is modeled\nby learning motion patterns of image features given by CNNs.\nReproduced with permission from Ref. [20], c© The Author(s) 2017.\n\n = = 1 h- 2.2 hn-1 (xl r.2 \". . . , x) 1 Classifier 1 Tu Tv2 Tv\" CNN CNN CNN Material type LSTM cell = Concatenate op T1 T2 rgb Tn rgb rgb \n\n\n\nMachine learning for digital try-on: Challenges and progress 163\n\nFig. 4 Learned CNN conv5-layer activation visualization from\nRef. [20]. Experiments show that the trained model is able to capture\nmoving parts of the cloth even in an unseen video. Reproduced with\npermission from Ref. [20], c© The Author(s) 2017.\n\nFig. 5 Yang et al. [20] modeled clothes materials in input videos (left),\nand applied those materials to a simulated skirt (right). Reproduced\nwith permission from Ref. [20], c© The Author(s) 2017.\n\noptimization. Assuming that the environment is\nknown to the system, computation of the estimated\nmotion and its gradient with respect to the material\nparameters can be achieved using differentiable\nsimulation. A typical usage of differentiable\nsimulation is motion control (see Fig. 6), where the\ndifference to the target is measured and the loss\nbackpropagated to the network. Similar processes\ncan be applied to material parameter estimation as\nwell. By measuring the distance to the target as the\nloss and computing corresponding gradients, either in\npixel space or in 3D space, the material parameters\n\ncan be learned or optimized to achieve the desired\ncloth motion or visual effect. Recent differentiable\nphysics work covers rigid bodies [22, 23], cloth [24],\nand particle-grid systems [25, 26]. The state-of-\nthe-art is Ref. [24] �, which proposes a method for\ndifferentiable cloth simulation. It is the first work\nto tackle a high dimensional simulation problem\nand to propose a general differentiable collision\nhandling algorithm. Later, a follow-up work [21]\nextended the algorithm to be applicable to coupled\ndynamics with rigid bodies. Overall, they follow\nthe computational flow of the common approach\nto cloth simulation: discretization using the finite\nelement method, integration using an implicit Euler\nmethod, and collision response on impact zones. They\nuse implicit differentiation in the linear solver and\noptimization in order to compute the gradient with\nrespect to the input parameters. The discontinuity\nintroduced by collision response is negligible because\nthe discontinuous states constitute a zero-measure\nset. During backpropagation in the optimization,\ngradient values can be directly computed after QR\ndecomposition of the constraint matrix. Their\npipeline contains several techniques that can be\nemployed in other differentiable simulations.\n4.3.1 Derivatives of the physical solution\nIn modern simulation algorithms, an implicit Euler\nmethod is often used for stable integration results.\nThus the mass matrix M often includes the Jacobian\nof the forces, and is denoted as M̂ to indicate this\ndifference. A linear solver is needed to compute the\nacceleration since it is time-consuming to compute\nM̂−1. Implicit differentiation is used to compute the\ngradients of the linear solution. Given an equation\nM̂a = f with a solution z and propagated gradient\n∂L/∂a|a=z, where L is the task-specific loss function,\nimplicit differentiation is used to derive the gradients.\nWe refer readers to the original paper [24] for more\ndetails.\n4.3.2 Derivatives of the collision response\nA general approach using LCP to integrate collision\nconstraints into physics simulations has been\nproposed, but constructing a static LCP is often\nimpractical in cloth simulation due to the high\ndimensionality. Collisions and contacts which happen\nat each step are very sparse compared to the complete\n\n� Liang et al.’s data and code are available at https://gamma.umd.edu/\nresearchdirections/virtualtryon/differentiablecloth\n\n F-1 F-25 F-3 F-105 F-1 F-25 F-38 F-105 1 F-1 F-3 0.9 F-4 0.8 0.7 0.6 0.5 0.4 0.3 0.2 F-1 F-2 F-4 0.1 F-3 0 \n\n  \n\n\n\n164 J. Liang, M. C. Lin\n\nFig. 6 Differentiable simulation embedding example from Ref. [21]. The loss can be backpropagated through the physics simulator to the\nneural network, enabling learning tasks such as material modeling and motion control.\n\ndata. Therefore, a dynamic approach is used that\nincorporates collision detection and response.\n\nCollision handling in their implementation is based\non impact zone optimization. It finds all colliding\ninstances using continuous collision detection and\nsets up the constraints for all collisions. In order\nto introduce minimum change to the original mesh\nstate, a QP problem is developed to determine the\nconstraints. Since the signed distance function is\nlinear in x, the optimization takes a quadratic form,\nas shown originally in Ref. [24]:\n\nminimize\nz\n\n1\n2\n\n(z − x)TW (z − x),\n\nsubject to Gz + h � 0\nwhere W is a constant diagonal weight matrix related\nto the mass of each vertex, and G and h are constraint\nparameters. The numbers of variables and constraints\nare n and m, i.e. x ∈ R\n\nn, h ∈ R\nm, and G ∈ R\n\nm×n.\nNote that this optimization problem has inputs x,\nG, and h, and output z. The goal here is to derive\n∂L/∂x, ∂L/∂G, and ∂L/∂h given ∂L/∂z, where L\nis the loss function.\n\nWhen computing the gradient using implicit\ndifferentiation, the dimensionality of the linear system\ncan be very high. Their key observation here is that\nn >> m > rank(G), since one contact often involves 4\nvertices (thus 12 variables) and some contacts may be\nlinearly dependent (e.g., multiple adjacent collision\n\npairs). They minimize the size of the linear equation\nbased on the QR decomposition of G, which is the key\nto accelerating backpropagation of high dimensional\nQP problems.\n\nOne of their experiments shows its ability to\noptimize material parameters from observation. The\nscene features a piece of cloth hanging under gravity\nand subjected to a constant wind force. The material\nmodel consists of three parts: density d, stretching\nstiffness S, and bending stiffness B. The stretching\nstiffness quantifies the reaction force when the cloth\nis stretched; the bending stiffness models how easily\nthe cloth can be bent and folded. Table 1 shows\nresults. They achieve a much smaller error in most\nmeasurements in comparison to the baselines; the\nlinear part of the stiffness matrix is modeled well.\nWith the computed gradient using their model, one\ncan effectively optimize the unknown parameters that\ndominate cloth movement to fit the observed data.\n\nIn follow-up work, Qiao et al. extended the\ndifferentiable simulation pipeline to couple with\nrigid body dynamics, formulated using generalized\ncoordinates:\n\nd\ndt\n\n⎛\n⎝ q\n\nq̇\n\n⎞\n⎠ =\n\n⎛\n⎝ q̇\n\nq̈\n\n⎞\n⎠ =\n\n⎛\n⎝ q̇\n\nM−1f(q, q̇)\n\n⎞\n⎠\n\nand update the optimization formulation for collision\nresponse accordingly (see Ref. [21] for details):\n\nTable 1 Material parameter estimation results from Ref. [24]. Their proposed method runs faster than L-BFGS. Values of material parameters\nare Frobenius norms of the difference normalized by the Frobenius norm of the target. Values of the simulated result are the average pairwise\nvertex distances normalized by the size of the cloth. The gradient-based method yields much smaller errors than the baselines\n\nMethod\nRuntime\n\n(sec/step/iter)\n\nDensity\n\nerror (%)\n\nLinear stretching\n\nstiffness error (%)\n\nBending stiffness\n\nerror (%)\n\nSimulation\n\nerror (%)\n\nBaseline — 68 ± 46 160 ± 119 70 ± 42 12 ± 3.0\n\nL-BFGS 2.89 ± 0.02 4.2 ± 5.6 72 ± 90 70 ± 43 4.9 ± 3.3\n\nLiang et al. [24] 2.03 ± 0.06 1.8 ± 2.0 45 ± 41 77 ± 36 1.6 ± 1.4\n\n New Observation 00000000 Control Simulation Observations Signals Time Collision Results Integration Response Loss State: velocity, position, material, force ... Trainable Network Layers Differentiable Simulation Layer \n\n\n\nMachine learning for digital try-on: Challenges and progress 165\n\nminimize\nq′\n\n1\n2\n\n(q − q′)TM̂(q − q′)\n\nsubject to Gf(q′) + h � 0\n\nDue to the inclusion of rigid bodies, the constraints\nused in the optimization are no longer linear. When\ncomputing gradients, they linearize the constraints\naround a neighborhood as an approximation to enable\nQR decomposition for acceleration as previously\nmentioned.\n\n5 Garment modeling and design\nRealistic apparel model generation has become\nincreasingly popular, due to the rapid changes in\nfashion trends and the growing need for garment\nmodels in different applications such as virtual try-\non. It is already used even for state-of-the-art\ninteractive apparel design systems [27]. Application\nrequirements mean that it is important to have a\ngeneral cloth model that can represent a diverse set\nof garments. However, there are many challenges\nin automatic garment model generation. Firstly,\ngarments usually have different types of topology,\nespecially for fashion apparel, that makes it difficult\nto design a universal pipeline. Moreover, it is often\nnot straightforward for general garments design to\nbe retargeted onto another body shape, making\ncustomization difficult.\n\nPrevious work has addressed this problem to some\nextent. Huang et al. [28] proposed a realistic 3D\ngarment generation algorithm based on front and\nback image sketches, but it cannot readily retarget\ngenerated garments to other body shapes. Wang et\nal. [29] proposed an algorithm which can conveniently\nperform retargeting, but permits limited topology\nlike T-shirts or skirts. There is no recent work that\naddresses these two problems at the same time.\n\nWe introduce a learning-based parametric\ngenerative model to overcome the above difficulties,\ngiven garment sewing patterns and human body\nshapes as input. One possible approach would be to\ncompute a displacement image on the U–V space\nof the human body as a unified representation of\nthe garment mesh. Different topology and sizes\nof the garment are represented by different values\nin the image. The 2D displacement image, as the\nrepresentation of the 3D garment mesh data, can\n\nthen be fed into a conditional generative adversarial\nnetwork (cGAN) for latent space learning. The 2D\nrepresentation for the garment mesh can transfer\nthe irregular 3D mesh data to regular image data\nwhere a traditional CNN can easily learn. It can also\nextract relative geometric information with respect\nto the human body, enabling garment retargeting to\na different person.\n\n6 Conclusions\nAlthough virtual reality and digital try-on have\nexcellent potential and are rapidly developing, there\nremain open problems before online try-on systems\ncan be widely adopted. We have listed three major\nchallenges, all of which can be addressed or further\nimproved using machine learning algorithms. For\ngarment material prediction, state-of-the-art methods\nare still limited in that the training data is highly\nconstrained: the scenario contains only a piece\nof cloth floating in the wind. To improve its\napplicability to daily tasks, it is necessary to focus\non solving the problem on a more diverse set of\ninputs. Predicting the material from a garment\non a fixed human body could be a good start,\nbefore generalizing to arbitrary human motions and\npredicting multiple garments on the same body. In\nthe area of human shape estimation, it would be\ninteresting to learn how external constraints could\nimprove estimation accuracy. For example, the shape\nand size of the garment are hard constraints to\nwhich the predicted body should conform. While\noptimization-based methods can integrate these\nconstraints fairly easily, doing so remains elusive\nfor learning-based approaches. One possibility is\nto jointly estimate body and garment together and\nintroduce an intersection loss. This approach would\nrequire a new solution to the open problem of unified\ndeep garment representation, if we do not want to\ntrain one model for every garment type, which could\nbe even more challenging. We believe that substantial\nbreakthroughs in digital try-on are achievable with\nmore investigation in these directions.\n\nAcknowledgements\nThis research was supported in part by the Iribe\nProfessorship and the National Science Foundation.\n\n\n\n166 J. Liang, M. C. Lin\n\nReferences\n\n[1] Zheng, Z. H.; Zhang, H. T.; Zhang, F. L.; Mu, T. J.\nImage-based clothes changing system. Computational\nVisual Media Vol. 3, No. 4, 337–347, 2017.\n\n[2] Dibra, E.; Jain, H.; Öztireli, C.; Ziegler, R.; Gross,\nM. HS-Nets: Estimating human body shape from\nsilhouettes with convolutional neural networks. In:\nProceedings of the 4th International Conference on\n3D Vision, 108–117, 2016.\n\n[3] Bălan, A. O.; Black, M. J. The naked truth: Estimating\nbody shape under clothing. In: Computer Vision –\nECCV 2008. Lecture Notes in Computer Science, Vol.\n5303. Forsyth, D.; Torr, P.; Zisserman, A. Eds. Springer\nBerlin, 15–29, 2008.\n\n[4] Lassner, C.; Romero, J.; Kiefel, M.; Bogo, F.; Black,\nM. J.; Gehler, P. V. Unite the people: Closing the\nloop between 3D and 2D human representations. In:\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 4704–4713, 2017.\n\n[5] Loper, M.; Mahmood, N.; Romero, J.; Pons-Moll, G.;\nBlack, M. J. SMPL: A skinned multi-person linear\nmodel. ACM Transactions on Graphics Vol. 34, No. 6,\nArticle No. 248, 2015.\n\n[6] Wei, S.-E.; Ramakrishna, V.; Kanade, T.; Sheikh, Y.\nConvolutional pose machines. In: Proceedings of the\nIEEE conference on Computer Vision and Pattern\nRecognition, 4724–4732, 2016.\n\n[7] Cao, Z.; Simon, T.; Wei, S.; Sheikh, Y. Realtime multi-\nperson 2D pose estimation using part affinity fields.\nIn: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 1302–1310, 2017.\n\n[8] Mehta, D.; Sridhar, S.; Sotnychenko, O.; Rhodin, H.;\nShafiei, M.; Seidel, H.-P.; Xu, W.; Casas, D.; Theobalt,\nC. VNect: Realtime 3D human pose estimation with\na single RGB camera. ACM Transactions on Graphics\nVol. 36, No. 4, Article No. 44, 2017.\n\n[9] Alldieck, T.; Magnor, M.; Xu, W.; Theobalt, C.; Pons-\nMoll, G. Video based reconstruction of 3D people\nmodels. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 8387–8397,\n2018.\n\n[10] Kanazawa, A.; Black, M. J.; Jacobs, D. W.; Malik, J.\nEnd-to-end recovery of human shape and pose. In:\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 7122–7131, 2018.\n\n[11] Varol, G.; Ceylan, D.; Russell, B.; Yang, J.; Yumer,\nE.; Laptev, I. Bodynet: Volumetric inference of 3D\nhuman body shapes. In: Proceedings of the European\nConference on Computer Vision, 20–36, 2018.\n\n[12] Zheng, Z.; Yu, T.; Wei, Y.; Dai, Q.; Liu, Y. Deephuman:\n3D human reconstruction from a single image. In:\n\nProceedings of the IEEE International Conference on\nComputer Vision, 7739–7749, 2019.\n\n[13] Saito, S.; Huang, Z.; Natsume, R.; Morishima, S.;\nKanazawa, A.; Li, H. PIFu: Pixel-aligned implicit\nfunction for high-resolution clothed human digitization.\nIn: Proceedings of the IEEE International Conference\non Computer Vision, 2304–2314, 2019.\n\n[14] Xu, Y.; Zhu, S.-C.; Tung, T. Denserac: Joint 3D pose\nand shape estimation by dense render-and-compare. In:\nProceedings of the IEEE International Conference on\nComputer Vision, 7760–7770, 2019.\n\n[15] Smith, D.; Loper, M.; Hu, X.; Mavroidis, P.; Romero,\nJ. FACSIMILE: Fast and accurate scans from an image\nin less than a second. In: Proceedings of the IEEE\nInternational Conference on Computer Vision, 5329–\n5338, 2019.\n\n[16] Alldieck, T.; Magnor, M.; Bhatnagar, B. L.; Theobalt,\nC.; Pons-Moll, G. Learning to reconstruct people in\nclothing from a single RGB camera. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern\nRecognition, 1175–1186, 2019.\n\n[17] Kolotouros, N.; Pavlakos, G.; Black, M. J.; Daniilidis,\nK. Learning to reconstruct 3D human pose and shape\nvia modelfitting in the loop. In: Proceedings of the\nIEEE International Conference on Computer Vision,\n2252–2261, 2019.\n\n[18] Liang, J.; Lin, M. C. Shape-aware human pose and\nshape reconstruction using multi-view images. In:\nProceedings of the IEEE International Conference on\nComputer Vision, 4352–4362, 2019.\n\n[19] Yang, S.; Pan, Z. R.; Amert, T.; Wang, K.; Yu, L.\nC.; Berg, T.; Lin, M. C. Physics-inspired garment\nrecovery from a single-view image. ACM Transactions\non Graphics Vol. 37, No. 5, Article No. 170, 2018.\n\n[20] Yang, S.; Liang, J.; Lin, M. C.; Learning-based cloth\nmaterial recovery from video. In: Proceedings of the\nIEEE International Conference on Computer Vision,\n4383–4393, 2017.\n\n[21] Qiao, Y. L.; Liang, J. B.; Koltun, V.; Lin, M. C.\nScalable differentiable physics for learning and control.\narXiv preprint arXiv:2007.02168, 2020.\n\n[22] De Avila Belbute-Peres, F.; Smith, K. A.; Allen, K.;\nTenenbaum, J.; Kolter, J. Z. End-to-end differentiable\nphysics for learning and control. In: Proceedings of the\nAdvances in Neural Information Processing Systems,\n2018.\n\n[23] Degrave, J.; Hermans, M.; Dambre, J.; Wyffels, F.\nA differentiable physics engine for deep learning in\nrobotics. Frontiers in Neurorobotics Vol. 13, 6, 2019.\n\n[24] Liang, J.; Lin, M.; Koltun, V. Differentiable cloth\nsimulation for inverse problems. In: Proceedings of\nthe 33rd Conference on Neural Information Processing\nSystems, 2019.\n\n\n\nMachine learning for digital try-on: Challenges and progress 167\n\n[25] Hu, Y.; Liu, J.; Spielberg, A.; Tenenbaum, J.\nB.; Freeman, W. T.; Wu, J.; Rus, D.; Matusik,\nW. ChainQueen: A real-time differentiable physical\nsimulator for soft robotics. In: Proceedings of the\nInternational Conference on Robotics and Automation,\n6265–6271, 2019.\n\n[26] Hu, Y. M.; Anderson, L.; Li, T. M.; Sun, Q.; Carr, N.;\nRagan-Kelley, J.; Durand, F. DiffTaichi: Differentiable\nprogramming for physical simulation. arXiv preprint\narXiv:1910.00935, 2019.\n\n[27] Liu, K. X.; Zeng, X. Y.; Bruniaux, P.; Tao, X. Y.; Yao,\nX. F.; Li, V.; Wang, J. 3D interactive garment pattern-\nmaking technology. Computer-Aided Design Vol. 104,\n113–124, 2018.\n\n[28] Huang, P.; Yao, J.; Zhao, H. Automatic realistic\n3D garment generation based on two images. In:\nProceedings of the International Conference on Virtual\nReality and Visualization, 250–257, 2016.\n\n[29] Wang, T. Y.; Ceylan, D.; Popović, J.; Mitra, N. J.\nLearning a shared shape space for multimodal garment\ndesign. ACM Transactions on Graphics Vol. 37, No. 6",
      "text": [
        "",
        "TSINGHUA Springer UNIVERSITY PRESS",
        "TSINGHUA Springer UNIVERSITY PRESS",
        "A 1,1 102,1 3,1 b b b fi f1 f1 f1 I View 1 01,1 Regression Regression Regression Block Q2,1 Block 3,1 Block (1,1) c (2,1) C (3,1) - - 103.2 - b @ 2,2 b b - f2 f2 f2 f2 Image Encoder Regression Regression Regression - View 2 C1,2 Block 2,2 Block 3.2 Block No C (1,2) C (2,2) C (3,2) - - ... 1 1 b b b ... ... fn fn fn - Regression Regression Regression View n 1,77 Block A2,n Block 3,n Block (1,n) (2,n) C (3,n) C - - fn @3,1 1 b b b Stage 1 Stage 2 Stage 3 Multi-View Multi-Stage Regression Network",
        "dreamstime dreamstie (a) Input image (b) Recovered body",
        "= = 1 h- 2.2 hn-1 (xl r.2 \". . . , x) 1 Classifier 1 Tu Tv2 Tv\" CNN CNN CNN Material type LSTM cell = Concatenate op T1 T2 rgb Tn rgb rgb",
        "F-1 F-25 F-3 F-105 F-1 F-25 F-38 F-105 1 F-1 F-3 0.9 F-4 0.8 0.7 0.6 0.5 0.4 0.3 0.2 F-1 F-2 F-4 0.1 F-3 0",
        "",
        "New Observation 00000000 Control Simulation Observations Signals Time Collision Results Integration Response Loss State: velocity, position, material, force ... Trainable Network Layers Differentiable Simulation Layer",
        "",
        ""
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"TSINGHUA Springer UNIVERSITY PRESS\",\"lines\":[{\"boundingBox\":[{\"x\":1283,\"y\":9},{\"x\":1522,\"y\":9},{\"x\":1522,\"y\":39},{\"x\":1283,\"y\":39}],\"text\":\"TSINGHUA\"},{\"boundingBox\":[{\"x\":1605,\"y\":14},{\"x\":1761,\"y\":15},{\"x\":1761,\"y\":60},{\"x\":1605,\"y\":59}],\"text\":\"Springer\"},{\"boundingBox\":[{\"x\":1287,\"y\":40},{\"x\":1523,\"y\":39},{\"x\":1523,\"y\":60},{\"x\":1287,\"y\":61}],\"text\":\"UNIVERSITY PRESS\"}],\"words\":[{\"boundingBox\":[{\"x\":1286,\"y\":9},{\"x\":1509,\"y\":9},{\"x\":1510,\"y\":40},{\"x\":1285,\"y\":40}],\"text\":\"TSINGHUA\"},{\"boundingBox\":[{\"x\":1606,\"y\":15},{\"x\":1760,\"y\":17},{\"x\":1758,\"y\":60},{\"x\":1606,\"y\":60}],\"text\":\"Springer\"},{\"boundingBox\":[{\"x\":1288,\"y\":41},{\"x\":1428,\"y\":40},{\"x\":1428,\"y\":61},{\"x\":1288,\"y\":60}],\"text\":\"UNIVERSITY\"},{\"boundingBox\":[{\"x\":1439,\"y\":40},{\"x\":1516,\"y\":40},{\"x\":1516,\"y\":61},{\"x\":1439,\"y\":61}],\"text\":\"PRESS\"}]}",
        "{\"language\":\"en\",\"text\":\"TSINGHUA Springer UNIVERSITY PRESS\",\"lines\":[{\"boundingBox\":[{\"x\":74,\"y\":11},{\"x\":306,\"y\":12},{\"x\":306,\"y\":36},{\"x\":74,\"y\":35}],\"text\":\"TSINGHUA\"},{\"boundingBox\":[{\"x\":390,\"y\":14},{\"x\":560,\"y\":16},{\"x\":559,\"y\":59},{\"x\":390,\"y\":57}],\"text\":\"Springer\"},{\"boundingBox\":[{\"x\":77,\"y\":41},{\"x\":311,\"y\":41},{\"x\":311,\"y\":60},{\"x\":77,\"y\":60}],\"text\":\"UNIVERSITY PRESS\"}],\"words\":[{\"boundingBox\":[{\"x\":77,\"y\":12},{\"x\":296,\"y\":13},{\"x\":296,\"y\":36},{\"x\":78,\"y\":36}],\"text\":\"TSINGHUA\"},{\"boundingBox\":[{\"x\":390,\"y\":14},{\"x\":560,\"y\":18},{\"x\":559,\"y\":59},{\"x\":390,\"y\":58}],\"text\":\"Springer\"},{\"boundingBox\":[{\"x\":77,\"y\":41},{\"x\":217,\"y\":41},{\"x\":218,\"y\":61},{\"x\":77,\"y\":61}],\"text\":\"UNIVERSITY\"},{\"boundingBox\":[{\"x\":227,\"y\":41},{\"x\":303,\"y\":41},{\"x\":304,\"y\":61},{\"x\":227,\"y\":61}],\"text\":\"PRESS\"}]}",
        "{\"language\":\"en\",\"text\":\"A 1,1 102,1 3,1 b b b fi f1 f1 f1 I View 1 01,1 Regression Regression Regression Block Q2,1 Block 3,1 Block (1,1) c (2,1) C (3,1) - - 103.2 - b @ 2,2 b b - f2 f2 f2 f2 Image Encoder Regression Regression Regression - View 2 C1,2 Block 2,2 Block 3.2 Block No C (1,2) C (2,2) C (3,2) - - ... 1 1 b b b ... ... fn fn fn - Regression Regression Regression View n 1,77 Block A2,n Block 3,n Block (1,n) (2,n) C (3,n) C - - fn @3,1 1 b b b Stage 1 Stage 2 Stage 3 Multi-View Multi-Stage Regression Network\",\"lines\":[{\"boundingBox\":[{\"x\":923,\"y\":25},{\"x\":937,\"y\":26},{\"x\":938,\"y\":41},{\"x\":923,\"y\":42}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":943,\"y\":18},{\"x\":969,\"y\":17},{\"x\":970,\"y\":36},{\"x\":944,\"y\":36}],\"text\":\"1,1\"},{\"boundingBox\":[{\"x\":1236,\"y\":14},{\"x\":1297,\"y\":14},{\"x\":1297,\"y\":41},{\"x\":1235,\"y\":54}],\"text\":\"102,1\"},{\"boundingBox\":[{\"x\":1600,\"y\":17},{\"x\":1626,\"y\":17},{\"x\":1627,\"y\":36},{\"x\":1601,\"y\":36}],\"text\":\"3,1\"},{\"boundingBox\":[{\"x\":942,\"y\":44},{\"x\":954,\"y\":43},{\"x\":955,\"y\":59},{\"x\":942,\"y\":59}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1267,\"y\":41},{\"x\":1284,\"y\":41},{\"x\":1285,\"y\":60},{\"x\":1268,\"y\":60}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1600,\"y\":44},{\"x\":1611,\"y\":44},{\"x\":1611,\"y\":59},{\"x\":1600,\"y\":58}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":555,\"y\":68},{\"x\":579,\"y\":69},{\"x\":578,\"y\":100},{\"x\":553,\"y\":98}],\"text\":\"fi\"},{\"boundingBox\":[{\"x\":777,\"y\":61},{\"x\":801,\"y\":64},{\"x\":798,\"y\":91},{\"x\":774,\"y\":88}],\"text\":\"f1\"},{\"boundingBox\":[{\"x\":1099,\"y\":59},{\"x\":1124,\"y\":62},{\"x\":1122,\"y\":93},{\"x\":1097,\"y\":90}],\"text\":\"f1\"},{\"boundingBox\":[{\"x\":1428,\"y\":60},{\"x\":1451,\"y\":62},{\"x\":1449,\"y\":91},{\"x\":1426,\"y\":88}],\"text\":\"f1\"},{\"boundingBox\":[{\"x\":613,\"y\":118},{\"x\":814,\"y\":133},{\"x\":811,\"y\":169},{\"x\":611,\"y\":157}],\"text\":\"I View 1 01,1\"},{\"boundingBox\":[{\"x\":838,\"y\":93},{\"x\":990,\"y\":94},{\"x\":990,\"y\":125},{\"x\":838,\"y\":124}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1161,\"y\":94},{\"x\":1316,\"y\":94},{\"x\":1316,\"y\":124},{\"x\":1161,\"y\":124}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1489,\"y\":94},{\"x\":1644,\"y\":94},{\"x\":1644,\"y\":124},{\"x\":1489,\"y\":124}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":875,\"y\":129},{\"x\":953,\"y\":130},{\"x\":953,\"y\":157},{\"x\":875,\"y\":156}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1053,\"y\":145},{\"x\":1102,\"y\":139},{\"x\":1103,\"y\":159},{\"x\":1054,\"y\":163}],\"text\":\"Q2,1\"},{\"boundingBox\":[{\"x\":1201,\"y\":130},{\"x\":1280,\"y\":131},{\"x\":1279,\"y\":157},{\"x\":1201,\"y\":156}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1399,\"y\":140},{\"x\":1427,\"y\":139},{\"x\":1428,\"y\":158},{\"x\":1400,\"y\":158}],\"text\":\"3,1\"},{\"boundingBox\":[{\"x\":1526,\"y\":130},{\"x\":1604,\"y\":130},{\"x\":1604,\"y\":157},{\"x\":1525,\"y\":157}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":884,\"y\":167},{\"x\":945,\"y\":166},{\"x\":947,\"y\":197},{\"x\":882,\"y\":197}],\"text\":\"(1,1)\"},{\"boundingBox\":[{\"x\":1075,\"y\":163},{\"x\":1088,\"y\":163},{\"x\":1088,\"y\":177},{\"x\":1074,\"y\":177}],\"text\":\"c\"},{\"boundingBox\":[{\"x\":1211,\"y\":167},{\"x\":1269,\"y\":166},{\"x\":1271,\"y\":196},{\"x\":1210,\"y\":196}],\"text\":\"(2,1)\"},{\"boundingBox\":[{\"x\":1401,\"y\":163},{\"x\":1414,\"y\":163},{\"x\":1413,\"y\":176},{\"x\":1401,\"y\":176}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":1537,\"y\":166},{\"x\":1596,\"y\":167},{\"x\":1596,\"y\":197},{\"x\":1535,\"y\":197}],\"text\":\"(3,1)\"},{\"boundingBox\":[{\"x\":631,\"y\":193},{\"x\":630,\"y\":244},{\"x\":610,\"y\":243},{\"x\":610,\"y\":191}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1792,\"y\":223},{\"x\":1793,\"y\":277},{\"x\":1774,\"y\":278},{\"x\":1774,\"y\":224}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1560,\"y\":258},{\"x\":1625,\"y\":251},{\"x\":1626,\"y\":300},{\"x\":1561,\"y\":307}],\"text\":\"103.2\"},{\"boundingBox\":[{\"x\":630,\"y\":269},{\"x\":629,\"y\":322},{\"x\":610,\"y\":322},{\"x\":610,\"y\":270}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":939,\"y\":284},{\"x\":959,\"y\":284},{\"x\":958,\"y\":308},{\"x\":939,\"y\":308}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1236,\"y\":262},{\"x\":1302,\"y\":259},{\"x\":1303,\"y\":290},{\"x\":1237,\"y\":296}],\"text\":\"@ 2,2\"},{\"boundingBox\":[{\"x\":1271,\"y\":290},{\"x\":1283,\"y\":290},{\"x\":1283,\"y\":306},{\"x\":1271,\"y\":306}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1595,\"y\":287},{\"x\":1616,\"y\":287},{\"x\":1615,\"y\":307},{\"x\":1594,\"y\":307}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1794,\"y\":301},{\"x\":1794,\"y\":353},{\"x\":1773,\"y\":352},{\"x\":1774,\"y\":301}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":554,\"y\":321},{\"x\":581,\"y\":323},{\"x\":581,\"y\":359},{\"x\":554,\"y\":357}],\"text\":\"f2\"},{\"boundingBox\":[{\"x\":776,\"y\":333},{\"x\":802,\"y\":334},{\"x\":800,\"y\":367},{\"x\":774,\"y\":366}],\"text\":\"f2\"},{\"boundingBox\":[{\"x\":1101,\"y\":334},{\"x\":1126,\"y\":335},{\"x\":1126,\"y\":367},{\"x\":1100,\"y\":365}],\"text\":\"f2\"},{\"boundingBox\":[{\"x\":1426,\"y\":332},{\"x\":1452,\"y\":335},{\"x\":1451,\"y\":368},{\"x\":1426,\"y\":366}],\"text\":\"f2\"},{\"boundingBox\":[{\"x\":475,\"y\":364},{\"x\":476,\"y\":596},{\"x\":444,\"y\":596},{\"x\":443,\"y\":364}],\"text\":\"Image Encoder\"},{\"boundingBox\":[{\"x\":838,\"y\":367},{\"x\":991,\"y\":368},{\"x\":991,\"y\":397},{\"x\":838,\"y\":396}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1162,\"y\":367},{\"x\":1316,\"y\":368},{\"x\":1316,\"y\":397},{\"x\":1162,\"y\":397}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1489,\"y\":368},{\"x\":1643,\"y\":368},{\"x\":1643,\"y\":396},{\"x\":1489,\"y\":397}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1796,\"y\":373},{\"x\":1796,\"y\":431},{\"x\":1773,\"y\":430},{\"x\":1774,\"y\":373}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":640,\"y\":400},{\"x\":752,\"y\":404},{\"x\":751,\"y\":436},{\"x\":639,\"y\":432}],\"text\":\"View 2\"},{\"boundingBox\":[{\"x\":758,\"y\":410},{\"x\":817,\"y\":409},{\"x\":817,\"y\":439},{\"x\":758,\"y\":442}],\"text\":\"C1,2\"},{\"boundingBox\":[{\"x\":875,\"y\":403},{\"x\":954,\"y\":403},{\"x\":954,\"y\":431},{\"x\":875,\"y\":429}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1070,\"y\":413},{\"x\":1103,\"y\":412},{\"x\":1103,\"y\":433},{\"x\":1071,\"y\":434}],\"text\":\"2,2\"},{\"boundingBox\":[{\"x\":1202,\"y\":403},{\"x\":1280,\"y\":404},{\"x\":1280,\"y\":431},{\"x\":1201,\"y\":429}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1399,\"y\":413},{\"x\":1428,\"y\":412},{\"x\":1429,\"y\":430},{\"x\":1400,\"y\":430}],\"text\":\"3.2\"},{\"boundingBox\":[{\"x\":1530,\"y\":403},{\"x\":1607,\"y\":403},{\"x\":1607,\"y\":431},{\"x\":1529,\"y\":429}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1834,\"y\":416},{\"x\":1833,\"y\":454},{\"x\":1822,\"y\":454},{\"x\":1823,\"y\":416}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":786,\"y\":435},{\"x\":797,\"y\":436},{\"x\":796,\"y\":450},{\"x\":786,\"y\":450}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":885,\"y\":439},{\"x\":944,\"y\":440},{\"x\":943,\"y\":471},{\"x\":883,\"y\":469}],\"text\":\"(1,2)\"},{\"boundingBox\":[{\"x\":1075,\"y\":437},{\"x\":1086,\"y\":436},{\"x\":1086,\"y\":449},{\"x\":1075,\"y\":450}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":1209,\"y\":440},{\"x\":1268,\"y\":440},{\"x\":1269,\"y\":470},{\"x\":1208,\"y\":470}],\"text\":\"(2,2)\"},{\"boundingBox\":[{\"x\":1401,\"y\":436},{\"x\":1412,\"y\":436},{\"x\":1412,\"y\":450},{\"x\":1401,\"y\":450}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":1536,\"y\":439},{\"x\":1594,\"y\":440},{\"x\":1594,\"y\":472},{\"x\":1536,\"y\":470}],\"text\":\"(3,2)\"},{\"boundingBox\":[{\"x\":1795,\"y\":457},{\"x\":1794,\"y\":510},{\"x\":1774,\"y\":510},{\"x\":1773,\"y\":455}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1794,\"y\":536},{\"x\":1795,\"y\":594},{\"x\":1774,\"y\":594},{\"x\":1773,\"y\":537}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":697,\"y\":535},{\"x\":697,\"y\":571},{\"x\":684,\"y\":572},{\"x\":684,\"y\":537}],\"text\":\"...\"},{\"boundingBox\":[{\"x\":940,\"y\":545},{\"x\":938,\"y\":567},{\"x\":920,\"y\":561},{\"x\":921,\"y\":540}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":1600,\"y\":546},{\"x\":1597,\"y\":567},{\"x\":1578,\"y\":561},{\"x\":1581,\"y\":539}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":937,\"y\":557},{\"x\":961,\"y\":557},{\"x\":960,\"y\":583},{\"x\":937,\"y\":583}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1266,\"y\":560},{\"x\":1288,\"y\":560},{\"x\":1286,\"y\":583},{\"x\":1265,\"y\":581}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1595,\"y\":559},{\"x\":1615,\"y\":559},{\"x\":1613,\"y\":582},{\"x\":1594,\"y\":582}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":573,\"y\":584},{\"x\":574,\"y\":616},{\"x\":559,\"y\":617},{\"x\":559,\"y\":585}],\"text\":\"...\"},{\"boundingBox\":[{\"x\":314,\"y\":590},{\"x\":314,\"y\":626},{\"x\":303,\"y\":626},{\"x\":302,\"y\":591}],\"text\":\"...\"},{\"boundingBox\":[{\"x\":777,\"y\":610},{\"x\":802,\"y\":611},{\"x\":799,\"y\":641},{\"x\":774,\"y\":638}],\"text\":\"fn\"},{\"boundingBox\":[{\"x\":1103,\"y\":609},{\"x\":1126,\"y\":611},{\"x\":1123,\"y\":641},{\"x\":1098,\"y\":638}],\"text\":\"fn\"},{\"boundingBox\":[{\"x\":1428,\"y\":609},{\"x\":1453,\"y\":612},{\"x\":1450,\"y\":640},{\"x\":1424,\"y\":638}],\"text\":\"fn\"},{\"boundingBox\":[{\"x\":1795,\"y\":614},{\"x\":1794,\"y\":665},{\"x\":1774,\"y\":665},{\"x\":1773,\"y\":616}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":839,\"y\":641},{\"x\":992,\"y\":642},{\"x\":992,\"y\":671},{\"x\":839,\"y\":670}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1162,\"y\":641},{\"x\":1317,\"y\":641},{\"x\":1317,\"y\":671},{\"x\":1162,\"y\":671}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1491,\"y\":642},{\"x\":1642,\"y\":642},{\"x\":1642,\"y\":671},{\"x\":1491,\"y\":671}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":609,\"y\":675},{\"x\":817,\"y\":684},{\"x\":815,\"y\":720},{\"x\":608,\"y\":709}],\"text\":\"View n 1,77\"},{\"boundingBox\":[{\"x\":875,\"y\":676},{\"x\":955,\"y\":677},{\"x\":954,\"y\":704},{\"x\":875,\"y\":703}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1050,\"y\":690},{\"x\":1104,\"y\":688},{\"x\":1104,\"y\":711},{\"x\":1049,\"y\":713}],\"text\":\"A2,n\"},{\"boundingBox\":[{\"x\":1200,\"y\":677},{\"x\":1281,\"y\":678},{\"x\":1281,\"y\":704},{\"x\":1200,\"y\":704}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1400,\"y\":688},{\"x\":1428,\"y\":689},{\"x\":1428,\"y\":705},{\"x\":1401,\"y\":704}],\"text\":\"3,n\"},{\"boundingBox\":[{\"x\":1526,\"y\":677},{\"x\":1605,\"y\":677},{\"x\":1605,\"y\":704},{\"x\":1525,\"y\":704}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":884,\"y\":713},{\"x\":945,\"y\":714},{\"x\":945,\"y\":745},{\"x\":883,\"y\":743}],\"text\":\"(1,n)\"},{\"boundingBox\":[{\"x\":1208,\"y\":714},{\"x\":1270,\"y\":714},{\"x\":1270,\"y\":744},{\"x\":1208,\"y\":743}],\"text\":\"(2,n)\"},{\"boundingBox\":[{\"x\":1401,\"y\":710},{\"x\":1412,\"y\":710},{\"x\":1411,\"y\":723},{\"x\":1401,\"y\":723}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":1535,\"y\":714},{\"x\":1594,\"y\":714},{\"x\":1594,\"y\":745},{\"x\":1535,\"y\":744}],\"text\":\"(3,n)\"},{\"boundingBox\":[{\"x\":1825,\"y\":714},{\"x\":1834,\"y\":714},{\"x\":1834,\"y\":727},{\"x\":1825,\"y\":727}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":630,\"y\":749},{\"x\":630,\"y\":776},{\"x\":612,\"y\":777},{\"x\":612,\"y\":750}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1791,\"y\":782},{\"x\":1791,\"y\":821},{\"x\":1773,\"y\":820},{\"x\":1772,\"y\":782}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":553,\"y\":799},{\"x\":578,\"y\":800},{\"x\":577,\"y\":832},{\"x\":552,\"y\":830}],\"text\":\"fn\"},{\"boundingBox\":[{\"x\":1237,\"y\":791},{\"x\":1299,\"y\":787},{\"x\":1300,\"y\":817},{\"x\":1238,\"y\":825}],\"text\":\"@3,1\"},{\"boundingBox\":[{\"x\":943,\"y\":801},{\"x\":934,\"y\":823},{\"x\":918,\"y\":813},{\"x\":926,\"y\":791}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":940,\"y\":812},{\"x\":959,\"y\":813},{\"x\":957,\"y\":837},{\"x\":939,\"y\":836}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1827,\"y\":804},{\"x\":1838,\"y\":805},{\"x\":1837,\"y\":820},{\"x\":1826,\"y\":819}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1271,\"y\":818},{\"x\":1282,\"y\":818},{\"x\":1282,\"y\":834},{\"x\":1271,\"y\":834}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":857,\"y\":855},{\"x\":971,\"y\":856},{\"x\":971,\"y\":889},{\"x\":857,\"y\":888}],\"text\":\"Stage 1\"},{\"boundingBox\":[{\"x\":1180,\"y\":856},{\"x\":1301,\"y\":856},{\"x\":1300,\"y\":888},{\"x\":1180,\"y\":888}],\"text\":\"Stage 2\"},{\"boundingBox\":[{\"x\":1507,\"y\":856},{\"x\":1626,\"y\":856},{\"x\":1626,\"y\":888},{\"x\":1507,\"y\":888}],\"text\":\"Stage 3\"},{\"boundingBox\":[{\"x\":906,\"y\":892},{\"x\":1573,\"y\":892},{\"x\":1573,\"y\":926},{\"x\":906,\"y\":925}],\"text\":\"Multi-View Multi-Stage Regression Network\"}],\"words\":[{\"boundingBox\":[{\"x\":926,\"y\":25},{\"x\":937,\"y\":25},{\"x\":937,\"y\":42},{\"x\":926,\"y\":42}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":944,\"y\":17},{\"x\":969,\"y\":17},{\"x\":970,\"y\":35},{\"x\":944,\"y\":36}],\"text\":\"1,1\"},{\"boundingBox\":[{\"x\":1235,\"y\":14},{\"x\":1293,\"y\":14},{\"x\":1297,\"y\":47},{\"x\":1235,\"y\":53}],\"text\":\"102,1\"},{\"boundingBox\":[{\"x\":1601,\"y\":17},{\"x\":1627,\"y\":17},{\"x\":1627,\"y\":36},{\"x\":1601,\"y\":36}],\"text\":\"3,1\"},{\"boundingBox\":[{\"x\":946,\"y\":43},{\"x\":954,\"y\":43},{\"x\":955,\"y\":58},{\"x\":946,\"y\":59}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1272,\"y\":41},{\"x\":1285,\"y\":41},{\"x\":1285,\"y\":60},{\"x\":1272,\"y\":60}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1600,\"y\":44},{\"x\":1610,\"y\":44},{\"x\":1609,\"y\":59},{\"x\":1600,\"y\":58}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":555,\"y\":68},{\"x\":579,\"y\":69},{\"x\":577,\"y\":100},{\"x\":553,\"y\":98}],\"text\":\"fi\"},{\"boundingBox\":[{\"x\":777,\"y\":61},{\"x\":798,\"y\":64},{\"x\":795,\"y\":90},{\"x\":774,\"y\":88}],\"text\":\"f1\"},{\"boundingBox\":[{\"x\":1099,\"y\":59},{\"x\":1124,\"y\":62},{\"x\":1120,\"y\":93},{\"x\":1097,\"y\":90}],\"text\":\"f1\"},{\"boundingBox\":[{\"x\":1429,\"y\":60},{\"x\":1451,\"y\":62},{\"x\":1448,\"y\":91},{\"x\":1426,\"y\":88}],\"text\":\"f1\"},{\"boundingBox\":[{\"x\":613,\"y\":119},{\"x\":627,\"y\":120},{\"x\":625,\"y\":159},{\"x\":612,\"y\":158}],\"text\":\"I\"},{\"boundingBox\":[{\"x\":634,\"y\":121},{\"x\":704,\"y\":127},{\"x\":702,\"y\":163},{\"x\":632,\"y\":159}],\"text\":\"View\"},{\"boundingBox\":[{\"x\":718,\"y\":128},{\"x\":739,\"y\":130},{\"x\":737,\"y\":165},{\"x\":716,\"y\":164}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":763,\"y\":132},{\"x\":813,\"y\":136},{\"x\":810,\"y\":169},{\"x\":761,\"y\":166}],\"text\":\"01,1\"},{\"boundingBox\":[{\"x\":840,\"y\":94},{\"x\":987,\"y\":96},{\"x\":987,\"y\":124},{\"x\":838,\"y\":124}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1163,\"y\":94},{\"x\":1313,\"y\":95},{\"x\":1313,\"y\":123},{\"x\":1162,\"y\":123}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1491,\"y\":95},{\"x\":1639,\"y\":95},{\"x\":1638,\"y\":124},{\"x\":1490,\"y\":123}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":875,\"y\":129},{\"x\":950,\"y\":130},{\"x\":949,\"y\":157},{\"x\":875,\"y\":156}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1054,\"y\":144},{\"x\":1101,\"y\":139},{\"x\":1103,\"y\":159},{\"x\":1056,\"y\":164}],\"text\":\"Q2,1\"},{\"boundingBox\":[{\"x\":1201,\"y\":130},{\"x\":1275,\"y\":131},{\"x\":1274,\"y\":157},{\"x\":1201,\"y\":156}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1401,\"y\":139},{\"x\":1426,\"y\":139},{\"x\":1427,\"y\":157},{\"x\":1401,\"y\":158}],\"text\":\"3,1\"},{\"boundingBox\":[{\"x\":1525,\"y\":130},{\"x\":1600,\"y\":130},{\"x\":1600,\"y\":157},{\"x\":1525,\"y\":157}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":883,\"y\":166},{\"x\":946,\"y\":166},{\"x\":946,\"y\":196},{\"x\":884,\"y\":197}],\"text\":\"(1,1)\"},{\"boundingBox\":[{\"x\":1074,\"y\":163},{\"x\":1082,\"y\":163},{\"x\":1082,\"y\":177},{\"x\":1074,\"y\":177}],\"text\":\"c\"},{\"boundingBox\":[{\"x\":1210,\"y\":166},{\"x\":1270,\"y\":166},{\"x\":1270,\"y\":195},{\"x\":1210,\"y\":196}],\"text\":\"(2,1)\"},{\"boundingBox\":[{\"x\":1405,\"y\":163},{\"x\":1413,\"y\":163},{\"x\":1413,\"y\":176},{\"x\":1405,\"y\":176}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":1535,\"y\":166},{\"x\":1595,\"y\":166},{\"x\":1595,\"y\":197},{\"x\":1535,\"y\":196}],\"text\":\"(3,1)\"},{\"boundingBox\":[{\"x\":632,\"y\":192},{\"x\":631,\"y\":204},{\"x\":610,\"y\":204},{\"x\":611,\"y\":192}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1793,\"y\":223},{\"x\":1793,\"y\":234},{\"x\":1774,\"y\":234},{\"x\":1774,\"y\":223}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1560,\"y\":258},{\"x\":1619,\"y\":252},{\"x\":1624,\"y\":299},{\"x\":1561,\"y\":306}],\"text\":\"103.2\"},{\"boundingBox\":[{\"x\":630,\"y\":273},{\"x\":630,\"y\":284},{\"x\":610,\"y\":284},{\"x\":610,\"y\":272}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":943,\"y\":284},{\"x\":958,\"y\":284},{\"x\":958,\"y\":308},{\"x\":943,\"y\":308}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1244,\"y\":261},{\"x\":1256,\"y\":261},{\"x\":1258,\"y\":295},{\"x\":1247,\"y\":295}],\"text\":\"@\"},{\"boundingBox\":[{\"x\":1262,\"y\":260},{\"x\":1298,\"y\":259},{\"x\":1300,\"y\":292},{\"x\":1265,\"y\":294}],\"text\":\"2,2\"},{\"boundingBox\":[{\"x\":1273,\"y\":290},{\"x\":1282,\"y\":290},{\"x\":1282,\"y\":306},{\"x\":1273,\"y\":306}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1602,\"y\":287},{\"x\":1615,\"y\":287},{\"x\":1615,\"y\":307},{\"x\":1602,\"y\":307}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1795,\"y\":301},{\"x\":1794,\"y\":313},{\"x\":1773,\"y\":313},{\"x\":1774,\"y\":301}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":554,\"y\":321},{\"x\":582,\"y\":323},{\"x\":580,\"y\":359},{\"x\":554,\"y\":357}],\"text\":\"f2\"},{\"boundingBox\":[{\"x\":775,\"y\":333},{\"x\":801,\"y\":334},{\"x\":800,\"y\":367},{\"x\":774,\"y\":366}],\"text\":\"f2\"},{\"boundingBox\":[{\"x\":1102,\"y\":334},{\"x\":1127,\"y\":335},{\"x\":1125,\"y\":367},{\"x\":1100,\"y\":365}],\"text\":\"f2\"},{\"boundingBox\":[{\"x\":1427,\"y\":332},{\"x\":1453,\"y\":334},{\"x\":1450,\"y\":368},{\"x\":1426,\"y\":365}],\"text\":\"f2\"},{\"boundingBox\":[{\"x\":475,\"y\":365},{\"x\":476,\"y\":456},{\"x\":444,\"y\":457},{\"x\":443,\"y\":365}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":476,\"y\":463},{\"x\":475,\"y\":597},{\"x\":445,\"y\":597},{\"x\":444,\"y\":463}],\"text\":\"Encoder\"},{\"boundingBox\":[{\"x\":838,\"y\":368},{\"x\":986,\"y\":369},{\"x\":986,\"y\":397},{\"x\":838,\"y\":396}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1163,\"y\":368},{\"x\":1313,\"y\":368},{\"x\":1313,\"y\":398},{\"x\":1163,\"y\":397}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1490,\"y\":369},{\"x\":1639,\"y\":368},{\"x\":1638,\"y\":397},{\"x\":1490,\"y\":397}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1796,\"y\":379},{\"x\":1796,\"y\":392},{\"x\":1773,\"y\":392},{\"x\":1773,\"y\":379}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":642,\"y\":401},{\"x\":701,\"y\":403},{\"x\":699,\"y\":435},{\"x\":639,\"y\":433}],\"text\":\"View\"},{\"boundingBox\":[{\"x\":718,\"y\":404},{\"x\":739,\"y\":405},{\"x\":736,\"y\":436},{\"x\":716,\"y\":435}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":760,\"y\":410},{\"x\":817,\"y\":409},{\"x\":818,\"y\":439},{\"x\":762,\"y\":441}],\"text\":\"C1,2\"},{\"boundingBox\":[{\"x\":875,\"y\":403},{\"x\":951,\"y\":403},{\"x\":950,\"y\":431},{\"x\":875,\"y\":430}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1075,\"y\":413},{\"x\":1101,\"y\":412},{\"x\":1102,\"y\":433},{\"x\":1075,\"y\":434}],\"text\":\"2,2\"},{\"boundingBox\":[{\"x\":1201,\"y\":403},{\"x\":1274,\"y\":404},{\"x\":1273,\"y\":431},{\"x\":1201,\"y\":429}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1401,\"y\":412},{\"x\":1428,\"y\":412},{\"x\":1428,\"y\":429},{\"x\":1401,\"y\":430}],\"text\":\"3.2\"},{\"boundingBox\":[{\"x\":1529,\"y\":403},{\"x\":1601,\"y\":403},{\"x\":1601,\"y\":431},{\"x\":1529,\"y\":430}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1834,\"y\":425},{\"x\":1834,\"y\":453},{\"x\":1823,\"y\":453},{\"x\":1823,\"y\":425}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":788,\"y\":435},{\"x\":796,\"y\":435},{\"x\":795,\"y\":450},{\"x\":788,\"y\":450}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":885,\"y\":439},{\"x\":943,\"y\":440},{\"x\":942,\"y\":471},{\"x\":884,\"y\":469}],\"text\":\"(1,2)\"},{\"boundingBox\":[{\"x\":1079,\"y\":437},{\"x\":1085,\"y\":436},{\"x\":1086,\"y\":448},{\"x\":1080,\"y\":449}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":1208,\"y\":440},{\"x\":1268,\"y\":440},{\"x\":1268,\"y\":470},{\"x\":1208,\"y\":470}],\"text\":\"(2,2)\"},{\"boundingBox\":[{\"x\":1403,\"y\":436},{\"x\":1410,\"y\":436},{\"x\":1410,\"y\":450},{\"x\":1403,\"y\":450}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":1536,\"y\":439},{\"x\":1594,\"y\":440},{\"x\":1593,\"y\":472},{\"x\":1536,\"y\":470}],\"text\":\"(3,2)\"},{\"boundingBox\":[{\"x\":1795,\"y\":460},{\"x\":1795,\"y\":473},{\"x\":1773,\"y\":473},{\"x\":1773,\"y\":460}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1794,\"y\":538},{\"x\":1794,\"y\":550},{\"x\":1773,\"y\":551},{\"x\":1773,\"y\":538}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":697,\"y\":538},{\"x\":697,\"y\":568},{\"x\":684,\"y\":568},{\"x\":684,\"y\":538}],\"text\":\"...\"},{\"boundingBox\":[{\"x\":940,\"y\":551},{\"x\":939,\"y\":564},{\"x\":920,\"y\":562},{\"x\":921,\"y\":550}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":1599,\"y\":551},{\"x\":1598,\"y\":564},{\"x\":1579,\"y\":561},{\"x\":1581,\"y\":549}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":941,\"y\":557},{\"x\":958,\"y\":557},{\"x\":958,\"y\":583},{\"x\":941,\"y\":583}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1266,\"y\":560},{\"x\":1284,\"y\":560},{\"x\":1283,\"y\":583},{\"x\":1265,\"y\":582}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1597,\"y\":559},{\"x\":1612,\"y\":559},{\"x\":1612,\"y\":582},{\"x\":1597,\"y\":582}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":574,\"y\":585},{\"x\":574,\"y\":615},{\"x\":559,\"y\":615},{\"x\":559,\"y\":585}],\"text\":\"...\"},{\"boundingBox\":[{\"x\":314,\"y\":592},{\"x\":314,\"y\":621},{\"x\":302,\"y\":622},{\"x\":302,\"y\":592}],\"text\":\"...\"},{\"boundingBox\":[{\"x\":776,\"y\":610},{\"x\":800,\"y\":611},{\"x\":798,\"y\":641},{\"x\":774,\"y\":639}],\"text\":\"fn\"},{\"boundingBox\":[{\"x\":1101,\"y\":609},{\"x\":1125,\"y\":611},{\"x\":1122,\"y\":641},{\"x\":1098,\"y\":638}],\"text\":\"fn\"},{\"boundingBox\":[{\"x\":1426,\"y\":609},{\"x\":1449,\"y\":610},{\"x\":1446,\"y\":639},{\"x\":1424,\"y\":637}],\"text\":\"fn\"},{\"boundingBox\":[{\"x\":1795,\"y\":618},{\"x\":1795,\"y\":631},{\"x\":1773,\"y\":631},{\"x\":1773,\"y\":618}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":839,\"y\":642},{\"x\":985,\"y\":642},{\"x\":985,\"y\":671},{\"x\":839,\"y\":670}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1164,\"y\":642},{\"x\":1312,\"y\":642},{\"x\":1313,\"y\":671},{\"x\":1163,\"y\":670}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1492,\"y\":643},{\"x\":1637,\"y\":643},{\"x\":1638,\"y\":670},{\"x\":1491,\"y\":670}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":634,\"y\":676},{\"x\":702,\"y\":677},{\"x\":701,\"y\":712},{\"x\":634,\"y\":711}],\"text\":\"View\"},{\"boundingBox\":[{\"x\":715,\"y\":678},{\"x\":734,\"y\":680},{\"x\":734,\"y\":714},{\"x\":715,\"y\":713}],\"text\":\"n\"},{\"boundingBox\":[{\"x\":783,\"y\":686},{\"x\":815,\"y\":691},{\"x\":815,\"y\":720},{\"x\":782,\"y\":717}],\"text\":\"1,77\"},{\"boundingBox\":[{\"x\":876,\"y\":677},{\"x\":949,\"y\":678},{\"x\":949,\"y\":705},{\"x\":875,\"y\":703}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1058,\"y\":690},{\"x\":1100,\"y\":688},{\"x\":1101,\"y\":711},{\"x\":1059,\"y\":713}],\"text\":\"A2,n\"},{\"boundingBox\":[{\"x\":1201,\"y\":678},{\"x\":1275,\"y\":678},{\"x\":1274,\"y\":705},{\"x\":1201,\"y\":704}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1401,\"y\":688},{\"x\":1426,\"y\":689},{\"x\":1426,\"y\":705},{\"x\":1400,\"y\":704}],\"text\":\"3,n\"},{\"boundingBox\":[{\"x\":1525,\"y\":677},{\"x\":1602,\"y\":677},{\"x\":1602,\"y\":704},{\"x\":1525,\"y\":704}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":883,\"y\":713},{\"x\":945,\"y\":714},{\"x\":944,\"y\":745},{\"x\":883,\"y\":743}],\"text\":\"(1,n)\"},{\"boundingBox\":[{\"x\":1210,\"y\":714},{\"x\":1270,\"y\":714},{\"x\":1270,\"y\":744},{\"x\":1209,\"y\":743}],\"text\":\"(2,n)\"},{\"boundingBox\":[{\"x\":1401,\"y\":710},{\"x\":1408,\"y\":710},{\"x\":1408,\"y\":723},{\"x\":1401,\"y\":723}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":1535,\"y\":714},{\"x\":1593,\"y\":714},{\"x\":1593,\"y\":745},{\"x\":1535,\"y\":744}],\"text\":\"(3,n)\"},{\"boundingBox\":[{\"x\":1827,\"y\":714},{\"x\":1834,\"y\":714},{\"x\":1834,\"y\":727},{\"x\":1827,\"y\":727}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":630,\"y\":749},{\"x\":630,\"y\":760},{\"x\":612,\"y\":760},{\"x\":612,\"y\":749}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1791,\"y\":790},{\"x\":1791,\"y\":802},{\"x\":1772,\"y\":802},{\"x\":1772,\"y\":790}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":553,\"y\":799},{\"x\":574,\"y\":800},{\"x\":572,\"y\":832},{\"x\":552,\"y\":830}],\"text\":\"fn\"},{\"boundingBox\":[{\"x\":1244,\"y\":790},{\"x\":1296,\"y\":787},{\"x\":1299,\"y\":818},{\"x\":1247,\"y\":823}],\"text\":\"@3,1\"},{\"boundingBox\":[{\"x\":941,\"y\":807},{\"x\":937,\"y\":819},{\"x\":919,\"y\":812},{\"x\":923,\"y\":800}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":940,\"y\":812},{\"x\":956,\"y\":813},{\"x\":955,\"y\":837},{\"x\":939,\"y\":836}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1827,\"y\":804},{\"x\":1837,\"y\":805},{\"x\":1836,\"y\":820},{\"x\":1826,\"y\":819}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1273,\"y\":818},{\"x\":1282,\"y\":818},{\"x\":1282,\"y\":834},{\"x\":1273,\"y\":834}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":858,\"y\":856},{\"x\":942,\"y\":857},{\"x\":941,\"y\":889},{\"x\":857,\"y\":888}],\"text\":\"Stage\"},{\"boundingBox\":[{\"x\":953,\"y\":857},{\"x\":970,\"y\":857},{\"x\":970,\"y\":889},{\"x\":952,\"y\":889}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":1181,\"y\":857},{\"x\":1269,\"y\":857},{\"x\":1269,\"y\":889},{\"x\":1180,\"y\":887}],\"text\":\"Stage\"},{\"boundingBox\":[{\"x\":1278,\"y\":857},{\"x\":1296,\"y\":857},{\"x\":1296,\"y\":889},{\"x\":1277,\"y\":889}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":1508,\"y\":857},{\"x\":1593,\"y\":857},{\"x\":1594,\"y\":889},{\"x\":1507,\"y\":887}],\"text\":\"Stage\"},{\"boundingBox\":[{\"x\":1602,\"y\":857},{\"x\":1620,\"y\":857},{\"x\":1621,\"y\":889},{\"x\":1603,\"y\":889}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":907,\"y\":893},{\"x\":1057,\"y\":893},{\"x\":1058,\"y\":925},{\"x\":907,\"y\":922}],\"text\":\"Multi-View\"},{\"boundingBox\":[{\"x\":1072,\"y\":893},{\"x\":1251,\"y\":893},{\"x\":1252,\"y\":927},{\"x\":1073,\"y\":925}],\"text\":\"Multi-Stage\"},{\"boundingBox\":[{\"x\":1258,\"y\":893},{\"x\":1427,\"y\":892},{\"x\":1428,\"y\":926},{\"x\":1258,\"y\":927}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1436,\"y\":892},{\"x\":1568,\"y\":892},{\"x\":1569,\"y\":925},{\"x\":1438,\"y\":926}],\"text\":\"Network\"}]}",
        "{\"language\":\"en\",\"text\":\"dreamstime dreamstie (a) Input image (b) Recovered body\",\"lines\":[{\"boundingBox\":[{\"x\":96,\"y\":216},{\"x\":177,\"y\":139},{\"x\":187,\"y\":148},{\"x\":106,\"y\":227}],\"text\":\"dreamstime\"},{\"boundingBox\":[{\"x\":112,\"y\":521},{\"x\":167,\"y\":466},{\"x\":177,\"y\":475},{\"x\":122,\"y\":531}],\"text\":\"dreamstie\"},{\"boundingBox\":[{\"x\":13,\"y\":594},{\"x\":242,\"y\":594},{\"x\":243,\"y\":626},{\"x\":13,\"y\":627}],\"text\":\"(a) Input image\"},{\"boundingBox\":[{\"x\":382,\"y\":594},{\"x\":670,\"y\":593},{\"x\":671,\"y\":625},{\"x\":382,\"y\":626}],\"text\":\"(b) Recovered body\"}],\"words\":[{\"boundingBox\":[{\"x\":114,\"y\":200},{\"x\":176,\"y\":142},{\"x\":185,\"y\":149},{\"x\":123,\"y\":212}],\"text\":\"dreamstime\"},{\"boundingBox\":[{\"x\":112,\"y\":521},{\"x\":165,\"y\":467},{\"x\":176,\"y\":476},{\"x\":122,\"y\":531}],\"text\":\"dreamstie\"},{\"boundingBox\":[{\"x\":13,\"y\":595},{\"x\":52,\"y\":595},{\"x\":52,\"y\":627},{\"x\":13,\"y\":627}],\"text\":\"(a)\"},{\"boundingBox\":[{\"x\":59,\"y\":595},{\"x\":140,\"y\":594},{\"x\":139,\"y\":626},{\"x\":59,\"y\":626}],\"text\":\"Input\"},{\"boundingBox\":[{\"x\":146,\"y\":594},{\"x\":241,\"y\":595},{\"x\":240,\"y\":627},{\"x\":146,\"y\":626}],\"text\":\"image\"},{\"boundingBox\":[{\"x\":384,\"y\":596},{\"x\":422,\"y\":595},{\"x\":421,\"y\":625},{\"x\":383,\"y\":625}],\"text\":\"(b)\"},{\"boundingBox\":[{\"x\":428,\"y\":595},{\"x\":589,\"y\":594},{\"x\":588,\"y\":626},{\"x\":427,\"y\":625}],\"text\":\"Recovered\"},{\"boundingBox\":[{\"x\":597,\"y\":594},{\"x\":671,\"y\":595},{\"x\":670,\"y\":626},{\"x\":596,\"y\":626}],\"text\":\"body\"}]}",
        "{\"language\":\"en\",\"text\":\"= = 1 h- 2.2 hn-1 (xl r.2 \\\". . . , x) 1 Classifier 1 Tu Tv2 Tv\\\" CNN CNN CNN Material type LSTM cell = Concatenate op T1 T2 rgb Tn rgb rgb\",\"lines\":[{\"boundingBox\":[{\"x\":570,\"y\":9},{\"x\":568,\"y\":48},{\"x\":554,\"y\":46},{\"x\":556,\"y\":9}],\"text\":\"=\"},{\"boundingBox\":[{\"x\":312,\"y\":13},{\"x\":309,\"y\":50},{\"x\":295,\"y\":48},{\"x\":298,\"y\":11}],\"text\":\"=\"},{\"boundingBox\":[{\"x\":118,\"y\":80},{\"x\":137,\"y\":80},{\"x\":137,\"y\":99},{\"x\":118,\"y\":99}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":234,\"y\":104},{\"x\":257,\"y\":101},{\"x\":258,\"y\":132},{\"x\":235,\"y\":135}],\"text\":\"h-\"},{\"boundingBox\":[{\"x\":319,\"y\":93},{\"x\":358,\"y\":83},{\"x\":359,\"y\":110},{\"x\":319,\"y\":119}],\"text\":\"2.2\"},{\"boundingBox\":[{\"x\":470,\"y\":107},{\"x\":536,\"y\":98},{\"x\":538,\"y\":121},{\"x\":473,\"y\":133}],\"text\":\"hn-1\"},{\"boundingBox\":[{\"x\":656,\"y\":85},{\"x\":746,\"y\":83},{\"x\":747,\"y\":108},{\"x\":657,\"y\":112}],\"text\":\"(xl r.2\"},{\"boundingBox\":[{\"x\":737,\"y\":99},{\"x\":859,\"y\":85},{\"x\":861,\"y\":110},{\"x\":738,\"y\":122}],\"text\":\"\\\". . . , x)\"},{\"boundingBox\":[{\"x\":265,\"y\":103},{\"x\":265,\"y\":123},{\"x\":253,\"y\":120},{\"x\":253,\"y\":100}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":758,\"y\":169},{\"x\":760,\"y\":304},{\"x\":729,\"y\":305},{\"x\":727,\"y\":170}],\"text\":\"Classifier\"},{\"boundingBox\":[{\"x\":126,\"y\":183},{\"x\":126,\"y\":199},{\"x\":116,\"y\":196},{\"x\":116,\"y\":180}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":81,\"y\":190},{\"x\":122,\"y\":189},{\"x\":121,\"y\":215},{\"x\":82,\"y\":218}],\"text\":\"Tu\"},{\"boundingBox\":[{\"x\":297,\"y\":190},{\"x\":342,\"y\":183},{\"x\":343,\"y\":211},{\"x\":299,\"y\":217}],\"text\":\"Tv2\"},{\"boundingBox\":[{\"x\":554,\"y\":192},{\"x\":605,\"y\":187},{\"x\":605,\"y\":213},{\"x\":558,\"y\":217}],\"text\":\"Tv\\\"\"},{\"boundingBox\":[{\"x\":60,\"y\":233},{\"x\":117,\"y\":234},{\"x\":118,\"y\":255},{\"x\":60,\"y\":254}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":278,\"y\":233},{\"x\":333,\"y\":234},{\"x\":333,\"y\":255},{\"x\":277,\"y\":254}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":536,\"y\":233},{\"x\":594,\"y\":234},{\"x\":594,\"y\":255},{\"x\":535,\"y\":255}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":843,\"y\":221},{\"x\":968,\"y\":223},{\"x\":968,\"y\":244},{\"x\":843,\"y\":242}],\"text\":\"Material type\"},{\"boundingBox\":[{\"x\":796,\"y\":322},{\"x\":917,\"y\":322},{\"x\":917,\"y\":346},{\"x\":796,\"y\":346}],\"text\":\"LSTM cell\"},{\"boundingBox\":[{\"x\":768,\"y\":379},{\"x\":766,\"y\":419},{\"x\":752,\"y\":418},{\"x\":754,\"y\":379}],\"text\":\"=\"},{\"boundingBox\":[{\"x\":795,\"y\":386},{\"x\":985,\"y\":388},{\"x\":984,\"y\":414},{\"x\":794,\"y\":412}],\"text\":\"Concatenate op\"},{\"boundingBox\":[{\"x\":138,\"y\":415},{\"x\":156,\"y\":415},{\"x\":156,\"y\":434},{\"x\":137,\"y\":435}],\"text\":\"T1\"},{\"boundingBox\":[{\"x\":356,\"y\":413},{\"x\":378,\"y\":415},{\"x\":377,\"y\":434},{\"x\":355,\"y\":434}],\"text\":\"T2\"},{\"boundingBox\":[{\"x\":367,\"y\":430},{\"x\":394,\"y\":431},{\"x\":394,\"y\":445},{\"x\":367,\"y\":444}],\"text\":\"rgb\"},{\"boundingBox\":[{\"x\":615,\"y\":417},{\"x\":640,\"y\":413},{\"x\":639,\"y\":430},{\"x\":615,\"y\":434}],\"text\":\"Tn\"},{\"boundingBox\":[{\"x\":146,\"y\":431},{\"x\":176,\"y\":431},{\"x\":177,\"y\":445},{\"x\":146,\"y\":444}],\"text\":\"rgb\"},{\"boundingBox\":[{\"x\":626,\"y\":426},{\"x\":655,\"y\":428},{\"x\":655,\"y\":443},{\"x\":626,\"y\":442}],\"text\":\"rgb\"}],\"words\":[{\"boundingBox\":[{\"x\":569,\"y\":30},{\"x\":568,\"y\":41},{\"x\":554,\"y\":40},{\"x\":555,\"y\":29}],\"text\":\"=\"},{\"boundingBox\":[{\"x\":310,\"y\":38},{\"x\":309,\"y\":49},{\"x\":295,\"y\":48},{\"x\":296,\"y\":37}],\"text\":\"=\"},{\"boundingBox\":[{\"x\":124,\"y\":80},{\"x\":135,\"y\":80},{\"x\":135,\"y\":99},{\"x\":124,\"y\":99}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":234,\"y\":104},{\"x\":255,\"y\":101},{\"x\":259,\"y\":131},{\"x\":235,\"y\":134}],\"text\":\"h-\"},{\"boundingBox\":[{\"x\":319,\"y\":94},{\"x\":356,\"y\":83},{\"x\":360,\"y\":108},{\"x\":319,\"y\":119}],\"text\":\"2.2\"},{\"boundingBox\":[{\"x\":470,\"y\":106},{\"x\":532,\"y\":98},{\"x\":537,\"y\":123},{\"x\":473,\"y\":133}],\"text\":\"hn-1\"},{\"boundingBox\":[{\"x\":657,\"y\":85},{\"x\":694,\"y\":86},{\"x\":697,\"y\":109},{\"x\":658,\"y\":113}],\"text\":\"(xl\"},{\"boundingBox\":[{\"x\":707,\"y\":86},{\"x\":741,\"y\":84},{\"x\":745,\"y\":106},{\"x\":710,\"y\":108}],\"text\":\"r.2\"},{\"boundingBox\":[{\"x\":741,\"y\":102},{\"x\":763,\"y\":100},{\"x\":763,\"y\":120},{\"x\":741,\"y\":121}],\"text\":\"\\\".\"},{\"boundingBox\":[{\"x\":767,\"y\":99},{\"x\":777,\"y\":98},{\"x\":778,\"y\":119},{\"x\":768,\"y\":120}],\"text\":\".\"},{\"boundingBox\":[{\"x\":781,\"y\":98},{\"x\":791,\"y\":96},{\"x\":792,\"y\":118},{\"x\":782,\"y\":118}],\"text\":\".\"},{\"boundingBox\":[{\"x\":795,\"y\":96},{\"x\":805,\"y\":94},{\"x\":807,\"y\":116},{\"x\":797,\"y\":117}],\"text\":\",\"},{\"boundingBox\":[{\"x\":810,\"y\":94},{\"x\":858,\"y\":86},{\"x\":860,\"y\":111},{\"x\":811,\"y\":116}],\"text\":\"x)\"},{\"boundingBox\":[{\"x\":265,\"y\":100},{\"x\":265,\"y\":107},{\"x\":253,\"y\":107},{\"x\":253,\"y\":100}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":757,\"y\":170},{\"x\":758,\"y\":305},{\"x\":730,\"y\":306},{\"x\":727,\"y\":170}],\"text\":\"Classifier\"},{\"boundingBox\":[{\"x\":126,\"y\":181},{\"x\":126,\"y\":187},{\"x\":116,\"y\":187},{\"x\":116,\"y\":181}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":81,\"y\":190},{\"x\":114,\"y\":189},{\"x\":115,\"y\":216},{\"x\":82,\"y\":218}],\"text\":\"Tu\"},{\"boundingBox\":[{\"x\":297,\"y\":189},{\"x\":339,\"y\":183},{\"x\":343,\"y\":211},{\"x\":299,\"y\":217}],\"text\":\"Tv2\"},{\"boundingBox\":[{\"x\":557,\"y\":191},{\"x\":602,\"y\":187},{\"x\":604,\"y\":212},{\"x\":560,\"y\":217}],\"text\":\"Tv\\\"\"},{\"boundingBox\":[{\"x\":60,\"y\":233},{\"x\":108,\"y\":234},{\"x\":108,\"y\":255},{\"x\":60,\"y\":254}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":277,\"y\":233},{\"x\":325,\"y\":234},{\"x\":325,\"y\":255},{\"x\":277,\"y\":254}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":535,\"y\":233},{\"x\":584,\"y\":233},{\"x\":584,\"y\":255},{\"x\":535,\"y\":254}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":844,\"y\":222},{\"x\":917,\"y\":223},{\"x\":916,\"y\":244},{\"x\":843,\"y\":243}],\"text\":\"Material\"},{\"boundingBox\":[{\"x\":921,\"y\":223},{\"x\":965,\"y\":224},{\"x\":965,\"y\":245},{\"x\":920,\"y\":244}],\"text\":\"type\"},{\"boundingBox\":[{\"x\":796,\"y\":323},{\"x\":853,\"y\":322},{\"x\":853,\"y\":346},{\"x\":797,\"y\":347}],\"text\":\"LSTM\"},{\"boundingBox\":[{\"x\":870,\"y\":323},{\"x\":917,\"y\":323},{\"x\":917,\"y\":346},{\"x\":870,\"y\":346}],\"text\":\"cell\"},{\"boundingBox\":[{\"x\":767,\"y\":403},{\"x\":766,\"y\":416},{\"x\":752,\"y\":416},{\"x\":753,\"y\":403}],\"text\":\"=\"},{\"boundingBox\":[{\"x\":795,\"y\":386},{\"x\":946,\"y\":388},{\"x\":946,\"y\":415},{\"x\":795,\"y\":412}],\"text\":\"Concatenate\"},{\"boundingBox\":[{\"x\":952,\"y\":388},{\"x\":982,\"y\":388},{\"x\":982,\"y\":415},{\"x\":952,\"y\":415}],\"text\":\"op\"},{\"boundingBox\":[{\"x\":137,\"y\":415},{\"x\":155,\"y\":415},{\"x\":156,\"y\":435},{\"x\":138,\"y\":435}],\"text\":\"T1\"},{\"boundingBox\":[{\"x\":356,\"y\":413},{\"x\":377,\"y\":414},{\"x\":376,\"y\":435},{\"x\":355,\"y\":434}],\"text\":\"T2\"},{\"boundingBox\":[{\"x\":367,\"y\":430},{\"x\":393,\"y\":431},{\"x\":393,\"y\":445},{\"x\":367,\"y\":444}],\"text\":\"rgb\"},{\"boundingBox\":[{\"x\":617,\"y\":417},{\"x\":635,\"y\":414},{\"x\":638,\"y\":429},{\"x\":619,\"y\":433}],\"text\":\"Tn\"},{\"boundingBox\":[{\"x\":149,\"y\":431},{\"x\":175,\"y\":431},{\"x\":175,\"y\":445},{\"x\":148,\"y\":445}],\"text\":\"rgb\"},{\"boundingBox\":[{\"x\":627,\"y\":426},{\"x\":655,\"y\":427},{\"x\":654,\"y\":443},{\"x\":626,\"y\":441}],\"text\":\"rgb\"}]}",
        "{\"language\":\"en\",\"text\":\"F-1 F-25 F-3 F-105 F-1 F-25 F-38 F-105 1 F-1 F-3 0.9 F-4 0.8 0.7 0.6 0.5 0.4 0.3 0.2 F-1 F-2 F-4 0.1 F-3 0\",\"lines\":[{\"boundingBox\":[{\"x\":5,\"y\":199},{\"x\":28,\"y\":199},{\"x\":27,\"y\":215},{\"x\":4,\"y\":214}],\"text\":\"F-1\"},{\"boundingBox\":[{\"x\":236,\"y\":200},{\"x\":272,\"y\":200},{\"x\":271,\"y\":215},{\"x\":237,\"y\":215}],\"text\":\"F-25\"},{\"boundingBox\":[{\"x\":473,\"y\":200},{\"x\":496,\"y\":200},{\"x\":497,\"y\":213},{\"x\":474,\"y\":214}],\"text\":\"F-3\"},{\"boundingBox\":[{\"x\":701,\"y\":200},{\"x\":745,\"y\":200},{\"x\":745,\"y\":216},{\"x\":701,\"y\":215}],\"text\":\"F-105\"},{\"boundingBox\":[{\"x\":6,\"y\":432},{\"x\":28,\"y\":432},{\"x\":27,\"y\":447},{\"x\":5,\"y\":447}],\"text\":\"F-1\"},{\"boundingBox\":[{\"x\":236,\"y\":432},{\"x\":270,\"y\":433},{\"x\":270,\"y\":447},{\"x\":236,\"y\":446}],\"text\":\"F-25\"},{\"boundingBox\":[{\"x\":472,\"y\":433},{\"x\":506,\"y\":434},{\"x\":505,\"y\":446},{\"x\":472,\"y\":445}],\"text\":\"F-38\"},{\"boundingBox\":[{\"x\":701,\"y\":432},{\"x\":745,\"y\":432},{\"x\":745,\"y\":448},{\"x\":702,\"y\":447}],\"text\":\"F-105\"},{\"boundingBox\":[{\"x\":959,\"y\":625},{\"x\":968,\"y\":625},{\"x\":969,\"y\":638},{\"x\":959,\"y\":638}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":7,\"y\":664},{\"x\":28,\"y\":664},{\"x\":28,\"y\":678},{\"x\":6,\"y\":678}],\"text\":\"F-1\"},{\"boundingBox\":[{\"x\":471,\"y\":666},{\"x\":495,\"y\":666},{\"x\":494,\"y\":680},{\"x\":471,\"y\":679}],\"text\":\"F-3\"},{\"boundingBox\":[{\"x\":956,\"y\":653},{\"x\":981,\"y\":654},{\"x\":981,\"y\":667},{\"x\":956,\"y\":667}],\"text\":\"0.9\"},{\"boundingBox\":[{\"x\":704,\"y\":667},{\"x\":725,\"y\":667},{\"x\":725,\"y\":677},{\"x\":704,\"y\":678}],\"text\":\"F-4\"},{\"boundingBox\":[{\"x\":958,\"y\":682},{\"x\":982,\"y\":682},{\"x\":981,\"y\":696},{\"x\":958,\"y\":696}],\"text\":\"0.8\"},{\"boundingBox\":[{\"x\":957,\"y\":710},{\"x\":977,\"y\":710},{\"x\":977,\"y\":724},{\"x\":957,\"y\":724}],\"text\":\"0.7\"},{\"boundingBox\":[{\"x\":958,\"y\":739},{\"x\":983,\"y\":739},{\"x\":983,\"y\":752},{\"x\":958,\"y\":752}],\"text\":\"0.6\"},{\"boundingBox\":[{\"x\":957,\"y\":768},{\"x\":984,\"y\":768},{\"x\":984,\"y\":781},{\"x\":957,\"y\":781}],\"text\":\"0.5\"},{\"boundingBox\":[{\"x\":958,\"y\":795},{\"x\":981,\"y\":795},{\"x\":981,\"y\":809},{\"x\":958,\"y\":809}],\"text\":\"0.4\"},{\"boundingBox\":[{\"x\":957,\"y\":824},{\"x\":982,\"y\":823},{\"x\":982,\"y\":838},{\"x\":957,\"y\":837}],\"text\":\"0.3\"},{\"boundingBox\":[{\"x\":956,\"y\":852},{\"x\":984,\"y\":851},{\"x\":983,\"y\":865},{\"x\":956,\"y\":866}],\"text\":\"0.2\"},{\"boundingBox\":[{\"x\":5,\"y\":893},{\"x\":27,\"y\":893},{\"x\":27,\"y\":909},{\"x\":5,\"y\":909}],\"text\":\"F-1\"},{\"boundingBox\":[{\"x\":238,\"y\":893},{\"x\":260,\"y\":894},{\"x\":260,\"y\":909},{\"x\":239,\"y\":909}],\"text\":\"F-2\"},{\"boundingBox\":[{\"x\":702,\"y\":893},{\"x\":725,\"y\":894},{\"x\":725,\"y\":909},{\"x\":703,\"y\":908}],\"text\":\"F-4\"},{\"boundingBox\":[{\"x\":957,\"y\":881},{\"x\":980,\"y\":881},{\"x\":981,\"y\":894},{\"x\":958,\"y\":895}],\"text\":\"0.1\"},{\"boundingBox\":[{\"x\":473,\"y\":895},{\"x\":494,\"y\":895},{\"x\":494,\"y\":908},{\"x\":472,\"y\":907}],\"text\":\"F-3\"},{\"boundingBox\":[{\"x\":958,\"y\":909},{\"x\":968,\"y\":909},{\"x\":968,\"y\":921},{\"x\":958,\"y\":920}],\"text\":\"0\"}],\"words\":[{\"boundingBox\":[{\"x\":5,\"y\":199},{\"x\":27,\"y\":199},{\"x\":27,\"y\":215},{\"x\":4,\"y\":214}],\"text\":\"F-1\"},{\"boundingBox\":[{\"x\":236,\"y\":200},{\"x\":270,\"y\":200},{\"x\":270,\"y\":215},{\"x\":236,\"y\":215}],\"text\":\"F-25\"},{\"boundingBox\":[{\"x\":473,\"y\":200},{\"x\":495,\"y\":200},{\"x\":495,\"y\":214},{\"x\":473,\"y\":214}],\"text\":\"F-3\"},{\"boundingBox\":[{\"x\":701,\"y\":200},{\"x\":743,\"y\":200},{\"x\":743,\"y\":216},{\"x\":701,\"y\":215}],\"text\":\"F-105\"},{\"boundingBox\":[{\"x\":5,\"y\":432},{\"x\":28,\"y\":432},{\"x\":28,\"y\":447},{\"x\":5,\"y\":447}],\"text\":\"F-1\"},{\"boundingBox\":[{\"x\":237,\"y\":432},{\"x\":270,\"y\":433},{\"x\":269,\"y\":447},{\"x\":236,\"y\":446}],\"text\":\"F-25\"},{\"boundingBox\":[{\"x\":472,\"y\":433},{\"x\":504,\"y\":434},{\"x\":503,\"y\":446},{\"x\":472,\"y\":445}],\"text\":\"F-38\"},{\"boundingBox\":[{\"x\":701,\"y\":432},{\"x\":744,\"y\":432},{\"x\":744,\"y\":448},{\"x\":701,\"y\":447}],\"text\":\"F-105\"},{\"boundingBox\":[{\"x\":961,\"y\":625},{\"x\":968,\"y\":625},{\"x\":968,\"y\":638},{\"x\":961,\"y\":638}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":6,\"y\":664},{\"x\":27,\"y\":664},{\"x\":27,\"y\":678},{\"x\":6,\"y\":678}],\"text\":\"F-1\"},{\"boundingBox\":[{\"x\":471,\"y\":666},{\"x\":494,\"y\":666},{\"x\":494,\"y\":680},{\"x\":471,\"y\":679}],\"text\":\"F-3\"},{\"boundingBox\":[{\"x\":957,\"y\":653},{\"x\":979,\"y\":653},{\"x\":979,\"y\":667},{\"x\":956,\"y\":666}],\"text\":\"0.9\"},{\"boundingBox\":[{\"x\":704,\"y\":667},{\"x\":723,\"y\":667},{\"x\":723,\"y\":678},{\"x\":704,\"y\":678}],\"text\":\"F-4\"},{\"boundingBox\":[{\"x\":958,\"y\":682},{\"x\":979,\"y\":682},{\"x\":979,\"y\":696},{\"x\":958,\"y\":696}],\"text\":\"0.8\"},{\"boundingBox\":[{\"x\":957,\"y\":710},{\"x\":977,\"y\":710},{\"x\":977,\"y\":724},{\"x\":957,\"y\":724}],\"text\":\"0.7\"},{\"boundingBox\":[{\"x\":958,\"y\":739},{\"x\":978,\"y\":739},{\"x\":978,\"y\":752},{\"x\":958,\"y\":752}],\"text\":\"0.6\"},{\"boundingBox\":[{\"x\":958,\"y\":768},{\"x\":980,\"y\":768},{\"x\":980,\"y\":781},{\"x\":958,\"y\":781}],\"text\":\"0.5\"},{\"boundingBox\":[{\"x\":958,\"y\":795},{\"x\":979,\"y\":795},{\"x\":979,\"y\":809},{\"x\":958,\"y\":809}],\"text\":\"0.4\"},{\"boundingBox\":[{\"x\":957,\"y\":823},{\"x\":980,\"y\":823},{\"x\":980,\"y\":838},{\"x\":957,\"y\":838}],\"text\":\"0.3\"},{\"boundingBox\":[{\"x\":956,\"y\":852},{\"x\":980,\"y\":851},{\"x\":981,\"y\":865},{\"x\":957,\"y\":866}],\"text\":\"0.2\"},{\"boundingBox\":[{\"x\":5,\"y\":893},{\"x\":26,\"y\":893},{\"x\":26,\"y\":909},{\"x\":5,\"y\":909}],\"text\":\"F-1\"},{\"boundingBox\":[{\"x\":238,\"y\":893},{\"x\":259,\"y\":893},{\"x\":259,\"y\":909},{\"x\":238,\"y\":908}],\"text\":\"F-2\"},{\"boundingBox\":[{\"x\":702,\"y\":893},{\"x\":725,\"y\":894},{\"x\":724,\"y\":909},{\"x\":702,\"y\":908}],\"text\":\"F-4\"},{\"boundingBox\":[{\"x\":957,\"y\":881},{\"x\":980,\"y\":881},{\"x\":980,\"y\":894},{\"x\":957,\"y\":895}],\"text\":\"0.1\"},{\"boundingBox\":[{\"x\":472,\"y\":895},{\"x\":494,\"y\":895},{\"x\":493,\"y\":908},{\"x\":472,\"y\":907}],\"text\":\"F-3\"},{\"boundingBox\":[{\"x\":958,\"y\":909},{\"x\":965,\"y\":909},{\"x\":964,\"y\":921},{\"x\":958,\"y\":920}],\"text\":\"0\"}]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"New Observation 00000000 Control Simulation Observations Signals Time Collision Results Integration Response Loss State: velocity, position, material, force ... Trainable Network Layers Differentiable Simulation Layer\",\"lines\":[{\"boundingBox\":[{\"x\":212,\"y\":22},{\"x\":260,\"y\":23},{\"x\":260,\"y\":41},{\"x\":212,\"y\":40}],\"text\":\"New\"},{\"boundingBox\":[{\"x\":176,\"y\":49},{\"x\":291,\"y\":50},{\"x\":291,\"y\":72},{\"x\":176,\"y\":71}],\"text\":\"Observation\"},{\"boundingBox\":[{\"x\":389,\"y\":115},{\"x\":389,\"y\":382},{\"x\":356,\"y\":382},{\"x\":356,\"y\":115}],\"text\":\"00000000\"},{\"boundingBox\":[{\"x\":745,\"y\":165},{\"x\":819,\"y\":166},{\"x\":819,\"y\":186},{\"x\":745,\"y\":185}],\"text\":\"Control\"},{\"boundingBox\":[{\"x\":1429,\"y\":165},{\"x\":1536,\"y\":165},{\"x\":1536,\"y\":187},{\"x\":1429,\"y\":187}],\"text\":\"Simulation\"},{\"boundingBox\":[{\"x\":228,\"y\":193},{\"x\":357,\"y\":194},{\"x\":357,\"y\":217},{\"x\":228,\"y\":215}],\"text\":\"Observations\"},{\"boundingBox\":[{\"x\":747,\"y\":194},{\"x\":818,\"y\":194},{\"x\":817,\"y\":217},{\"x\":747,\"y\":218}],\"text\":\"Signals\"},{\"boundingBox\":[{\"x\":979,\"y\":193},{\"x\":1048,\"y\":194},{\"x\":1048,\"y\":220},{\"x\":980,\"y\":217}],\"text\":\"Time\"},{\"boundingBox\":[{\"x\":1177,\"y\":191},{\"x\":1293,\"y\":192},{\"x\":1292,\"y\":221},{\"x\":1176,\"y\":219}],\"text\":\"Collision\"},{\"boundingBox\":[{\"x\":1445,\"y\":195},{\"x\":1520,\"y\":195},{\"x\":1520,\"y\":216},{\"x\":1445,\"y\":216}],\"text\":\"Results\"},{\"boundingBox\":[{\"x\":942,\"y\":230},{\"x\":1083,\"y\":231},{\"x\":1083,\"y\":261},{\"x\":941,\"y\":259}],\"text\":\"Integration\"},{\"boundingBox\":[{\"x\":1172,\"y\":230},{\"x\":1295,\"y\":231},{\"x\":1295,\"y\":261},{\"x\":1172,\"y\":260}],\"text\":\"Response\"},{\"boundingBox\":[{\"x\":1922,\"y\":239},{\"x\":1977,\"y\":238},{\"x\":1977,\"y\":261},{\"x\":1922,\"y\":261}],\"text\":\"Loss\"},{\"boundingBox\":[{\"x\":864,\"y\":317},{\"x\":1388,\"y\":318},{\"x\":1388,\"y\":351},{\"x\":864,\"y\":351}],\"text\":\"State: velocity, position, material, force ...\"},{\"boundingBox\":[{\"x\":353,\"y\":421},{\"x\":692,\"y\":423},{\"x\":692,\"y\":451},{\"x\":353,\"y\":449}],\"text\":\"Trainable Network Layers\"},{\"boundingBox\":[{\"x\":922,\"y\":419},{\"x\":1332,\"y\":421},{\"x\":1332,\"y\":450},{\"x\":922,\"y\":449}],\"text\":\"Differentiable Simulation Layer\"}],\"words\":[{\"boundingBox\":[{\"x\":212,\"y\":22},{\"x\":251,\"y\":23},{\"x\":251,\"y\":41},{\"x\":212,\"y\":40}],\"text\":\"New\"},{\"boundingBox\":[{\"x\":178,\"y\":49},{\"x\":291,\"y\":51},{\"x\":291,\"y\":72},{\"x\":177,\"y\":72}],\"text\":\"Observation\"},{\"boundingBox\":[{\"x\":390,\"y\":115},{\"x\":389,\"y\":366},{\"x\":358,\"y\":365},{\"x\":356,\"y\":117}],\"text\":\"00000000\"},{\"boundingBox\":[{\"x\":746,\"y\":165},{\"x\":819,\"y\":166},{\"x\":820,\"y\":187},{\"x\":745,\"y\":186}],\"text\":\"Control\"},{\"boundingBox\":[{\"x\":1430,\"y\":165},{\"x\":1534,\"y\":166},{\"x\":1534,\"y\":188},{\"x\":1429,\"y\":188}],\"text\":\"Simulation\"},{\"boundingBox\":[{\"x\":229,\"y\":193},{\"x\":358,\"y\":195},{\"x\":357,\"y\":217},{\"x\":228,\"y\":215}],\"text\":\"Observations\"},{\"boundingBox\":[{\"x\":747,\"y\":194},{\"x\":817,\"y\":194},{\"x\":817,\"y\":217},{\"x\":747,\"y\":218}],\"text\":\"Signals\"},{\"boundingBox\":[{\"x\":979,\"y\":193},{\"x\":1044,\"y\":194},{\"x\":1043,\"y\":220},{\"x\":979,\"y\":218}],\"text\":\"Time\"},{\"boundingBox\":[{\"x\":1177,\"y\":191},{\"x\":1290,\"y\":194},{\"x\":1289,\"y\":222},{\"x\":1177,\"y\":220}],\"text\":\"Collision\"},{\"boundingBox\":[{\"x\":1445,\"y\":195},{\"x\":1520,\"y\":196},{\"x\":1520,\"y\":217},{\"x\":1445,\"y\":217}],\"text\":\"Results\"},{\"boundingBox\":[{\"x\":942,\"y\":230},{\"x\":1076,\"y\":231},{\"x\":1077,\"y\":261},{\"x\":942,\"y\":258}],\"text\":\"Integration\"},{\"boundingBox\":[{\"x\":1174,\"y\":230},{\"x\":1294,\"y\":233},{\"x\":1294,\"y\":260},{\"x\":1173,\"y\":260}],\"text\":\"Response\"},{\"boundingBox\":[{\"x\":1922,\"y\":238},{\"x\":1975,\"y\":238},{\"x\":1976,\"y\":261},{\"x\":1922,\"y\":261}],\"text\":\"Loss\"},{\"boundingBox\":[{\"x\":865,\"y\":319},{\"x\":935,\"y\":318},{\"x\":935,\"y\":351},{\"x\":865,\"y\":350}],\"text\":\"State:\"},{\"boundingBox\":[{\"x\":942,\"y\":318},{\"x\":1053,\"y\":318},{\"x\":1053,\"y\":352},{\"x\":941,\"y\":351}],\"text\":\"velocity,\"},{\"boundingBox\":[{\"x\":1060,\"y\":318},{\"x\":1171,\"y\":318},{\"x\":1171,\"y\":352},{\"x\":1059,\"y\":352}],\"text\":\"position,\"},{\"boundingBox\":[{\"x\":1178,\"y\":318},{\"x\":1289,\"y\":319},{\"x\":1289,\"y\":351},{\"x\":1177,\"y\":352}],\"text\":\"material,\"},{\"boundingBox\":[{\"x\":1296,\"y\":319},{\"x\":1356,\"y\":319},{\"x\":1355,\"y\":351},{\"x\":1295,\"y\":351}],\"text\":\"force\"},{\"boundingBox\":[{\"x\":1362,\"y\":319},{\"x\":1388,\"y\":320},{\"x\":1388,\"y\":350},{\"x\":1362,\"y\":350}],\"text\":\"...\"},{\"boundingBox\":[{\"x\":355,\"y\":421},{\"x\":474,\"y\":422},{\"x\":474,\"y\":450},{\"x\":355,\"y\":450}],\"text\":\"Trainable\"},{\"boundingBox\":[{\"x\":480,\"y\":422},{\"x\":592,\"y\":424},{\"x\":591,\"y\":450},{\"x\":479,\"y\":450}],\"text\":\"Network\"},{\"boundingBox\":[{\"x\":599,\"y\":424},{\"x\":690,\"y\":425},{\"x\":688,\"y\":451},{\"x\":598,\"y\":450}],\"text\":\"Layers\"},{\"boundingBox\":[{\"x\":923,\"y\":420},{\"x\":1100,\"y\":420},{\"x\":1100,\"y\":450},{\"x\":923,\"y\":449}],\"text\":\"Differentiable\"},{\"boundingBox\":[{\"x\":1106,\"y\":420},{\"x\":1247,\"y\":422},{\"x\":1246,\"y\":450},{\"x\":1105,\"y\":450}],\"text\":\"Simulation\"},{\"boundingBox\":[{\"x\":1253,\"y\":422},{\"x\":1333,\"y\":424},{\"x\":1332,\"y\":450},{\"x\":1253,\"y\":450}],\"text\":\"Layer\"}]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}"
      ],
      "imageTags": [
        "rectangle",
        "line",
        "design",
        "whiteboard",
        "text",
        "font",
        "graphic design",
        "graphics",
        "screenshot",
        "white",
        "typography",
        "design",
        "text",
        "font",
        "graphics",
        "logo",
        "typography",
        "text",
        "diagram",
        "screenshot",
        "clothing",
        "model",
        "fashion",
        "fashion model",
        "fashion design",
        "day dress",
        "stomach",
        "fashion illustration",
        "joint",
        "cocktail dress",
        "waist",
        "footwear",
        "shoulder",
        "high heels",
        "dress",
        "person",
        "woman",
        "text",
        "diagram",
        "screenshot",
        "line",
        "plan",
        "design",
        "pattern",
        "screenshot",
        "mosaic",
        "pattern",
        "screenshot",
        "pattern (fashion design)",
        "design",
        "fabric",
        "mosaic",
        "text",
        "screenshot",
        "diagram",
        "design",
        "clothing",
        "human face",
        "person",
        "necktie",
        "chin",
        "collar",
        "dress shirt",
        "forehead",
        "eyebrow",
        "man",
        "formal wear",
        "portrait",
        "gentleman",
        "neck",
        "male person",
        "blazer",
        "white-collar worker",
        "headshot",
        "wearing",
        "portrait photography",
        "suit",
        "jaw",
        "blue",
        "shirt",
        "human face",
        "person",
        "smile",
        "lip",
        "clothing",
        "necklace",
        "eyebrow",
        "portrait photography",
        "chin",
        "skin",
        "fashion accessory",
        "tooth",
        "portrait",
        "layered hair",
        "woman",
        "neck",
        "cheek",
        "jaw",
        "forehead",
        "throat",
        "bob cut",
        "lady",
        "pageboy",
        "eyelash",
        "smiling",
        "bangs",
        "headshot",
        "indoor",
        "wall",
        "wearing"
      ],
      "imageCaption": [
        "{\"tags\":[\"shape\",\"rectangle\"],\"captions\":[{\"text\":\"shape, rectangle\",\"confidence\":0.93581891059875488}]}",
        "{\"tags\":[\"background pattern\"],\"captions\":[{\"text\":\"background pattern\",\"confidence\":0.46532714366912842}]}",
        "{\"tags\":[\"text\"],\"captions\":[{\"text\":\"text\",\"confidence\":0.45652365684509277}]}",
        "{\"tags\":[\"diagram\"],\"captions\":[{\"text\":\"diagram\",\"confidence\":0.92069017887115479}]}",
        "{\"tags\":[\"text\"],\"captions\":[{\"text\":\"a person in a dress\",\"confidence\":0.41421231627464294}]}",
        "{\"tags\":[\"diagram\",\"schematic\"],\"captions\":[{\"text\":\"diagram, schematic\",\"confidence\":0.93907362222671509}]}",
        "{\"tags\":[\"background pattern\"],\"captions\":[{\"text\":\"background pattern\",\"confidence\":0.38274392485618591}]}",
        "{\"tags\":[\"wooden\"],\"captions\":[{\"text\":\"a collage of different colored dresses\",\"confidence\":0.33559450507164}]}",
        "{\"tags\":[\"graphical user interface\",\"diagram\",\"text\"],\"captions\":[{\"text\":\"graphical user interface, diagram, text\",\"confidence\":0.77667760848999023}]}",
        "{\"tags\":[\"person\",\"sky\",\"man\",\"clothing\",\"suit\",\"posing\",\"male\"],\"captions\":[{\"text\":\"a man in a suit\",\"confidence\":0.5021471381187439}]}",
        "{\"tags\":[\"person\",\"indoor\",\"posing\"],\"captions\":[{\"text\":\"a woman with long hair smiling\",\"confidence\":0.38503155112266541}]}"
      ],
      "pii_entities": [
        {
          "text": "https://doi.org/10.1007/s41095-020-0189-1",
          "type": "URL",
          "subtype": null,
          "offset": 28,
          "length": 41,
          "score": 0.8
        },
        {
          "text": "2, June 2021",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 82,
          "length": 12,
          "score": 0.8
        },
        {
          "text": "Junbang Liang1",
          "type": "Person",
          "subtype": null,
          "offset": 183,
          "length": 14,
          "score": 0.99
        },
        {
          "text": "Ming C. Lin1",
          "type": "Person",
          "subtype": null,
          "offset": 203,
          "length": 12,
          "score": 0.99
        },
        {
          "text": "The Author",
          "type": "Organization",
          "subtype": null,
          "offset": 220,
          "length": 10,
          "score": 0.59
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 234,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "today",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 950,
          "length": 5,
          "score": 0.8
        },
        {
          "text": "College Park, MD 20785, USA",
          "type": "Address",
          "subtype": null,
          "offset": 1703,
          "length": 27,
          "score": 0.88
        },
        {
          "text": "J. Liang",
          "type": "Person",
          "subtype": null,
          "offset": 1740,
          "length": 8,
          "score": 0.98
        },
        {
          "text": "liangjb@cs.umd.edu",
          "type": "Email",
          "subtype": null,
          "offset": 1750,
          "length": 18,
          "score": 0.8
        },
        {
          "text": "M. C. Lin",
          "type": "Person",
          "subtype": null,
          "offset": 1774,
          "length": 9,
          "score": 0.97
        },
        {
          "text": "lin@cs.umd.edu",
          "type": "Email",
          "subtype": null,
          "offset": 1785,
          "length": 14,
          "score": 0.8
        },
        {
          "text": "2020-06-24",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 1823,
          "length": 10,
          "score": 0.8
        },
        {
          "text": "2020-07-21",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 1845,
          "length": 10,
          "score": 0.8
        },
        {
          "text": "customer",
          "type": "PersonType",
          "subtype": null,
          "offset": 1936,
          "length": 8,
          "score": 0.96
        },
        {
          "text": "end-users",
          "type": "PersonType",
          "subtype": null,
          "offset": 2615,
          "length": 9,
          "score": 0.76
        },
        {
          "text": "2D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 3083,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 3138,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "TSINGHUA Springer UNIVERSITY PRESS",
          "type": "Organization",
          "subtype": null,
          "offset": 3579,
          "length": 34,
          "score": 0.55
        },
        {
          "text": "J. Liang",
          "type": "Person",
          "subtype": null,
          "offset": 3622,
          "length": 8,
          "score": 0.88
        },
        {
          "text": "M. C. Lin",
          "type": "Person",
          "subtype": null,
          "offset": 3632,
          "length": 9,
          "score": 0.97
        },
        {
          "text": "shoppers",
          "type": "PersonType",
          "subtype": null,
          "offset": 3796,
          "length": 8,
          "score": 0.96
        },
        {
          "text": "customers",
          "type": "PersonType",
          "subtype": null,
          "offset": 4682,
          "length": 9,
          "score": 0.8
        },
        {
          "text": "2D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 4786,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 4806,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 4967,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "2D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 5005,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 5309,
          "length": 4,
          "score": 0.76
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 5634,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 6061,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 6394,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 6451,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 6677,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "2D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 7444,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "Liang",
          "type": "Person",
          "subtype": null,
          "offset": 8254,
          "length": 5,
          "score": 0.96
        },
        {
          "text": "Lin",
          "type": "Person",
          "subtype": null,
          "offset": 8264,
          "length": 3,
          "score": 0.98
        },
        {
          "text": "https://gamma.umd.edu/",
          "type": "URL",
          "subtype": null,
          "offset": 8301,
          "length": 22,
          "score": 0.8
        },
        {
          "text": "TSINGHUA Springer UNIVERSITY PRESS",
          "type": "Organization",
          "subtype": null,
          "offset": 8373,
          "length": 34,
          "score": 0.55
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8729,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 8935,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "2D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 8953,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "CNNs",
          "type": "Organization",
          "subtype": null,
          "offset": 9427,
          "length": 4,
          "score": 0.51
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 10531,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "Author",
          "type": "PersonType",
          "subtype": null,
          "offset": 11023,
          "length": 6,
          "score": 0.65
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 11033,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Q2",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 11128,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "J. Liang",
          "type": "Person",
          "subtype": null,
          "offset": 11607,
          "length": 8,
          "score": 0.9
        },
        {
          "text": "M. C. Lin",
          "type": "Person",
          "subtype": null,
          "offset": 11617,
          "length": 9,
          "score": 0.97
        },
        {
          "text": "researchers",
          "type": "PersonType",
          "subtype": null,
          "offset": 12221,
          "length": 11,
          "score": 0.8
        },
        {
          "text": "spring",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 12787,
          "length": 6,
          "score": 0.8
        },
        {
          "text": "Young",
          "type": "Person",
          "subtype": null,
          "offset": 12952,
          "length": 5,
          "score": 0.87
        },
        {
          "text": "Yang",
          "type": "Person",
          "subtype": null,
          "offset": 13453,
          "length": 4,
          "score": 0.88
        },
        {
          "text": "Yang",
          "type": "Person",
          "subtype": null,
          "offset": 13799,
          "length": 4,
          "score": 0.95
        },
        {
          "text": "http://gamma.cs.unc.edu/",
          "type": "URL",
          "subtype": null,
          "offset": 13844,
          "length": 24,
          "score": 0.8
        },
        {
          "text": "2D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 14310,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "CNN",
          "type": "Organization",
          "subtype": null,
          "offset": 14437,
          "length": 3,
          "score": 0.87
        },
        {
          "text": "CNNs",
          "type": "Organization",
          "subtype": null,
          "offset": 15841,
          "length": 4,
          "score": 0.86
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 15907,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "= 1 h",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 15917,
          "length": 5,
          "score": 0.8
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 16365,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Yang",
          "type": "Person",
          "subtype": null,
          "offset": 16379,
          "length": 4,
          "score": 0.84
        },
        {
          "text": "Author",
          "type": "Organization",
          "subtype": null,
          "offset": 16554,
          "length": 6,
          "score": 0.52
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 16564,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "in 3D",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 17140,
          "length": 5,
          "score": 0.8
        },
        {
          "text": "Euler",
          "type": "Person",
          "subtype": null,
          "offset": 18534,
          "length": 5,
          "score": 0.59
        },
        {
          "text": "Liang",
          "type": "Person",
          "subtype": null,
          "offset": 19478,
          "length": 5,
          "score": 0.96
        },
        {
          "text": "https://gamma.umd.edu/",
          "type": "URL",
          "subtype": null,
          "offset": 19524,
          "length": 22,
          "score": 0.8
        },
        {
          "text": "J. Liang",
          "type": "Person",
          "subtype": null,
          "offset": 19720,
          "length": 8,
          "score": 0.98
        },
        {
          "text": "M. C. Lin",
          "type": "Person",
          "subtype": null,
          "offset": 19730,
          "length": 9,
          "score": 0.96
        },
        {
          "text": "thus 12",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 21165,
          "length": 7,
          "score": 0.8
        },
        {
          "text": "Qiao",
          "type": "Person",
          "subtype": null,
          "offset": 22230,
          "length": 4,
          "score": 0.97
        },
        {
          "text": "Liang",
          "type": "Person",
          "subtype": null,
          "offset": 23195,
          "length": 5,
          "score": 0.97
        },
        {
          "text": "00000000",
          "type": "NZSocialWelfareNumber",
          "subtype": null,
          "offset": 23279,
          "length": 8,
          "score": 0.65
        },
        {
          "text": "00000000",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 23279,
          "length": 8,
          "score": 0.8
        },
        {
          "text": "previously",
          "type": "DateTime",
          "subtype": null,
          "offset": 23863,
          "length": 10,
          "score": 0.8
        },
        {
          "text": "Huang",
          "type": "Person",
          "subtype": null,
          "offset": 24755,
          "length": 5,
          "score": 0.97
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 24794,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "Wang",
          "type": "Person",
          "subtype": null,
          "offset": 24938,
          "length": 4,
          "score": 0.94
        },
        {
          "text": "2D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 25553,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 25605,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "2D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 25732,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 25798,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "CNN",
          "type": "Organization",
          "subtype": null,
          "offset": 25853,
          "length": 3,
          "score": 0.81
        },
        {
          "text": "daily",
          "type": "DateTime",
          "subtype": "Set",
          "offset": 26548,
          "length": 5,
          "score": 0.8
        },
        {
          "text": "Iribe",
          "type": "Organization",
          "subtype": null,
          "offset": 27688,
          "length": 5,
          "score": 0.87
        },
        {
          "text": "National Science Foundation",
          "type": "Organization",
          "subtype": null,
          "offset": 27716,
          "length": 27,
          "score": 0.95
        },
        {
          "text": "J. Liang",
          "type": "Person",
          "subtype": null,
          "offset": 27752,
          "length": 8,
          "score": 0.96
        },
        {
          "text": "M. C. Lin",
          "type": "Person",
          "subtype": null,
          "offset": 27762,
          "length": 9,
          "score": 0.97
        },
        {
          "text": "Zheng,",
          "type": "Person",
          "subtype": null,
          "offset": 27789,
          "length": 6,
          "score": 0.78
        },
        {
          "text": "Z. H.",
          "type": "Person",
          "subtype": null,
          "offset": 27796,
          "length": 5,
          "score": 0.79
        },
        {
          "text": "Zhang, H. T.",
          "type": "Person",
          "subtype": null,
          "offset": 27803,
          "length": 12,
          "score": 0.82
        },
        {
          "text": "Zhang, F. L.",
          "type": "Person",
          "subtype": null,
          "offset": 27817,
          "length": 12,
          "score": 0.82
        },
        {
          "text": "Mu, T. J.",
          "type": "Person",
          "subtype": null,
          "offset": 27831,
          "length": 9,
          "score": 0.78
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 27929,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Dibra, E.",
          "type": "Person",
          "subtype": null,
          "offset": 27940,
          "length": 9,
          "score": 0.79
        },
        {
          "text": "Jain, H.",
          "type": "Person",
          "subtype": null,
          "offset": 27951,
          "length": 8,
          "score": 0.83
        },
        {
          "text": "Öztireli, C.",
          "type": "Person",
          "subtype": null,
          "offset": 27961,
          "length": 12,
          "score": 0.84
        },
        {
          "text": "Ziegler, R.",
          "type": "Person",
          "subtype": null,
          "offset": 27975,
          "length": 11,
          "score": 0.83
        },
        {
          "text": "Gross",
          "type": "Person",
          "subtype": null,
          "offset": 27988,
          "length": 5,
          "score": 0.72
        },
        {
          "text": "M",
          "type": "Person",
          "subtype": null,
          "offset": 27995,
          "length": 1,
          "score": 0.74
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 28143,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 28163,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Bălan, A. O",
          "type": "Person",
          "subtype": null,
          "offset": 28174,
          "length": 11,
          "score": 0.85
        },
        {
          "text": "Black, M. J.",
          "type": "Person",
          "subtype": null,
          "offset": 28188,
          "length": 12,
          "score": 0.8
        },
        {
          "text": "ECCV",
          "type": "Organization",
          "subtype": null,
          "offset": 28278,
          "length": 4,
          "score": 0.62
        },
        {
          "text": "2008",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 28283,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Forsyth, D.",
          "type": "Person",
          "subtype": null,
          "offset": 28335,
          "length": 11,
          "score": 0.84
        },
        {
          "text": "Torr, P.",
          "type": "Person",
          "subtype": null,
          "offset": 28348,
          "length": 8,
          "score": 0.75
        },
        {
          "text": "Zisserman",
          "type": "Person",
          "subtype": null,
          "offset": 28358,
          "length": 9,
          "score": 0.94
        },
        {
          "text": "A",
          "type": "Person",
          "subtype": null,
          "offset": 28369,
          "length": 1,
          "score": 0.6
        },
        {
          "text": "Springer",
          "type": "Organization",
          "subtype": null,
          "offset": 28377,
          "length": 8,
          "score": 0.76
        },
        {
          "text": "2008",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 28401,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Lassner, C.",
          "type": "Person",
          "subtype": null,
          "offset": 28412,
          "length": 11,
          "score": 0.81
        },
        {
          "text": "Romero, J.",
          "type": "Person",
          "subtype": null,
          "offset": 28425,
          "length": 10,
          "score": 0.88
        },
        {
          "text": "Kiefel, M.",
          "type": "Person",
          "subtype": null,
          "offset": 28437,
          "length": 10,
          "score": 0.83
        },
        {
          "text": "Bogo, F.",
          "type": "Person",
          "subtype": null,
          "offset": 28449,
          "length": 8,
          "score": 0.76
        },
        {
          "text": "M. J",
          "type": "Person",
          "subtype": null,
          "offset": 28466,
          "length": 4,
          "score": 0.77
        },
        {
          "text": "Gehler, P. V",
          "type": "Person",
          "subtype": null,
          "offset": 28473,
          "length": 12,
          "score": 0.87
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 28530,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "2D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 28537,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 28657,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Loper, M.",
          "type": "Person",
          "subtype": null,
          "offset": 28668,
          "length": 9,
          "score": 0.76
        },
        {
          "text": "Mahmood, N.",
          "type": "Person",
          "subtype": null,
          "offset": 28679,
          "length": 11,
          "score": 0.8
        },
        {
          "text": "Romero, J.",
          "type": "Person",
          "subtype": null,
          "offset": 28692,
          "length": 10,
          "score": 0.88
        },
        {
          "text": "Pons-",
          "type": "Person",
          "subtype": null,
          "offset": 28704,
          "length": 5,
          "score": 0.66
        },
        {
          "text": "G.",
          "type": "Person",
          "subtype": null,
          "offset": 28715,
          "length": 2,
          "score": 0.6
        },
        {
          "text": "Black, M. J",
          "type": "Person",
          "subtype": null,
          "offset": 28719,
          "length": 11,
          "score": 0.8
        },
        {
          "text": "ACM",
          "type": "Organization",
          "subtype": null,
          "offset": 28775,
          "length": 3,
          "score": 0.71
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 28837,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Wei",
          "type": "Person",
          "subtype": null,
          "offset": 28848,
          "length": 3,
          "score": 0.9
        },
        {
          "text": "S.-E.",
          "type": "Person",
          "subtype": null,
          "offset": 28853,
          "length": 5,
          "score": 0.65
        },
        {
          "text": "Ramakrishna, V.",
          "type": "Person",
          "subtype": null,
          "offset": 28860,
          "length": 15,
          "score": 0.85
        },
        {
          "text": "Kanade, T.",
          "type": "Person",
          "subtype": null,
          "offset": 28877,
          "length": 10,
          "score": 0.75
        },
        {
          "text": "Sheikh, Y.",
          "type": "Person",
          "subtype": null,
          "offset": 28889,
          "length": 10,
          "score": 0.85
        },
        {
          "text": "IEEE",
          "type": "Organization",
          "subtype": null,
          "offset": 28952,
          "length": 4,
          "score": 0.85
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 29023,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Cao, Z.",
          "type": "Person",
          "subtype": null,
          "offset": 29034,
          "length": 7,
          "score": 0.79
        },
        {
          "text": "Simon, T.",
          "type": "Person",
          "subtype": null,
          "offset": 29043,
          "length": 9,
          "score": 0.69
        },
        {
          "text": "Wei, S.",
          "type": "Person",
          "subtype": null,
          "offset": 29054,
          "length": 7,
          "score": 0.78
        },
        {
          "text": "Sheikh, Y.",
          "type": "Person",
          "subtype": null,
          "offset": 29063,
          "length": 10,
          "score": 0.84
        },
        {
          "text": "2D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 29097,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 29238,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Mehta, D.",
          "type": "Person",
          "subtype": null,
          "offset": 29249,
          "length": 9,
          "score": 0.79
        },
        {
          "text": "Sridhar, S.",
          "type": "Person",
          "subtype": null,
          "offset": 29260,
          "length": 11,
          "score": 0.76
        },
        {
          "text": "Sotnychenko, O.",
          "type": "Person",
          "subtype": null,
          "offset": 29273,
          "length": 15,
          "score": 0.85
        },
        {
          "text": "Rhodin, H.",
          "type": "Person",
          "subtype": null,
          "offset": 29290,
          "length": 10,
          "score": 0.8
        },
        {
          "text": "Shafiei, M.",
          "type": "Person",
          "subtype": null,
          "offset": 29302,
          "length": 11,
          "score": 0.86
        },
        {
          "text": "Seidel, H.-P.",
          "type": "Person",
          "subtype": null,
          "offset": 29315,
          "length": 13,
          "score": 0.76
        },
        {
          "text": "Xu, W.",
          "type": "Person",
          "subtype": null,
          "offset": 29330,
          "length": 6,
          "score": 0.85
        },
        {
          "text": "Casas, D.",
          "type": "Person",
          "subtype": null,
          "offset": 29338,
          "length": 9,
          "score": 0.64
        },
        {
          "text": "Theobalt",
          "type": "Person",
          "subtype": null,
          "offset": 29349,
          "length": 8,
          "score": 0.65
        },
        {
          "text": "C",
          "type": "Person",
          "subtype": null,
          "offset": 29359,
          "length": 1,
          "score": 0.63
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 29378,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "ACM",
          "type": "Organization",
          "subtype": null,
          "offset": 29429,
          "length": 3,
          "score": 0.82
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 29490,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Alldieck, T.",
          "type": "Person",
          "subtype": null,
          "offset": 29501,
          "length": 12,
          "score": 0.86
        },
        {
          "text": "Magnor, M.",
          "type": "Person",
          "subtype": null,
          "offset": 29515,
          "length": 10,
          "score": 0.81
        },
        {
          "text": "Xu, W.",
          "type": "Person",
          "subtype": null,
          "offset": 29527,
          "length": 6,
          "score": 0.85
        },
        {
          "text": "Theobalt, C.",
          "type": "Person",
          "subtype": null,
          "offset": 29535,
          "length": 12,
          "score": 0.82
        },
        {
          "text": "Moll, G.",
          "type": "Person",
          "subtype": null,
          "offset": 29555,
          "length": 8,
          "score": 0.71
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 29594,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 29706,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Kanazawa, A.",
          "type": "Person",
          "subtype": null,
          "offset": 29718,
          "length": 12,
          "score": 0.77
        },
        {
          "text": "Black, M. J.",
          "type": "Person",
          "subtype": null,
          "offset": 29732,
          "length": 12,
          "score": 0.8
        },
        {
          "text": "Jacobs, D. W.",
          "type": "Person",
          "subtype": null,
          "offset": 29746,
          "length": 13,
          "score": 0.81
        },
        {
          "text": "Malik, J.",
          "type": "Person",
          "subtype": null,
          "offset": 29761,
          "length": 9,
          "score": 0.86
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 29910,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Varol, G.",
          "type": "Person",
          "subtype": null,
          "offset": 29922,
          "length": 9,
          "score": 0.82
        },
        {
          "text": "Ceylan, D.",
          "type": "Person",
          "subtype": null,
          "offset": 29933,
          "length": 10,
          "score": 0.76
        },
        {
          "text": "Russell, B.",
          "type": "Person",
          "subtype": null,
          "offset": 29945,
          "length": 11,
          "score": 0.71
        },
        {
          "text": "Yang, J.",
          "type": "Person",
          "subtype": null,
          "offset": 29958,
          "length": 8,
          "score": 0.84
        },
        {
          "text": "Yumer",
          "type": "Person",
          "subtype": null,
          "offset": 29968,
          "length": 5,
          "score": 0.84
        },
        {
          "text": "Laptev, I.",
          "type": "Person",
          "subtype": null,
          "offset": 29979,
          "length": 10,
          "score": 0.83
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 30023,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 30115,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Zheng, Z.",
          "type": "Person",
          "subtype": null,
          "offset": 30127,
          "length": 9,
          "score": 0.81
        },
        {
          "text": "Yu, T.",
          "type": "Person",
          "subtype": null,
          "offset": 30138,
          "length": 6,
          "score": 0.73
        },
        {
          "text": "Wei, Y.",
          "type": "Person",
          "subtype": null,
          "offset": 30146,
          "length": 7,
          "score": 0.83
        },
        {
          "text": "Dai, Q.",
          "type": "Person",
          "subtype": null,
          "offset": 30155,
          "length": 7,
          "score": 0.68
        },
        {
          "text": "Liu, Y. Deephuman",
          "type": "Person",
          "subtype": null,
          "offset": 30164,
          "length": 17,
          "score": 0.79
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 30183,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "IEEE",
          "type": "Organization",
          "subtype": null,
          "offset": 30252,
          "length": 4,
          "score": 0.5
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 30313,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Saito, S.",
          "type": "Person",
          "subtype": null,
          "offset": 30325,
          "length": 9,
          "score": 0.78
        },
        {
          "text": "Huang, Z.",
          "type": "Person",
          "subtype": null,
          "offset": 30336,
          "length": 9,
          "score": 0.77
        },
        {
          "text": "Natsume, R.",
          "type": "Person",
          "subtype": null,
          "offset": 30347,
          "length": 11,
          "score": 0.78
        },
        {
          "text": "Morishima, S.",
          "type": "Person",
          "subtype": null,
          "offset": 30360,
          "length": 13,
          "score": 0.77
        },
        {
          "text": "Kanazawa, A.",
          "type": "Person",
          "subtype": null,
          "offset": 30375,
          "length": 12,
          "score": 0.74
        },
        {
          "text": "Li, H. PIFu",
          "type": "Person",
          "subtype": null,
          "offset": 30389,
          "length": 11,
          "score": 0.7
        },
        {
          "text": "IEEE",
          "type": "Organization",
          "subtype": null,
          "offset": 30505,
          "length": 4,
          "score": 0.5
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 30566,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Xu, Y.",
          "type": "Person",
          "subtype": null,
          "offset": 30578,
          "length": 6,
          "score": 0.84
        },
        {
          "text": "Tung",
          "type": "Person",
          "subtype": null,
          "offset": 30598,
          "length": 4,
          "score": 0.93
        },
        {
          "text": "T. Denserac",
          "type": "Person",
          "subtype": null,
          "offset": 30604,
          "length": 11,
          "score": 0.78
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 30623,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "IEEE",
          "type": "Organization",
          "subtype": null,
          "offset": 30704,
          "length": 4,
          "score": 0.5
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 30765,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Smith, D.",
          "type": "Person",
          "subtype": null,
          "offset": 30777,
          "length": 9,
          "score": 0.78
        },
        {
          "text": "Loper, M.",
          "type": "Person",
          "subtype": null,
          "offset": 30788,
          "length": 9,
          "score": 0.82
        },
        {
          "text": "Hu, X.",
          "type": "Person",
          "subtype": null,
          "offset": 30799,
          "length": 6,
          "score": 0.74
        },
        {
          "text": "Mavroidis, P.",
          "type": "Person",
          "subtype": null,
          "offset": 30807,
          "length": 13,
          "score": 0.73
        },
        {
          "text": "Romero",
          "type": "Person",
          "subtype": null,
          "offset": 30822,
          "length": 6,
          "score": 0.98
        },
        {
          "text": "J. FACSIMILE",
          "type": "Person",
          "subtype": null,
          "offset": 30830,
          "length": 12,
          "score": 0.89
        },
        {
          "text": "in less than a second",
          "type": "DateTime",
          "subtype": null,
          "offset": 30882,
          "length": 21,
          "score": 0.8
        },
        {
          "text": "IEEE",
          "type": "Organization",
          "subtype": null,
          "offset": 30928,
          "length": 4,
          "score": 0.83
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 30990,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Alldieck, T.",
          "type": "Person",
          "subtype": null,
          "offset": 31002,
          "length": 12,
          "score": 0.8
        },
        {
          "text": "Magnor, M.",
          "type": "Person",
          "subtype": null,
          "offset": 31016,
          "length": 10,
          "score": 0.81
        },
        {
          "text": "Bhatnagar, B. L.",
          "type": "Person",
          "subtype": null,
          "offset": 31028,
          "length": 16,
          "score": 0.72
        },
        {
          "text": "Theobalt",
          "type": "Person",
          "subtype": null,
          "offset": 31046,
          "length": 8,
          "score": 0.65
        },
        {
          "text": "Pons-Moll, G.",
          "type": "Person",
          "subtype": null,
          "offset": 31060,
          "length": 13,
          "score": 0.88
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 31237,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Kolotouros",
          "type": "Person",
          "subtype": null,
          "offset": 31249,
          "length": 10,
          "score": 0.95
        },
        {
          "text": "N.",
          "type": "Person",
          "subtype": null,
          "offset": 31261,
          "length": 2,
          "score": 0.6
        },
        {
          "text": "Pavlakos, G.",
          "type": "Person",
          "subtype": null,
          "offset": 31265,
          "length": 12,
          "score": 0.85
        },
        {
          "text": "Black, M. J.",
          "type": "Person",
          "subtype": null,
          "offset": 31279,
          "length": 12,
          "score": 0.8
        },
        {
          "text": "Daniilidis",
          "type": "Person",
          "subtype": null,
          "offset": 31293,
          "length": 10,
          "score": 0.94
        },
        {
          "text": "K",
          "type": "Person",
          "subtype": null,
          "offset": 31305,
          "length": 1,
          "score": 0.73
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 31332,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 31470,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Liang, J.",
          "type": "Person",
          "subtype": null,
          "offset": 31482,
          "length": 9,
          "score": 0.88
        },
        {
          "text": "Lin, M. C.",
          "type": "Person",
          "subtype": null,
          "offset": 31493,
          "length": 10,
          "score": 0.79
        },
        {
          "text": "IEEE",
          "type": "Organization",
          "subtype": null,
          "offset": 31600,
          "length": 4,
          "score": 0.5
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 31661,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Yang, S.",
          "type": "Person",
          "subtype": null,
          "offset": 31673,
          "length": 8,
          "score": 0.72
        },
        {
          "text": "Pan, Z. R.",
          "type": "Person",
          "subtype": null,
          "offset": 31683,
          "length": 10,
          "score": 0.72
        },
        {
          "text": "Amert, T.",
          "type": "Person",
          "subtype": null,
          "offset": 31695,
          "length": 9,
          "score": 0.76
        },
        {
          "text": "Wang, K.",
          "type": "Person",
          "subtype": null,
          "offset": 31706,
          "length": 8,
          "score": 0.78
        },
        {
          "text": "Yu, L.",
          "type": "Person",
          "subtype": null,
          "offset": 31716,
          "length": 6,
          "score": 0.72
        },
        {
          "text": "Berg, T.",
          "type": "Person",
          "subtype": null,
          "offset": 31727,
          "length": 8,
          "score": 0.66
        },
        {
          "text": "Lin, M. C",
          "type": "Person",
          "subtype": null,
          "offset": 31737,
          "length": 9,
          "score": 0.83
        },
        {
          "text": "ACM",
          "type": "Organization",
          "subtype": null,
          "offset": 31808,
          "length": 3,
          "score": 0.81
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 31870,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Yang, S.",
          "type": "Person",
          "subtype": null,
          "offset": 31882,
          "length": 8,
          "score": 0.73
        },
        {
          "text": "Liang, J.",
          "type": "Person",
          "subtype": null,
          "offset": 31892,
          "length": 9,
          "score": 0.89
        },
        {
          "text": "Lin, M. C.",
          "type": "Person",
          "subtype": null,
          "offset": 31903,
          "length": 10,
          "score": 0.78
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 32050,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Qiao, Y. L.",
          "type": "Person",
          "subtype": null,
          "offset": 32062,
          "length": 11,
          "score": 0.85
        },
        {
          "text": "Liang, J. B.",
          "type": "Person",
          "subtype": null,
          "offset": 32075,
          "length": 12,
          "score": 0.86
        },
        {
          "text": "Koltun, V.",
          "type": "Person",
          "subtype": null,
          "offset": 32089,
          "length": 10,
          "score": 0.75
        },
        {
          "text": "Lin, M. C.",
          "type": "Person",
          "subtype": null,
          "offset": 32101,
          "length": 10,
          "score": 0.78
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 32203,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "De Avila Belbute-Peres, F.",
          "type": "Person",
          "subtype": null,
          "offset": 32215,
          "length": 26,
          "score": 0.82
        },
        {
          "text": "Smith, K. A.",
          "type": "Person",
          "subtype": null,
          "offset": 32243,
          "length": 12,
          "score": 0.8
        },
        {
          "text": "Allen, K.",
          "type": "Person",
          "subtype": null,
          "offset": 32257,
          "length": 9,
          "score": 0.78
        },
        {
          "text": "Tenenbaum, J.",
          "type": "Person",
          "subtype": null,
          "offset": 32268,
          "length": 13,
          "score": 0.87
        },
        {
          "text": "Kolter, J. Z",
          "type": "Person",
          "subtype": null,
          "offset": 32283,
          "length": 12,
          "score": 0.84
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 32431,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Degrave, J.",
          "type": "Person",
          "subtype": null,
          "offset": 32443,
          "length": 11,
          "score": 0.82
        },
        {
          "text": "Hermans, M.",
          "type": "Person",
          "subtype": null,
          "offset": 32456,
          "length": 11,
          "score": 0.79
        },
        {
          "text": "Dambre, J.",
          "type": "Person",
          "subtype": null,
          "offset": 32469,
          "length": 10,
          "score": 0.83
        },
        {
          "text": "Wyffels, F.",
          "type": "Person",
          "subtype": null,
          "offset": 32481,
          "length": 11,
          "score": 0.81
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 32595,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Liang, J.",
          "type": "Person",
          "subtype": null,
          "offset": 32607,
          "length": 9,
          "score": 0.87
        },
        {
          "text": "Lin, M.",
          "type": "Person",
          "subtype": null,
          "offset": 32618,
          "length": 7,
          "score": 0.78
        },
        {
          "text": "Koltun, V.",
          "type": "Person",
          "subtype": null,
          "offset": 32627,
          "length": 10,
          "score": 0.78
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 32773,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Hu, Y.",
          "type": "Person",
          "subtype": null,
          "offset": 32853,
          "length": 6,
          "score": 0.73
        },
        {
          "text": "Liu, J.",
          "type": "Person",
          "subtype": null,
          "offset": 32861,
          "length": 7,
          "score": 0.84
        },
        {
          "text": "Spielberg, A.",
          "type": "Person",
          "subtype": null,
          "offset": 32870,
          "length": 13,
          "score": 0.81
        },
        {
          "text": "Tenenbaum, J.",
          "type": "Person",
          "subtype": null,
          "offset": 32885,
          "length": 13,
          "score": 0.87
        },
        {
          "text": "Freeman, W. T.",
          "type": "Person",
          "subtype": null,
          "offset": 32903,
          "length": 14,
          "score": 0.83
        },
        {
          "text": "Wu, J.",
          "type": "Person",
          "subtype": null,
          "offset": 32919,
          "length": 6,
          "score": 0.81
        },
        {
          "text": "Rus, D.",
          "type": "Person",
          "subtype": null,
          "offset": 32927,
          "length": 7,
          "score": 0.66
        },
        {
          "text": "Matusik",
          "type": "Person",
          "subtype": null,
          "offset": 32936,
          "length": 7,
          "score": 0.79
        },
        {
          "text": "W. ChainQueen",
          "type": "Person",
          "subtype": null,
          "offset": 32945,
          "length": 13,
          "score": 0.9
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 33112,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Hu, Y. M.",
          "type": "Person",
          "subtype": null,
          "offset": 33124,
          "length": 9,
          "score": 0.73
        },
        {
          "text": "Anderson, L.",
          "type": "Person",
          "subtype": null,
          "offset": 33135,
          "length": 12,
          "score": 0.75
        },
        {
          "text": "Li, T. M.",
          "type": "Person",
          "subtype": null,
          "offset": 33149,
          "length": 9,
          "score": 0.71
        },
        {
          "text": "Sun, Q.",
          "type": "Person",
          "subtype": null,
          "offset": 33160,
          "length": 7,
          "score": 0.68
        },
        {
          "text": "Carr, N.",
          "type": "Person",
          "subtype": null,
          "offset": 33169,
          "length": 8,
          "score": 0.67
        },
        {
          "text": "Ragan-Kelley, J.",
          "type": "Person",
          "subtype": null,
          "offset": 33179,
          "length": 16,
          "score": 0.93
        },
        {
          "text": "Durand, F. DiffTaichi",
          "type": "Person",
          "subtype": null,
          "offset": 33197,
          "length": 21,
          "score": 0.81
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 33305,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Liu, K. X.",
          "type": "Person",
          "subtype": null,
          "offset": 33317,
          "length": 10,
          "score": 0.74
        },
        {
          "text": "Zeng, X. Y.",
          "type": "Person",
          "subtype": null,
          "offset": 33329,
          "length": 11,
          "score": 0.84
        },
        {
          "text": "Bruniaux, P.",
          "type": "Person",
          "subtype": null,
          "offset": 33342,
          "length": 12,
          "score": 0.83
        },
        {
          "text": "Tao, X. Y.",
          "type": "Person",
          "subtype": null,
          "offset": 33356,
          "length": 10,
          "score": 0.73
        },
        {
          "text": "Yao",
          "type": "Person",
          "subtype": null,
          "offset": 33368,
          "length": 3,
          "score": 0.86
        },
        {
          "text": "X",
          "type": "Person",
          "subtype": null,
          "offset": 33373,
          "length": 1,
          "score": 0.71
        },
        {
          "text": "Li, V.",
          "type": "Person",
          "subtype": null,
          "offset": 33380,
          "length": 6,
          "score": 0.68
        },
        {
          "text": "Wang, J.",
          "type": "Person",
          "subtype": null,
          "offset": 33388,
          "length": 8,
          "score": 0.76
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 33397,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 33489,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Huang, P.",
          "type": "Person",
          "subtype": null,
          "offset": 33501,
          "length": 9,
          "score": 0.8
        },
        {
          "text": "Yao, J.",
          "type": "Person",
          "subtype": null,
          "offset": 33512,
          "length": 7,
          "score": 0.87
        },
        {
          "text": "Zhao, H.",
          "type": "Person",
          "subtype": null,
          "offset": 33521,
          "length": 8,
          "score": 0.74
        },
        {
          "text": "3D",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 33550,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 33688,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Wang, T. Y.",
          "type": "Person",
          "subtype": null,
          "offset": 33700,
          "length": 11,
          "score": 0.75
        },
        {
          "text": "Ceylan, D.",
          "type": "Person",
          "subtype": null,
          "offset": 33713,
          "length": 10,
          "score": 0.76
        },
        {
          "text": "Popović, J.",
          "type": "Person",
          "subtype": null,
          "offset": 33725,
          "length": 11,
          "score": 0.91
        },
        {
          "text": "Mitra, N. J",
          "type": "Person",
          "subtype": null,
          "offset": 33738,
          "length": 11,
          "score": 0.71
        },
        {
          "text": "ACM",
          "type": "Organization",
          "subtype": null,
          "offset": 33812,
          "length": 3,
          "score": 0.69
        }
      ]
    }
  ]
}